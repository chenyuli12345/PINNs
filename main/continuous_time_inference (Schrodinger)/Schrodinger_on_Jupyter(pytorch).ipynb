{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the necessary libraries, set the gpu and random seed\n",
    "\n",
    "#下面这行代码，是为了把自己编写的代码文件当作一共模块导入，这里是把Utilities文件夹中的plotting.py文件当作python的模块导入，对应的是下面的from plotting import newfig, savefig。路径要随着不同设备的系统做相应的修改\n",
    "import sys #导入sys模块。sys模块提供了一些变量和函数，用于与 Python解释器进行交互和访问。例如，sys.path 是一个 Python 在导入模块时会查找的路径列表，sys.argv 是一个包含命令行参数的列表，sys.exit() 函数可以用于退出 Python 程序。导入 sys 模块后，你就可以在你的程序中使用这些变量和函数了。\n",
    "sys.path.insert(0, '../../Utilities/') #在 Python的sys.path列表中插入一个新的路径。sys.path是一个 Python 在导入模块时会查找的路径列表。新的路径'../../Utilities/'相对于当前脚本的路径。当你尝试导入一个模块时，Python 会在 sys.path 列表中的路径下查找这个模块。通过在列表开始位置插入一个路径，你可以让 Python 优先在这个路径下查找模块。这在你需要导入自定义模块或者不在 Python 标准库中的模块时非常有用。\n",
    "\n",
    "import torch\n",
    "#collections是python一个内置模块，提供了一些有用的数据结构\n",
    "from collections import OrderedDict  #这个类是字典dict的一个子类，用于创建有序的字典。普通字典中元素顺序是无序的，在OrderedDict中元素的顺序是有序的，元素的顺序是按照它们被添加到字典中的顺序决定的。\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#下面的`scipy`是一个用于科学计算和技术计算的Python库，提供了许多高级的数学函数和便利的操作，包括数值积分、插值、优化、图像处理、统计等。\n",
    "import scipy.io #导入了scipy库中的io模块。scipy.io模块包含了一些用于文件输入/输出的函数，例如读取和写入.mat文件（MATLAB格式）。\n",
    "from scipy.interpolate import griddata#`scipy.interpolate`是`scipy`库中的一个模块，提供了许多插值工具，用于在给定的离散数据点之间进行插值和拟合。`griddata`是这个模块中的一个函数，用于在无规则的数据点上进行插值。它使用方法如下：\n",
    "#griddata(points, values, xi, method='linear', fill_value=nan, rescale=False)；\n",
    "   # `points`： ndarray of floats, shape (n, D)。表示数据点的坐标。`values`： ndarray of float or complex, shape (n,)。表示数据点的值。`xi`： ndarray of float, shape (M, D)。表示插值点的坐标。`method`： 插值方法，可选'linear'、'nearest'、'cubic'。默认为'linear'。\n",
    "   #`fill_value`： 在插值范围外的点的值。默认为nan。`rescale`： 是否对坐标点进行重标定，以提高数值稳定性。默认为False。\n",
    "   #返回值：ndarray，shape (M,) or (M, 1)。插值点的值。这个函数可以用于从散列的数据点创建一个连续的函数，这对于处理实际数据非常有用，因为实际数据通常是不规则或者不完整的。\n",
    "from pyDOE import lhs #`pyDOE`是一个Python库，用于设计实验。它提供了一些函数来生成各种设计，如因子设计、拉丁超立方设计等。`lhs`是库中的一个函数，全名为\"Latin Hypercube Sampling\"，拉丁超立方采样。这是一种统计方法，用于生成一个近似均匀分布的多维样本点集。它在参数空间中生成一个非常均匀的样本，这对于高维数值优化问题非常有用，因为它可以更好地覆盖参数空间。\n",
    "#`lhs`函数的基本用法如下：lhs(n, samples=1000):其中，`n`是参数的数量，`samples`是想生成的样本点的数量。这个函数会返回一个形状为(samples, n)的数组，每一行都是一个n维的样本点，所有的样本点都在[0, 1]范围内。\n",
    "from plotting_torch import newfig, savefig #从自定义的plotting.py文件中导入了newfig和savefig函数。这两个函数用于创建和保存图形。这两个函数的定义在plotting.py文件中\n",
    "from mpl_toolkits.mplot3d import Axes3D #`mpl_toolkits.mplot3d`是`matplotlib`库的一个模块，用于创建三维图形。`Axes3D`是`mpl_toolkits.mplot3d`模块中的一个类，用于创建一个三维的坐标轴。可以在这个坐标轴上绘制三维的图形，如曲线、曲面等。\n",
    "import time #一个内置模块，用于处理时间相关的操作。\n",
    "import matplotlib.gridspec as gridspec #是`matplotlib`库的一个模块，用于创建一个网格布局来放置子图。在`matplotlib`中可以创建一个或多个子图（subplot），每个子图都有自己的坐标轴，并可以在其中绘制图形。`gridspec`模块提供了一个灵活的方式来创建和放置子图。\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable #`mpl_toolkits.axes_grid1`是`matplotlib`库的一个模块，提供了一些高级的工具来控制matplotlib图形中的坐标轴和颜色条。`make_axes_locatable`是模块中的一个函数，用于创建一个可分割的坐标轴。可以在这个坐标轴的四个方向（上、下、左、右）添加新的坐标轴或颜色条。\n",
    "\n",
    "\n",
    "np.random.seed(1234) #这里有变化，仅需要设置numpy的随机数生成器的种子。设置随机数生成器的种子可以确保每次运行程序时，NumPy生成的随机数序列都是一样的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA support \n",
    "\n",
    "#设置pytorch的设备，代表了在哪里执行张量积算，设备可以是cpu或者cuda（gpu），并将这个做运算的设备对象存储在变量device中，后续张量计算回在这个设备上执行\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the deep neural network\n",
    "class DNN(torch.nn.Module):\n",
    "    #第一个方法\n",
    "    def __init__(self, layers):\n",
    "        super(DNN, self).__init__() #调用父类的__init__方法进行初始化\n",
    "        \n",
    "        # parameters\n",
    "        self.depth = len(layers) - 1 #定义名为depth的属性，表示神经网络的深度，等于层数-1\n",
    "        \n",
    "        # set up layer order dict\n",
    "        self.activation = torch.nn.Tanh #设置激活函数为tanh\n",
    "         \n",
    "        layer_list = list() #定义一个空列表layer_list\n",
    "        for i in range(self.depth - 1):  #循环depth次\n",
    "            #将每一层（全连接层）添加到layer_list中\n",
    "            layer_list.append(\n",
    "                ('layer_%d' % i, torch.nn.Linear(layers[i], layers[i+1]))\n",
    "            )\n",
    "            #将每一层的激活函数添加到layer_list中\n",
    "            layer_list.append(('activation_%d' % i, self.activation()))\n",
    "\n",
    "        #循环结束后，将最后一层的线性变换添加到layer_list中（因为没有激活函数了）\n",
    "        layer_list.append(\n",
    "            ('layer_%d' % (self.depth - 1), torch.nn.Linear(layers[-2], layers[-1]))\n",
    "        )\n",
    "        #然后使用OrderedDict将layer_list中的元素转换为有序字典\n",
    "        layerDict = OrderedDict(layer_list)\n",
    "        \n",
    "        # deploy layers，将layerDict转换为一个神经网络模型，赋值给self.layers\n",
    "        self.layers = torch.nn.Sequential(layerDict)\n",
    "    \n",
    "    #第二个方法，定义了模型的前向传播过程\n",
    "    def forward(self, x):  #接收输入x\n",
    "        out = self.layers(x) #将输入x传入神经网络模型self.layers中，得到输出out\n",
    "        return out #返回输出out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the class of PINN\n",
    "\n",
    "#定义了一个名为`PhysicsInformedNN'的类，用于实现基于物理的神经网络。\n",
    "class PhysicsInformedNN:\n",
    "    # Initialize the class\n",
    "    def __init__(self, x0, u0, v0, tb, X_f, layers, lb, ub): #这个类包含的第一个方法__init__，这是一个特殊的方法，也就是这个类的构造函数，用于初始化新创建的对象，接受了几个参数\n",
    "        \n",
    "        \n",
    "        #`numpy.concatenate`是一个用于数组拼接的函数。它可以将多个数组沿指定的轴拼接在一起，形成一个新的数组：numpy.concatenate((a1,a2, ...), axis=0)其中，`a1,a2, ...`是需要拼接的数组（只能接受数组或序列类型的参数，且参数形状必须相同），可以是多个。`axis`参数用于指定拼接的轴向，`axis=0`表示沿着第一个轴（即行）进行拼接，不指定`axis`参数默认值是0。\n",
    "        X0 = np.concatenate((x0,0*x0), 1) # [x0, 0],将x0和0*x0两个数组在第二个维度（即列）上进行了合并。0*x0会生成一个与x0形状相同，但所有元素都为0的数组。因此，X0的结果是一个新的二维数组，其中第一列是x0的值，第二列全为0\n",
    "        X_lb = np.concatenate((0*tb+lb[0],tb), 1) # [lb[0], tb],将0*tb+lb[0]和tb两个数组在第二个维度（即列）上进行了合并。0*tb+lb[0]会生成一个与tb形状相同，但所有元素都为lb[0]的数组。因此，X_lb的结果是一个新的二维数组，其中第一列全为lb[0]的值，第二列是tb的值。\n",
    "        X_ub = np.concatenate((0*tb+ub[0],tb), 1) # [ub[0], tb],同上生成一个与tb形状相同，但所有元素都为ub[0]的数组。因此，X_ub的结果是一个新的二维数组，其中第一列全为ub[0]的值，第二列是tb的值\n",
    "        \n",
    "        #Python使用self关键字来表示类的实例。当在类的方法中定义一个变量时，例如lb和ub，这些变量只在该方法内部可见，也就是说它们的作用域仅限于该方法。当方法执行完毕后，这些变量就会被销毁，无法在其他方法中访问它们。但如果希望在类的其他方法中也能访问这些变量就需要将它们保存为类的实例属性。这就是self.lb和self.ub的作用。\n",
    "            #通过将lb和ub赋值给self.lb和self.ub，就可以在类的其他方法中通过self.lb和self.ub来访问这些值。总的来说，self.lb和self.ub是类的实例属性，它们的作用域是整个类，而不仅仅是定义它们的方法。\n",
    "        self.lb = torch.tensor(lb).float().to(device) #将传入的lb和ub参数的值存储在实例中，以便后续使用。这样可以在类的其他方法中通过self.lb和self.ub来访问这些值。\n",
    "        self.ub = torch.tensor(ub).float().to(device)\n",
    "\n",
    "\n",
    "        self.x0 = torch.tensor(X0[:,0:1], requires_grad=True).float().to(device) #将X0的第一列赋值给self.x0（:表示取所有行,0：1实际上表示取第一列，因为python是左闭右开的）,将X0的第二列赋值给self.t0。这样可以在类的其他方法中通过self.x0和self.t0来访问这些值。\n",
    "        self.t0 = torch.tensor(X0[:,1:2], requires_grad=True).float().to(device) #将x0的第二列赋值给self.t0\n",
    "\n",
    "        self.x_lb = torch.tensor(X_lb[:,0:1], requires_grad=True).float().to(device) #将X_lb的第一列赋值给self.x_lb\n",
    "        self.t_lb = torch.tensor(X_lb[:,1:2], requires_grad=True).float().to(device) #将X_lb的第二列赋值给self.t_lb\n",
    "\n",
    "        self.x_ub = torch.tensor(X_ub[:,0:1], requires_grad=True).float().to(device) #将X_ub的第一列赋值给self.x_ub\n",
    "        self.t_ub = torch.tensor(X_ub[:,0:1], requires_grad=True).float().to(device) #将X_ub的第二列赋值给self.t_ub\n",
    "        \n",
    "        self.x_f = torch.tensor(X_f[:,0:1], requires_grad=True).float().to(device) #将X_f的第一列赋值给self.x_f\n",
    "        self.t_f = torch.tensor(X_f[:,1:2], requires_grad=True).float().to(device) #将X_f的第二列赋值给self.t_f\n",
    "        \n",
    "        self.u0 = torch.tensor(u0).float().to(device) #将传入的u0和v0参数的值存储在实例中，以便后续使用。这样可以在类的其他方法中通过self.u0和self.v0来访问这些值。\n",
    "        self.v0 = torch.tensor(v0).float().to(device)\n",
    "        \n",
    "        # Initialize NNs \n",
    "        self.layers = layers #将传入的layers参数的值存储在实例中，以便后续使用。这样可以在类的其他方法中通过self.layers来访问这些值。\n",
    "        \n",
    "        \n",
    "        # deep neural networks\n",
    "        self.dnn = DNN(layers).to(device) #创建一个DNN类的实例，传入layers参数来实现神经网络的初始化，然后将这个实例移动到指定的设备上\n",
    "\n",
    "\n",
    "\n",
    "        # optimizers: using the same settings，这里是使用pytorch库进行优化的部分\n",
    "        #创建优化器optimizer，使用LBFGS算法，具体每个参数意义见下方\n",
    "        self.optimizer = torch.optim.LBFGS(\n",
    "            self.dnn.parameters(), #要优化的参数，这里返回的是一个生成器，包含了self.dnn中的所有参数（神经网络权重、偏置以及两个新加的变量）\n",
    "            lr=1.0,  #学习率设置为1\n",
    "            max_iter=50000,  #最大迭代次数为50000\n",
    "            max_eval=50000,  #最大评估次数为50000\n",
    "            history_size=50, #历史大小为50，即用于计算Hessian矩阵近似的最近几步的信息\n",
    "            tolerance_grad=1e-5,  #优化的第一个停止条件，当梯度的L2范数小于1e-5时停止优化\n",
    "            tolerance_change=1.0 * np.finfo(float).eps, #优化的第二个停止条件，当优化的目标函数值的变化小于1.0 * np.finfo(float).eps时停止优化\n",
    "            line_search_fn=\"strong_wolfe\"       # 制定了用于一维搜索的方法，这里表示用强Wolfe条件\n",
    "        )\n",
    "        #创建第二个优化器，括号内为要优化的参数，使用Adam优化方法\n",
    "        self.optimizer_Adam = torch.optim.Adam(self.dnn.parameters())\n",
    "                \n",
    "\n",
    "        self.iter = 0 #记录迭代次数 \n",
    "\n",
    "    \n",
    "    \n",
    "    #pytorch中\n",
    "    #定义了一个名为net_u的函数/方法，用于计算神经网络的输出。这个方法接受两个参数，分别是x和t，其中x是输入数据，t是时间数据。最后返回神经网络的输出。     \n",
    "    def net_uv(self, x, t):  \n",
    "        uv = self.dnn(torch.cat([x, t], dim=1))  #（第一个参数将输入的两个参数x和t在第二个维度（列）上进行拼接，形成一个新的张量）调用DNN，根据两个参数权重和偏置，以及新得到的张量，计算神经网络的输出u\n",
    "        #将uv（是一个二维张量）的第一列赋值给u，第二列赋值给v\n",
    "        u=uv[:,0:1]\n",
    "        v=uv[:,1:2]\n",
    "\n",
    "        u_x = torch.autograd.grad(\n",
    "            u, x, \n",
    "            grad_outputs=torch.ones_like(u),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        v_x = torch.autograd.grad(\n",
    "            v, x, \n",
    "            grad_outputs=torch.ones_like(v),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "\n",
    "        return u,v,u_x,v_x #返回神经网络的输出u和v，以及u关于x的梯度u_x和v关于x的梯度v_x\n",
    "\n",
    "\n",
    "    #定义了一个名为net_f的函数/方法，用于计算论文中的f。这个方法接受两个参数，分别是x和t，其中x是输入数据，t是时间数据。最后返回计算得到的f。\n",
    "    def net_f_uv(self, x, t):\n",
    "        \"\"\" The pytorch autograd version of calculating residual \"\"\"\n",
    "\n",
    "        u,v,u_x,v_x=self.net_uv(x,t) #调用上面的函数/方法，计算神经网络的输出（两个）以及输出关于输入x的梯度（两个）\n",
    "        \n",
    "        #计算u关于t的梯度，也就是u关于t的导数，这里使用了pytorch的自动求导功能\n",
    "        u_t = torch.autograd.grad(\n",
    "            u, t,  #输入的张量，要计算u关于t的导数\n",
    "            grad_outputs=torch.ones_like(u), #生成一个与u形状相同，所有元素均为1的张量，这个参数用于指定向量-雅可比积的像两部分\n",
    "            retain_graph=True, #表示计算完梯度之后保留计算图若需要多次计算梯度，则需要设置改参数为True\n",
    "            create_graph=True #创建梯度的计算图，使我们能够计算高阶导数\n",
    "        )[0] #这个函数的返回值是一个元组，其中包含了每个输入张量的梯度。这里只关心第一个输入张量u的梯度，所以我们使用[0]来获取这个梯度。？？？？又说只有一个梯度\n",
    "        v_t = torch.autograd.grad(\n",
    "            v, t, \n",
    "            grad_outputs=torch.ones_like(v),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        u_xx = torch.autograd.grad(\n",
    "            u_x, x, \n",
    "            grad_outputs=torch.ones_like(u_x),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        v_xx = torch.autograd.grad(\n",
    "            v_x, x, \n",
    "            grad_outputs=torch.ones_like(v_x),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        \n",
    "        f_u=u_t+0.5*v_xx+(u**2+v**2)*v    #计算f_u,定义见论文\n",
    "        f_v=v_t-0.5*u_xx-(u**2+v**2)*u   #计算f_v,定义见论文\n",
    "        return f_u, f_v  #返回计算得到的f_u和f_v\n",
    "\n",
    "\n",
    "    def loss_func(self):\n",
    "        self.optimizer.zero_grad() #清除之前计算的梯度（在PyTorch中，梯度会累积，所以在每次新的优化迭代之前，我们需要清除之前的梯度）\n",
    "\n",
    "        u0_pred, v0_pred, _ , _ = self.net_uv(self.x0, self.t0) #是调用net_uv函数,将self.x0_tf和self.t0_tf作为参数传入,然后将返回的前两个结果赋值给self.u0_pred和self.v0_pred。后两个_是Python惯用法，表示不关心net_uv函数返回的后两个结果。\n",
    "        u_lb_pred, v_lb_pred, u_x_lb_pred, v_x_lb_pred = self.net_uv(self.x_lb, self.t_lb) #同上，不过这里函数返回的后两个结果会赋值给self.u_x_lb_pred和self.v_x_lb_pred。\n",
    "        u_ub_pred, v_ub_pred, u_x_ub_pred, v_x_ub_pred = self.net_uv(self.x_ub, self.t_ub) #同上\n",
    "        f_u_pred, f_v_pred = self.net_f_uv(self.x_f, self.t_f) #调用net_f_uv函数,将self.x_f_tf和self.t_f_tf作为参数传入,然后将返回的结果赋值给self.f_u_pred和self.f_v_pred。\n",
    "\n",
    "        loss = torch.mean((self.u0 - u0_pred) ** 2)  + \\\n",
    "                    torch.mean((self.v0 - v0_pred) ** 2) + \\\n",
    "                    torch.mean((u_lb_pred - u_ub_pred) ** 2) + \\\n",
    "                    torch.mean((v_lb_pred - v_ub_pred) ** 2) + \\\n",
    "                    torch.mean((u_x_lb_pred - u_x_ub_pred) ** 2) + \\\n",
    "                    torch.mean((v_x_lb_pred - v_x_ub_pred) ** 2) + \\\n",
    "                    torch.mean(f_u_pred ** 2) + \\\n",
    "                    torch.mean(f_v_pred ** 2)\n",
    "        loss.backward() #被调用以计算损失函数关于神经网络参数的梯度。这个梯度将被用于优化器来更新神经网络参数\n",
    "        \n",
    "        self.iter += 1 #每调用一次损失函数，迭代次数加1\n",
    "\n",
    "\n",
    "        # self.loss_value.append(loss) #将计算得到的loss值添加到self.loss_value列表中\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        if self.iter % 100 == 0:\n",
    "            print(\n",
    "                'Iter %d, Loss: %e' % \n",
    "                (\n",
    "                    self.iter,\n",
    "                    loss#这里使用了detach()方法，将lambda_2从计算图中分离出来，这样就不会计算lambda_2的梯度，只是将lambda_2的值返回？？？why\n",
    "                ) #每100次迭代，打印一次迭代次数、总的loss、loss_u和loss_f\n",
    "            )\n",
    "        return loss #返回loss\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    #定义了一个名为train的函数/方法，用于训练神经网络。这个方法接受一个参数nIter，表示训练的迭代次数。\n",
    "    def train(self, nIter):\n",
    "        self.dnn.train()#将神经网络设置为训练模式而不是评估模式\n",
    "\n",
    "        #先使用Adam优化器优化nIter次\n",
    "        for epoch in range(nIter):\n",
    "            u0_pred, v0_pred, _ , _ = self.net_uv(self.x0, self.t0) #是调用net_uv函数,将self.x0_tf和self.t0_tf作为参数传入,然后将返回的前两个结果赋值给self.u0_pred和self.v0_pred。后两个_是Python惯用法，表示不关心net_uv函数返回的后两个结果。\n",
    "            u_lb_pred, v_lb_pred, u_x_lb_pred, v_x_lb_pred = self.net_uv(self.x_lb, self.t_lb) #同上，不过这里函数返回的后两个结果会赋值给self.u_x_lb_pred和self.v_x_lb_pred。\n",
    "            u_ub_pred, v_ub_pred, u_x_ub_pred, v_x_ub_pred = self.net_uv(self.x_ub, self.t_ub) #同上\n",
    "            f_u_pred, f_v_pred = self.net_f_uv(self.x_f, self.t_f) #调用net_f_uv函数,将self.x_f_tf和self.t_f_tf作为参数传入,然后将返回的结果赋值给self.f_u_pred和self.f_v_pred。\n",
    "\n",
    "            loss = torch.mean((self.u0 - u0_pred) ** 2)  + \\\n",
    "                    torch.mean((self.v0 - v0_pred) ** 2) + \\\n",
    "                    torch.mean((u_lb_pred - u_ub_pred) ** 2) + \\\n",
    "                    torch.mean((v_lb_pred - v_ub_pred) ** 2) + \\\n",
    "                    torch.mean((u_x_lb_pred - u_x_ub_pred) ** 2) + \\\n",
    "                    torch.mean((v_x_lb_pred - v_x_ub_pred) ** 2) + \\\n",
    "                    torch.mean(f_u_pred ** 2) + \\\n",
    "                    torch.mean(f_v_pred ** 2)\n",
    "            \n",
    "            # Backward and optimize\n",
    "            self.optimizer_Adam.zero_grad() #清除该优化器之前计算的梯度（在PyTorch中，梯度会累积，所以在每次新的优化迭代之前，我们需要清除之前的梯度）\n",
    "            loss.backward() #被调用以计算损失函数关于神经网络参数的梯度。这个梯度将被用于优化器来更新神经网络参数\n",
    "            self.optimizer_Adam.step()  #使用之前的优化器self.optimizer_Adam，调用step方法(执行一步优化算法)，传入损失函数self.loss_func，进行优化\n",
    "            \n",
    "            if epoch % 100 == 0:\n",
    "                print(\n",
    "                    'Iter: %d, Loss: %.3e' % \n",
    "                    (\n",
    "                        epoch, \n",
    "                        loss\n",
    "                    ) #每100次迭代，打印一次迭代次数、总的loss\n",
    "                )\n",
    "\n",
    "\n",
    "        # Backward and optimize，用LBFGS优化器进行进一步优化\n",
    "        self.optimizer.step(self.loss_func)  #使用之前的优化器self.optimizer，调用step方法(执行一步优化算法)，传入计算损失函数的方法self.loss_func，进行优化   \n",
    "\n",
    "                                    \n",
    "    #定义了一个名为predict的函数/方法，用于预测神经网络的输出。这个方法接受一个参数X_star，表示输入数据。最后返回预测的两个输出和两个输出的梯度。\n",
    "    def predict(self, X):\n",
    "        x = torch.tensor(X[:, 0:1], requires_grad=True).float().to(device) #从输入中得到x和t（第一列和第二列），是张量，需要计算梯度，转换为浮点数类型，并将张量移动到指定设备上\n",
    "        t = torch.tensor(X[:, 1:2], requires_grad=True).float().to(device)\n",
    "\n",
    "        self.dnn.eval() #将神经网络切换为评估模式\n",
    "        u, v, _, _ = self.net_uv(x, t) #调用之前定义的函数得到神经网络的输出u,以及f\n",
    "        f_u, f_v = self.net_f_uv(x, t) \n",
    "\n",
    "        u = u.detach().cpu().numpy() #将张量u和v先从计算图中分离出来，然后转换为numpy数组，最后将这个数组移动到cpu上\n",
    "        v = v.detach().cpu().numpy()\n",
    "        f_u = f_u.detach().cpu().numpy()\n",
    "        f_v = f_v.detach().cpu().numpy()\n",
    "        return u, v, f_u, f_v \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0, Loss: 8.348e-01\n",
      "Iter: 100, Loss: 9.005e-02\n",
      "Iter: 200, Loss: 6.357e-02\n",
      "Iter: 300, Loss: 7.056e-02\n",
      "Iter: 400, Loss: 5.653e-02\n",
      "Iter: 500, Loss: 5.301e-02\n",
      "Iter: 600, Loss: 4.998e-02\n",
      "Iter: 700, Loss: 4.779e-02\n",
      "Iter: 800, Loss: 6.081e-02\n",
      "Iter: 900, Loss: 4.222e-02\n",
      "Iter: 1000, Loss: 4.132e-02\n",
      "Iter: 1100, Loss: 4.412e-02\n",
      "Iter: 1200, Loss: 3.790e-02\n",
      "Iter: 1300, Loss: 3.683e-02\n",
      "Iter: 1400, Loss: 3.480e-02\n",
      "Iter: 1500, Loss: 3.371e-02\n",
      "Iter: 1600, Loss: 3.384e-02\n",
      "Iter: 1700, Loss: 3.178e-02\n",
      "Iter: 1800, Loss: 1.495e-01\n",
      "Iter: 1900, Loss: 3.097e-02\n",
      "Iter: 2000, Loss: 2.922e-02\n",
      "Iter: 2100, Loss: 9.905e-02\n",
      "Iter: 2200, Loss: 2.791e-02\n",
      "Iter: 2300, Loss: 2.634e-02\n",
      "Iter: 2400, Loss: 2.588e-02\n",
      "Iter: 2500, Loss: 2.594e-02\n",
      "Iter: 2600, Loss: 2.348e-02\n",
      "Iter: 2700, Loss: 2.265e-02\n",
      "Iter: 2800, Loss: 2.231e-02\n",
      "Iter: 2900, Loss: 2.104e-02\n",
      "Iter: 3000, Loss: 2.020e-02\n",
      "Iter: 3100, Loss: 1.931e-02\n",
      "Iter: 3200, Loss: 1.838e-02\n",
      "Iter: 3300, Loss: 3.539e-02\n",
      "Iter: 3400, Loss: 1.702e-02\n",
      "Iter: 3500, Loss: 1.633e-02\n",
      "Iter: 3600, Loss: 1.599e-02\n",
      "Iter: 3700, Loss: 1.505e-02\n",
      "Iter: 3800, Loss: 1.559e-02\n",
      "Iter: 3900, Loss: 1.416e-02\n",
      "Iter: 4000, Loss: 1.310e-02\n",
      "Iter: 4100, Loss: 1.264e-02\n",
      "Iter: 4200, Loss: 1.221e-02\n",
      "Iter: 4300, Loss: 1.256e-02\n",
      "Iter: 4400, Loss: 1.524e-02\n",
      "Iter: 4500, Loss: 1.093e-02\n",
      "Iter: 4600, Loss: 1.086e-02\n",
      "Iter: 4700, Loss: 1.076e-02\n",
      "Iter: 4800, Loss: 1.014e-02\n",
      "Iter: 4900, Loss: 1.049e-02\n",
      "Iter: 5000, Loss: 1.484e-02\n",
      "Iter: 5100, Loss: 8.909e-03\n",
      "Iter: 5200, Loss: 8.757e-03\n",
      "Iter: 5300, Loss: 8.987e-03\n",
      "Iter: 5400, Loss: 8.270e-03\n",
      "Iter: 5500, Loss: 9.783e-03\n",
      "Iter: 5600, Loss: 7.792e-03\n",
      "Iter: 5700, Loss: 7.631e-03\n",
      "Iter: 5800, Loss: 8.304e-03\n",
      "Iter: 5900, Loss: 1.053e-02\n",
      "Iter: 6000, Loss: 8.518e-03\n",
      "Iter: 6100, Loss: 6.861e-03\n",
      "Iter: 6200, Loss: 6.684e-03\n",
      "Iter: 6300, Loss: 6.975e-03\n",
      "Iter: 6400, Loss: 6.429e-03\n",
      "Iter: 6500, Loss: 8.880e-03\n",
      "Iter: 6600, Loss: 6.650e-03\n",
      "Iter: 6700, Loss: 7.018e-03\n",
      "Iter: 6800, Loss: 8.885e-03\n",
      "Iter: 6900, Loss: 5.883e-03\n",
      "Iter: 7000, Loss: 7.449e-03\n",
      "Iter: 7100, Loss: 6.949e-03\n",
      "Iter: 7200, Loss: 3.480e-02\n",
      "Iter: 7300, Loss: 5.883e-03\n",
      "Iter: 7400, Loss: 6.308e-03\n",
      "Iter: 7500, Loss: 5.233e-03\n",
      "Iter: 7600, Loss: 4.732e-03\n",
      "Iter: 7700, Loss: 4.595e-03\n",
      "Iter: 7800, Loss: 1.023e-02\n",
      "Iter: 7900, Loss: 5.873e-03\n",
      "Iter: 8000, Loss: 3.754e-02\n",
      "Iter: 8100, Loss: 3.920e-03\n",
      "Iter: 8200, Loss: 3.959e-03\n",
      "Iter: 8300, Loss: 3.704e-03\n",
      "Iter: 8400, Loss: 3.737e-03\n",
      "Iter: 8500, Loss: 5.998e-03\n",
      "Iter: 8600, Loss: 5.697e-03\n",
      "Iter: 8700, Loss: 4.207e-03\n",
      "Iter: 8800, Loss: 3.259e-03\n",
      "Iter: 8900, Loss: 3.874e-03\n",
      "Iter: 9000, Loss: 3.198e-03\n",
      "Iter: 9100, Loss: 5.878e-03\n",
      "Iter: 9200, Loss: 2.907e-03\n",
      "Iter: 9300, Loss: 2.951e-03\n",
      "Iter: 9400, Loss: 2.814e-03\n",
      "Iter: 9500, Loss: 2.841e-03\n",
      "Iter: 9600, Loss: 4.880e-03\n",
      "Iter: 9700, Loss: 3.492e-03\n",
      "Iter: 9800, Loss: 2.791e-02\n",
      "Iter: 9900, Loss: 2.469e-03\n",
      "Iter: 10000, Loss: 2.429e-03\n",
      "Iter: 10100, Loss: 2.374e-03\n",
      "Iter: 10200, Loss: 2.394e-03\n",
      "Iter: 10300, Loss: 1.120e-02\n",
      "Iter: 10400, Loss: 2.228e-03\n",
      "Iter: 10500, Loss: 2.215e-03\n",
      "Iter: 10600, Loss: 2.101e-03\n",
      "Iter: 10700, Loss: 2.125e-03\n",
      "Iter: 10800, Loss: 2.107e-03\n",
      "Iter: 10900, Loss: 1.996e-03\n",
      "Iter: 11000, Loss: 1.993e-03\n",
      "Iter: 11100, Loss: 1.511e-02\n",
      "Iter: 11200, Loss: 2.023e-03\n",
      "Iter: 11300, Loss: 2.006e-03\n",
      "Iter: 11400, Loss: 1.846e-03\n",
      "Iter: 11500, Loss: 1.884e-03\n",
      "Iter: 11600, Loss: 4.138e-03\n",
      "Iter: 11700, Loss: 2.279e-03\n",
      "Iter: 11800, Loss: 2.104e-03\n",
      "Iter: 11900, Loss: 9.416e-03\n",
      "Iter: 12000, Loss: 1.603e-03\n",
      "Iter: 12100, Loss: 1.673e-03\n",
      "Iter: 12200, Loss: 1.562e-03\n",
      "Iter: 12300, Loss: 1.536e-03\n",
      "Iter: 12400, Loss: 4.692e-03\n",
      "Iter: 12500, Loss: 1.811e-03\n",
      "Iter: 12600, Loss: 2.062e-03\n",
      "Iter: 12700, Loss: 1.942e-03\n",
      "Iter: 12800, Loss: 1.817e-03\n",
      "Iter: 12900, Loss: 2.079e-03\n",
      "Iter: 13000, Loss: 1.413e-03\n",
      "Iter: 13100, Loss: 1.345e-03\n",
      "Iter: 13200, Loss: 1.306e-03\n",
      "Iter: 13300, Loss: 1.259e-03\n",
      "Iter: 13400, Loss: 1.265e-03\n",
      "Iter: 13500, Loss: 1.219e-03\n",
      "Iter: 13600, Loss: 2.215e-03\n",
      "Iter: 13700, Loss: 3.774e-03\n",
      "Iter: 13800, Loss: 2.079e-03\n",
      "Iter: 13900, Loss: 1.690e-03\n",
      "Iter: 14000, Loss: 1.284e-03\n",
      "Iter: 14100, Loss: 1.104e-02\n",
      "Iter: 14200, Loss: 1.160e-03\n",
      "Iter: 14300, Loss: 1.087e-03\n",
      "Iter: 14400, Loss: 1.502e-02\n",
      "Iter: 14500, Loss: 3.281e-03\n",
      "Iter: 14600, Loss: 3.175e-03\n",
      "Iter: 14700, Loss: 2.433e-03\n",
      "Iter: 14800, Loss: 4.869e-03\n",
      "Iter: 14900, Loss: 2.745e-02\n",
      "Iter: 15000, Loss: 1.050e-03\n",
      "Iter: 15100, Loss: 6.596e-03\n",
      "Iter: 15200, Loss: 9.259e-04\n",
      "Iter: 15300, Loss: 1.117e-03\n",
      "Iter: 15400, Loss: 5.636e-03\n",
      "Iter: 15500, Loss: 8.375e-03\n",
      "Iter: 15600, Loss: 1.969e-02\n",
      "Iter: 15700, Loss: 8.995e-04\n",
      "Iter: 15800, Loss: 1.027e-03\n",
      "Iter: 15900, Loss: 9.086e-04\n",
      "Iter: 16000, Loss: 8.443e-03\n",
      "Iter: 16100, Loss: 1.023e-03\n",
      "Iter: 16200, Loss: 8.227e-04\n",
      "Iter: 16300, Loss: 9.019e-04\n",
      "Iter: 16400, Loss: 8.407e-04\n",
      "Iter: 16500, Loss: 6.291e-03\n",
      "Iter: 16600, Loss: 1.344e-02\n",
      "Iter: 16700, Loss: 7.995e-03\n",
      "Iter: 16800, Loss: 1.122e-03\n",
      "Iter: 16900, Loss: 6.786e-03\n",
      "Iter: 17000, Loss: 2.761e-02\n",
      "Iter: 17100, Loss: 1.038e-03\n",
      "Iter: 17200, Loss: 1.140e-03\n",
      "Iter: 17300, Loss: 9.272e-03\n",
      "Iter: 17400, Loss: 6.924e-04\n",
      "Iter: 17500, Loss: 6.968e-04\n",
      "Iter: 17600, Loss: 7.037e-04\n",
      "Iter: 17700, Loss: 7.958e-04\n",
      "Iter: 17800, Loss: 8.942e-03\n",
      "Iter: 17900, Loss: 2.024e-03\n",
      "Iter: 18000, Loss: 3.518e-03\n",
      "Iter: 18100, Loss: 2.163e-03\n",
      "Iter: 18200, Loss: 1.262e-03\n",
      "Iter: 18300, Loss: 1.387e-03\n",
      "Iter: 18400, Loss: 4.664e-03\n",
      "Iter: 18500, Loss: 5.272e-03\n",
      "Iter: 18600, Loss: 3.747e-03\n",
      "Iter: 18700, Loss: 8.177e-04\n",
      "Iter: 18800, Loss: 6.930e-04\n",
      "Iter: 18900, Loss: 2.532e-03\n",
      "Iter: 19000, Loss: 7.909e-04\n",
      "Iter: 19100, Loss: 2.085e-03\n",
      "Iter: 19200, Loss: 2.338e-03\n",
      "Iter: 19300, Loss: 2.294e-03\n",
      "Iter: 19400, Loss: 7.482e-04\n",
      "Iter: 19500, Loss: 8.780e-03\n",
      "Iter: 19600, Loss: 6.326e-04\n",
      "Iter: 19700, Loss: 6.449e-04\n",
      "Iter: 19800, Loss: 7.218e-04\n",
      "Iter: 19900, Loss: 5.086e-03\n",
      "Iter: 20000, Loss: 5.713e-04\n",
      "Iter: 20100, Loss: 1.115e-02\n",
      "Iter: 20200, Loss: 3.528e-03\n",
      "Iter: 20300, Loss: 4.772e-04\n",
      "Iter: 20400, Loss: 1.232e-02\n",
      "Iter: 20500, Loss: 3.123e-02\n",
      "Iter: 20600, Loss: 5.048e-04\n",
      "Iter: 20700, Loss: 4.585e-04\n",
      "Iter: 20800, Loss: 5.350e-04\n",
      "Iter: 20900, Loss: 1.404e-03\n",
      "Iter: 21000, Loss: 4.570e-04\n",
      "Iter: 21100, Loss: 4.969e-04\n",
      "Iter: 21200, Loss: 4.362e-04\n",
      "Iter: 21300, Loss: 3.588e-03\n",
      "Iter: 21400, Loss: 5.839e-04\n",
      "Iter: 21500, Loss: 4.378e-04\n",
      "Iter: 21600, Loss: 8.915e-04\n",
      "Iter: 21700, Loss: 4.223e-04\n",
      "Iter: 21800, Loss: 4.327e-04\n",
      "Iter: 21900, Loss: 5.595e-04\n",
      "Iter: 22000, Loss: 4.405e-04\n",
      "Iter: 22100, Loss: 8.018e-03\n",
      "Iter: 22200, Loss: 5.788e-04\n",
      "Iter: 22300, Loss: 7.506e-04\n",
      "Iter: 22400, Loss: 2.192e-02\n",
      "Iter: 22500, Loss: 9.891e-04\n",
      "Iter: 22600, Loss: 4.326e-04\n",
      "Iter: 22700, Loss: 1.366e-03\n",
      "Iter: 22800, Loss: 6.964e-04\n",
      "Iter: 22900, Loss: 3.735e-04\n",
      "Iter: 23000, Loss: 4.230e-04\n",
      "Iter: 23100, Loss: 1.181e-03\n",
      "Iter: 23200, Loss: 1.307e-02\n",
      "Iter: 23300, Loss: 1.578e-03\n",
      "Iter: 23400, Loss: 5.878e-03\n",
      "Iter: 23500, Loss: 2.010e-02\n",
      "Iter: 23600, Loss: 5.963e-04\n",
      "Iter: 23700, Loss: 1.286e-03\n",
      "Iter: 23800, Loss: 9.526e-04\n",
      "Iter: 23900, Loss: 3.665e-04\n",
      "Iter: 24000, Loss: 3.384e-04\n",
      "Iter: 24100, Loss: 3.557e-04\n",
      "Iter: 24200, Loss: 4.099e-04\n",
      "Iter: 24300, Loss: 4.809e-04\n",
      "Iter: 24400, Loss: 3.330e-04\n",
      "Iter: 24500, Loss: 1.370e-03\n",
      "Iter: 24600, Loss: 9.818e-04\n",
      "Iter: 24700, Loss: 5.273e-04\n",
      "Iter: 24800, Loss: 3.422e-04\n",
      "Iter: 24900, Loss: 3.333e-04\n",
      "Iter: 25000, Loss: 1.125e-02\n",
      "Iter: 25100, Loss: 3.377e-03\n",
      "Iter: 25200, Loss: 2.431e-03\n",
      "Iter: 25300, Loss: 1.010e-03\n",
      "Iter: 25400, Loss: 3.829e-03\n",
      "Iter: 25500, Loss: 3.017e-04\n",
      "Iter: 25600, Loss: 5.368e-03\n",
      "Iter: 25700, Loss: 8.260e-04\n",
      "Iter: 25800, Loss: 6.803e-03\n",
      "Iter: 25900, Loss: 8.765e-04\n",
      "Iter: 26000, Loss: 1.838e-03\n",
      "Iter: 26100, Loss: 3.623e-03\n",
      "Iter: 26200, Loss: 3.493e-04\n",
      "Iter: 26300, Loss: 3.201e-04\n",
      "Iter: 26400, Loss: 3.717e-04\n",
      "Iter: 26500, Loss: 3.311e-04\n",
      "Iter: 26600, Loss: 2.991e-04\n",
      "Iter: 26700, Loss: 3.002e-04\n",
      "Iter: 26800, Loss: 2.924e-04\n",
      "Iter: 26900, Loss: 4.560e-04\n",
      "Iter: 27000, Loss: 2.757e-04\n",
      "Iter: 27100, Loss: 5.039e-04\n",
      "Iter: 27200, Loss: 2.799e-04\n",
      "Iter: 27300, Loss: 2.946e-04\n",
      "Iter: 27400, Loss: 3.356e-04\n",
      "Iter: 27500, Loss: 4.037e-04\n",
      "Iter: 27600, Loss: 4.443e-03\n",
      "Iter: 27700, Loss: 2.743e-04\n",
      "Iter: 27800, Loss: 4.176e-04\n",
      "Iter: 27900, Loss: 4.930e-03\n",
      "Iter: 28000, Loss: 4.557e-04\n",
      "Iter: 28100, Loss: 1.331e-02\n",
      "Iter: 28200, Loss: 1.389e-02\n",
      "Iter: 28300, Loss: 3.999e-03\n",
      "Iter: 28400, Loss: 8.043e-04\n",
      "Iter: 28500, Loss: 2.889e-04\n",
      "Iter: 28600, Loss: 8.875e-04\n",
      "Iter: 28700, Loss: 1.105e-03\n",
      "Iter: 28800, Loss: 3.157e-04\n",
      "Iter: 28900, Loss: 4.788e-03\n",
      "Iter: 29000, Loss: 4.538e-04\n",
      "Iter: 29100, Loss: 3.798e-04\n",
      "Iter: 29200, Loss: 1.038e-03\n",
      "Iter: 29300, Loss: 4.668e-04\n",
      "Iter: 29400, Loss: 4.031e-04\n",
      "Iter: 29500, Loss: 2.654e-04\n",
      "Iter: 29600, Loss: 2.371e-04\n",
      "Iter: 29700, Loss: 2.338e-04\n",
      "Iter: 29800, Loss: 2.407e-04\n",
      "Iter: 29900, Loss: 2.315e-04\n",
      "Iter: 30000, Loss: 2.351e-04\n",
      "Iter: 30100, Loss: 2.493e-04\n",
      "Iter: 30200, Loss: 3.373e-04\n",
      "Iter: 30300, Loss: 2.741e-04\n",
      "Iter: 30400, Loss: 5.423e-04\n",
      "Iter: 30500, Loss: 2.651e-03\n",
      "Iter: 30600, Loss: 3.424e-03\n",
      "Iter: 30700, Loss: 4.522e-04\n",
      "Iter: 30800, Loss: 1.073e-03\n",
      "Iter: 30900, Loss: 2.401e-04\n",
      "Iter: 31000, Loss: 3.393e-04\n",
      "Iter: 31100, Loss: 1.102e-03\n",
      "Iter: 31200, Loss: 8.672e-04\n",
      "Iter: 31300, Loss: 8.523e-04\n",
      "Iter: 31400, Loss: 1.550e-03\n",
      "Iter: 31500, Loss: 2.034e-03\n",
      "Iter: 31600, Loss: 3.690e-04\n",
      "Iter: 31700, Loss: 2.619e-04\n",
      "Iter: 31800, Loss: 2.522e-03\n",
      "Iter: 31900, Loss: 7.483e-04\n",
      "Iter: 32000, Loss: 4.227e-04\n",
      "Iter: 32100, Loss: 4.131e-04\n",
      "Iter: 32200, Loss: 1.211e-03\n",
      "Iter: 32300, Loss: 3.610e-03\n",
      "Iter: 32400, Loss: 8.121e-04\n",
      "Iter: 32500, Loss: 2.377e-04\n",
      "Iter: 32600, Loss: 2.350e-04\n",
      "Iter: 32700, Loss: 3.010e-04\n",
      "Iter: 32800, Loss: 2.585e-04\n",
      "Iter: 32900, Loss: 2.298e-03\n",
      "Iter: 33000, Loss: 1.194e-03\n",
      "Iter: 33100, Loss: 4.458e-04\n",
      "Iter: 33200, Loss: 7.033e-03\n",
      "Iter: 33300, Loss: 7.706e-03\n",
      "Iter: 33400, Loss: 4.359e-04\n",
      "Iter: 33500, Loss: 1.931e-04\n",
      "Iter: 33600, Loss: 1.920e-04\n",
      "Iter: 33700, Loss: 8.372e-03\n",
      "Iter: 33800, Loss: 1.840e-04\n",
      "Iter: 33900, Loss: 4.133e-04\n",
      "Iter: 34000, Loss: 5.045e-04\n",
      "Iter: 34100, Loss: 2.328e-04\n",
      "Iter: 34200, Loss: 2.720e-04\n",
      "Iter: 34300, Loss: 3.392e-03\n",
      "Iter: 34400, Loss: 1.306e-03\n",
      "Iter: 34500, Loss: 1.205e-03\n",
      "Iter: 34600, Loss: 2.519e-03\n",
      "Iter: 34700, Loss: 4.317e-04\n",
      "Iter: 34800, Loss: 2.421e-04\n",
      "Iter: 34900, Loss: 3.403e-04\n",
      "Iter: 35000, Loss: 1.753e-04\n",
      "Iter: 35100, Loss: 4.804e-04\n",
      "Iter: 35200, Loss: 1.361e-02\n",
      "Iter: 35300, Loss: 1.762e-04\n",
      "Iter: 35400, Loss: 3.010e-04\n",
      "Iter: 35500, Loss: 1.974e-04\n",
      "Iter: 35600, Loss: 1.718e-04\n",
      "Iter: 35700, Loss: 1.971e-04\n",
      "Iter: 35800, Loss: 1.892e-04\n",
      "Iter: 35900, Loss: 2.716e-04\n",
      "Iter: 36000, Loss: 2.919e-04\n",
      "Iter: 36100, Loss: 1.776e-04\n",
      "Iter: 36200, Loss: 2.738e-04\n",
      "Iter: 36300, Loss: 3.183e-04\n",
      "Iter: 36400, Loss: 4.607e-03\n",
      "Iter: 36500, Loss: 2.261e-04\n",
      "Iter: 36600, Loss: 1.532e-03\n",
      "Iter: 36700, Loss: 1.095e-03\n",
      "Iter: 36800, Loss: 2.976e-04\n",
      "Iter: 36900, Loss: 1.805e-04\n",
      "Iter: 37000, Loss: 1.852e-03\n",
      "Iter: 37100, Loss: 3.766e-04\n",
      "Iter: 37200, Loss: 3.153e-03\n",
      "Iter: 37300, Loss: 2.365e-04\n",
      "Iter: 37400, Loss: 3.559e-04\n",
      "Iter: 37500, Loss: 1.997e-04\n",
      "Iter: 37600, Loss: 4.219e-03\n",
      "Iter: 37700, Loss: 1.811e-04\n",
      "Iter: 37800, Loss: 1.657e-04\n",
      "Iter: 37900, Loss: 2.525e-04\n",
      "Iter: 38000, Loss: 1.714e-03\n",
      "Iter: 38100, Loss: 1.372e-03\n",
      "Iter: 38200, Loss: 1.017e-03\n",
      "Iter: 38300, Loss: 8.363e-04\n",
      "Iter: 38400, Loss: 1.879e-04\n",
      "Iter: 38500, Loss: 2.568e-04\n",
      "Iter: 38600, Loss: 8.527e-04\n",
      "Iter: 38700, Loss: 4.087e-04\n",
      "Iter: 38800, Loss: 5.505e-04\n",
      "Iter: 38900, Loss: 4.542e-03\n",
      "Iter: 39000, Loss: 5.135e-04\n",
      "Iter: 39100, Loss: 6.980e-03\n",
      "Iter: 39200, Loss: 8.869e-04\n",
      "Iter: 39300, Loss: 4.071e-03\n",
      "Iter: 39400, Loss: 8.683e-04\n",
      "Iter: 39500, Loss: 1.047e-03\n",
      "Iter: 39600, Loss: 3.640e-04\n",
      "Iter: 39700, Loss: 1.102e-03\n",
      "Iter: 39800, Loss: 9.948e-03\n",
      "Iter: 39900, Loss: 2.154e-04\n",
      "Iter: 40000, Loss: 2.146e-03\n",
      "Iter: 40100, Loss: 2.546e-04\n",
      "Iter: 40200, Loss: 1.507e-03\n",
      "Iter: 40300, Loss: 1.905e-04\n",
      "Iter: 40400, Loss: 1.555e-04\n",
      "Iter: 40500, Loss: 4.343e-04\n",
      "Iter: 40600, Loss: 1.445e-03\n",
      "Iter: 40700, Loss: 8.413e-03\n",
      "Iter: 40800, Loss: 1.358e-04\n",
      "Iter: 40900, Loss: 6.288e-04\n",
      "Iter: 41000, Loss: 1.470e-04\n",
      "Iter: 41100, Loss: 4.003e-03\n",
      "Iter: 41200, Loss: 1.319e-04\n",
      "Iter: 41300, Loss: 1.410e-04\n",
      "Iter: 41400, Loss: 3.430e-04\n",
      "Iter: 41500, Loss: 5.211e-04\n",
      "Iter: 41600, Loss: 4.989e-04\n",
      "Iter: 41700, Loss: 5.577e-04\n",
      "Iter: 41800, Loss: 2.013e-04\n",
      "Iter: 41900, Loss: 2.911e-03\n",
      "Iter: 42000, Loss: 2.726e-04\n",
      "Iter: 42100, Loss: 1.016e-03\n",
      "Iter: 42200, Loss: 8.465e-04\n",
      "Iter: 42300, Loss: 1.755e-04\n",
      "Iter: 42400, Loss: 1.050e-03\n",
      "Iter: 42500, Loss: 1.827e-04\n",
      "Iter: 42600, Loss: 1.478e-02\n",
      "Iter: 42700, Loss: 1.460e-03\n",
      "Iter: 42800, Loss: 2.455e-04\n",
      "Iter: 42900, Loss: 2.819e-04\n",
      "Iter: 43000, Loss: 3.878e-04\n",
      "Iter: 43100, Loss: 2.356e-04\n",
      "Iter: 43200, Loss: 1.935e-04\n",
      "Iter: 43300, Loss: 6.357e-04\n",
      "Iter: 43400, Loss: 2.873e-04\n",
      "Iter: 43500, Loss: 1.919e-04\n",
      "Iter: 43600, Loss: 3.175e-04\n",
      "Iter: 43700, Loss: 3.142e-04\n",
      "Iter: 43800, Loss: 2.466e-03\n",
      "Iter: 43900, Loss: 6.704e-04\n",
      "Iter: 44000, Loss: 1.553e-04\n",
      "Iter: 44100, Loss: 7.884e-04\n",
      "Iter: 44200, Loss: 2.047e-03\n",
      "Iter: 44300, Loss: 4.516e-04\n",
      "Iter: 44400, Loss: 2.509e-04\n",
      "Iter: 44500, Loss: 3.452e-04\n",
      "Iter: 44600, Loss: 2.511e-04\n",
      "Iter: 44700, Loss: 1.759e-03\n",
      "Iter: 44800, Loss: 1.301e-04\n",
      "Iter: 44900, Loss: 2.602e-04\n",
      "Iter: 45000, Loss: 1.786e-03\n",
      "Iter: 45100, Loss: 1.114e-04\n",
      "Iter: 45200, Loss: 1.212e-04\n",
      "Iter: 45300, Loss: 6.737e-04\n",
      "Iter: 45400, Loss: 4.855e-04\n",
      "Iter: 45500, Loss: 2.229e-04\n",
      "Iter: 45600, Loss: 4.337e-04\n",
      "Iter: 45700, Loss: 3.260e-04\n",
      "Iter: 45800, Loss: 5.299e-04\n",
      "Iter: 45900, Loss: 3.032e-04\n",
      "Iter: 46000, Loss: 4.997e-04\n",
      "Iter: 46100, Loss: 1.784e-04\n",
      "Iter: 46200, Loss: 1.273e-03\n",
      "Iter: 46300, Loss: 8.936e-04\n",
      "Iter: 46400, Loss: 8.601e-04\n",
      "Iter: 46500, Loss: 1.633e-04\n",
      "Iter: 46600, Loss: 7.404e-04\n",
      "Iter: 46700, Loss: 4.149e-04\n",
      "Iter: 46800, Loss: 3.193e-04\n",
      "Iter: 46900, Loss: 1.519e-04\n",
      "Iter: 47000, Loss: 1.379e-04\n",
      "Iter: 47100, Loss: 1.260e-03\n",
      "Iter: 47200, Loss: 2.163e-04\n",
      "Iter: 47300, Loss: 1.030e-03\n",
      "Iter: 47400, Loss: 9.077e-04\n",
      "Iter: 47500, Loss: 1.117e-03\n",
      "Iter: 47600, Loss: 2.260e-04\n",
      "Iter: 47700, Loss: 5.842e-03\n",
      "Iter: 47800, Loss: 1.432e-03\n",
      "Iter: 47900, Loss: 4.880e-03\n",
      "Iter: 48000, Loss: 3.583e-04\n",
      "Iter: 48100, Loss: 3.592e-04\n",
      "Iter: 48200, Loss: 1.513e-04\n",
      "Iter: 48300, Loss: 2.246e-03\n",
      "Iter: 48400, Loss: 1.398e-04\n",
      "Iter: 48500, Loss: 2.619e-04\n",
      "Iter: 48600, Loss: 1.525e-04\n",
      "Iter: 48700, Loss: 4.444e-03\n",
      "Iter: 48800, Loss: 5.426e-04\n",
      "Iter: 48900, Loss: 2.043e-04\n",
      "Iter: 49000, Loss: 1.185e-04\n",
      "Iter: 49100, Loss: 1.057e-04\n",
      "Iter: 49200, Loss: 9.070e-03\n",
      "Iter: 49300, Loss: 6.243e-04\n",
      "Iter: 49400, Loss: 2.567e-04\n",
      "Iter: 49500, Loss: 1.143e-04\n",
      "Iter: 49600, Loss: 2.738e-03\n",
      "Iter: 49700, Loss: 1.586e-03\n",
      "Iter: 49800, Loss: 3.241e-03\n",
      "Iter: 49900, Loss: 1.204e-04\n",
      "Iter 100, Loss: 7.979407e-05\n",
      "Iter 200, Loss: 6.680255e-05\n",
      "Iter 300, Loss: 5.537261e-05\n",
      "Iter 400, Loss: 4.698071e-05\n",
      "Iter 500, Loss: 4.082020e-05\n",
      "Iter 600, Loss: 3.527974e-05\n",
      "Iter 700, Loss: 3.144929e-05\n",
      "Iter 800, Loss: 2.763765e-05\n",
      "Iter 900, Loss: 2.461670e-05\n",
      "Iter 1000, Loss: 2.223214e-05\n",
      "Iter 1100, Loss: 2.012432e-05\n",
      "Iter 1200, Loss: 1.824622e-05\n",
      "Iter 1300, Loss: 1.645183e-05\n",
      "Iter 1400, Loss: 1.500688e-05\n",
      "Iter 1500, Loss: 1.383284e-05\n",
      "Iter 1600, Loss: 1.290469e-05\n",
      "Iter 1700, Loss: 1.208959e-05\n",
      "Iter 1800, Loss: 1.125992e-05\n",
      "Iter 1900, Loss: 1.049772e-05\n",
      "Iter 2000, Loss: 9.881243e-06\n",
      "Iter 2100, Loss: 9.377770e-06\n",
      "Iter 2200, Loss: 8.960692e-06\n",
      "Iter 2300, Loss: 8.496485e-06\n",
      "Iter 2400, Loss: 8.103922e-06\n",
      "Iter 2500, Loss: 7.773571e-06\n",
      "Iter 2600, Loss: 7.481488e-06\n",
      "Iter 2700, Loss: 7.207986e-06\n",
      "Iter 2800, Loss: 6.964718e-06\n",
      "Iter 2900, Loss: 6.717809e-06\n",
      "Iter 3000, Loss: 6.442413e-06\n",
      "Iter 3100, Loss: 6.215160e-06\n",
      "Iter 3200, Loss: 6.005987e-06\n",
      "Iter 3300, Loss: 5.833299e-06\n",
      "Iter 3400, Loss: 5.667364e-06\n",
      "Iter 3500, Loss: 5.504092e-06\n",
      "Iter 3600, Loss: 5.355239e-06\n",
      "Iter 3700, Loss: 5.206333e-06\n",
      "Iter 3800, Loss: 5.086566e-06\n",
      "Iter 3900, Loss: 4.968029e-06\n",
      "Iter 4000, Loss: 4.834241e-06\n",
      "Iter 4100, Loss: 4.719815e-06\n",
      "Iter 4200, Loss: 4.612077e-06\n",
      "Iter 4300, Loss: 4.520527e-06\n",
      "Iter 4400, Loss: 4.438227e-06\n",
      "Iter 4500, Loss: 4.371202e-06\n",
      "Training time: 673.9115\n",
      "Error u: 7.002664e-03\n",
      "Error v: 1.550907e-02\n",
      "Error h: 5.421739e-03\n"
     ]
    }
   ],
   "source": [
    "#设置噪声值为0 \n",
    "noise = 0.0        \n",
    "\n",
    "# Doman bounds，定义两个一维数组lb和ub，问题域是一个二维空间，其中 x 的范围是 -5 到 5，t 的范围是 0 到 π/2(竖着的)\n",
    "lb = np.array([-5.0, 0.0])\n",
    "ub = np.array([5.0, np.pi/2])\n",
    "#定义三个整数，分别表示初始条件点数量、边界条件点数量和在问题域内部的点的数量（这些点用于训练神经网络）\n",
    "N0 = 50\n",
    "N_b = 50\n",
    "N_f = 20000\n",
    "#定义一个列表layers，其中包含了神经网络的层数和每一层的神经元数量\n",
    "layers = [2, 100, 100, 100, 100, 2]\n",
    "#读取名为NLS.mat的Matlab文件，文件中的数据存储在data变量中。这里的路径也要随着设备的情况修改    \n",
    "data = scipy.io.loadmat('../Data/NLS.mat')\n",
    "#从data字典中取出变量tt和x的值，并转换为一维数组（flatten方法），最后tongg[:,None]将一维数组转换为二维数组\n",
    "t = data['tt'].flatten()[:,None]\n",
    "x = data['x'].flatten()[:,None]\n",
    "Exact = data['uu'] #从data字典中取出变量uu的值，并赋值给Exact\n",
    "Exact_u = np.real(Exact)  #取Exact的实部，赋值给Exact_u\n",
    "Exact_v = np.imag(Exact)  #取Exact的虚部，赋值给Exact_v\n",
    "Exact_h = np.sqrt(Exact_u**2 + Exact_v**2) #计算复数uu的|uu|\n",
    "#生成一个二位网络，X和T是输出的二维数组\n",
    "X, T = np.meshgrid(x,t)\n",
    "\n",
    "X_star = np.hstack((X.flatten()[:,None], T.flatten()[:,None]))  #X_star是一个二维数组，其中第一列是X的展平，第二列是T的展平\n",
    "u_star = Exact_u.T.flatten()[:,None] #先对Exact_u进行转置，然后使用flatten方法将其转换为一维数组，最后使用[:,None]将其转换为二维数组\n",
    "v_star = Exact_v.T.flatten()[:,None] #同上，比如Exact_v是m*n二维数组，Exact_v.T是n*m二维数组，Exact_v.T.flatten()是一个长度为n*m的一维数组，Exact_v.T.flatten()[:,None]是一个(n*m)*1的三维数组\n",
    "h_star = Exact_h.T.flatten()[:,None]\n",
    "#上面五行代码的意义见Numpy库的索引的介绍\n",
    "\n",
    "\n",
    "###########################\n",
    "\n",
    "#从0~数组x的行数(256)中随机选择N0个数，replace=False表示不允许重复选择，最后将这N0个数赋值给idx_x\n",
    "idx_x = np.random.choice(x.shape[0], N0, replace=False)\n",
    "#从x中选择N0个对应的行(idx_x对应的行)，最后将这N0行赋值给x0\n",
    "x0 = x[idx_x,:]\n",
    "#从Exact_u中选择N0个对应的行(idx_x对应的行)的第一列元素，最后将这N0个元素赋值给u0\n",
    "u0 = Exact_u[idx_x,0:1]\n",
    "v0 = Exact_v[idx_x,0:1]\n",
    "#从0~数组t的行数中随机选择N_b个数，replace=False表示不允许重复选择，最后将这N_b个数赋值给idx_t\n",
    "idx_t = np.random.choice(t.shape[0], N_b, replace=False)\n",
    "#从t中选择N_b个对应的行(idx_t对应的行)，最后将这N_b行赋值给tb\n",
    "tb = t[idx_t,:]\n",
    "\n",
    "X_f = lb + (ub-lb)*lhs(2, N_f) #lhs函数采用拉丁超采样方法，生成一个近似均匀分布的多维样本点集，返回的是一个形状为（$N_f$，2）的数组，每一行都是一个2维的样本点，所有样本点都在[0,1]范围内，并对该样本集进行缩放，把每个样本从[0,1]区间缩放到[lb,ub]区域内，即得到了指定范围内均匀分布的样本$X_f$。\n",
    "\n",
    "#创建PINN模型并输入各种参数        \n",
    "model = PhysicsInformedNN(x0, u0, v0, tb, X_f, layers, lb, ub)\n",
    "#获取当前时间并赋值给start_time          \n",
    "start_time = time.time()       \n",
    "#训练模型50000次         \n",
    "model.train(50000)\n",
    "#获取当前时间并减去start_time，得到训练时间并赋值给elapsed\n",
    "elapsed = time.time() - start_time                \n",
    "#打印训练所需时间\n",
    "print('Training time: %.4f' % (elapsed))\n",
    "\n",
    "#用训练好的模型进行预测，返回四个值（均为数组）    \n",
    "u_pred, v_pred, f_u_pred, f_v_pred = model.predict(X_star)\n",
    "#计算u_pred和v_pred的模（平方和的平方根），赋值给h_pred\n",
    "h_pred = np.sqrt(u_pred**2 + v_pred**2)\n",
    "#计算误差（基于2范数）        \n",
    "error_u = np.linalg.norm(u_star-u_pred,2)/np.linalg.norm(u_star,2)\n",
    "error_v = np.linalg.norm(v_star-v_pred,2)/np.linalg.norm(v_star,2)\n",
    "error_h = np.linalg.norm(h_star-h_pred,2)/np.linalg.norm(h_star,2)\n",
    "#打印误差\n",
    "print('Error u: %e' % (error_u))\n",
    "print('Error v: %e' % (error_v))\n",
    "print('Error h: %e' % (error_h))\n",
    "\n",
    "#使用griddata函数将X_star、u_pred、v_pred和h_pred插值到网格上，得到U_pred、V_pred和H_pred\n",
    "U_pred = griddata(X_star, u_pred.flatten(), (X, T), method='cubic')\n",
    "V_pred = griddata(X_star, v_pred.flatten(), (X, T), method='cubic')\n",
    "H_pred = griddata(X_star, h_pred.flatten(), (X, T), method='cubic')\n",
    "#同上，使用griddata函数将X_star、f_u_pred和f_v_pred插值到网格上，得到FU_pred和FV_pred\n",
    "FU_pred = griddata(X_star, f_u_pred.flatten(), (X, T), method='cubic')\n",
    "FV_pred = griddata(X_star, f_v_pred.flatten(), (X, T), method='cubic')     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lcy\\AppData\\Local\\Temp\\ipykernel_94376\\601481680.py:17: MatplotlibDeprecationWarning: Auto-removal of overlapping axes is deprecated since 3.6 and will be removed two minor releases later; explicitly call ax.remove() as needed.\n",
      "  ax = plt.subplot(gs0[:,:]) #在gs0[:,:] 指定的位置创建了一个子图，并将返回的axes对象赋值给ax。gs0[:,:]表示GridSpec对象gs0的所有行和所有列，所以这行代码创建的子图占据了整个图形。\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, '$t = 0.98$')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 539.643x300.166 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "######################################################################\n",
    "############################# Plotting ###############################\n",
    "######################################################################    \n",
    "#调用concatenate函数拼接数组\n",
    "X0 = np.concatenate((x0, 0*x0), 1) # (x0, 0)\n",
    "X_lb = np.concatenate((0*tb + lb[0], tb), 1) # (lb[0], tb)\n",
    "X_ub = np.concatenate((0*tb + ub[0], tb), 1) # (ub[0], tb)\n",
    "X_u_train = np.vstack([X0, X_lb, X_ub])  #(X0;X_lb;X_ub)\n",
    "#调用plotting文件中的newfig函数，生成一个宽1英寸、高0.9英寸的图像fig和子图ax\n",
    "fig, ax = newfig(1.0, 0.9) #这里ax是一个axes对象，代表子图，figure是一个figure对象，是一个图形窗口，代表整个图形\n",
    "ax.axis('off') #关闭子图的轴的显示\n",
    "\n",
    "####### Row 0: h(t,x)，绘制第一个子图，展示x,t和|h(t,x)|的关系##################    \n",
    "#创建一个包含子图的网格，1行2列\n",
    "gs0 = gridspec.GridSpec(1,2)  #创建一个1×2的网络，用于存放子图\n",
    "gs0.update(top=1-0.06, bottom=1-1/3, left=0.15, right=0.85, wspace=0) #更新该网络的参数，第一个表示子图的顶部位置为0.94，第二个参数表示子图的底部位置为0.667，第三个表示子图左侧的位置为0.15，第四个参数表示子图的右侧位置为0.85，第五个参数表示子图之间的宽度，0表示子图之间没有空隙\n",
    "ax = plt.subplot(gs0[:,:]) #在gs0[:,:] 指定的位置创建了一个子图，并将返回的axes对象赋值给ax。gs0[:,:]表示GridSpec对象gs0的所有行和所有列，所以这行代码创建的子图占据了整个图形。\n",
    "\n",
    "#绘制热图\n",
    "h = ax.imshow(H_pred.T, interpolation='nearest', cmap='YlGnBu', \n",
    "                extent=[lb[1], ub[1], lb[0], ub[0]], \n",
    "                origin='lower', aspect='auto')  #imshow函数用于显示图像，接受一些参数，第一个参数是图像数据，这里是H_pred的转置；第二个参数是插值方法（用于在像素之间插入新的像素），这里是最邻近插值；\n",
    "                                                #第三个参数是颜色映射，这里是从黄色Yl到绿色Gn再到蓝色Bu；第四个参数是图像的范围，这里lb和ub分别是数据的下界和上界；第五个参数是图像的原点位置，这里表示原点在右下角；第六个参数是图像的纵横比，这里表示调整横纵比以填充整个axes对象\n",
    "                                                #最后的结果返回一个axesimage对象，也就是h，可以通过这个对象进一步设置图像的属性\n",
    "divider = make_axes_locatable(ax)  #使用 make_axes_locatable 函数创建了一个 AxesDivider 对象。这个函数接受一个 Axes 对象作为参数，返回一个 AxesDivider 对象。AxesDivider 对象可以用来管理子图的布局，特别是当你需要在一个图的旁边添加另一个图时。\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05) #使用append_axes方法在原始轴的右侧添加了一个新的轴。append_axes 方法接受三个参数：位置（\"right\"）、大小（\"5%\"）和间距（0.05）。在原始轴的右侧添加了一个新的轴，新轴的大小是原始轴的 5%，新轴与原始轴之间的间距是 0.05 英寸\n",
    "fig.colorbar(h, cax=cax) #使用colorbar方法在新轴上添加了一个颜色条。colorbar 方法接受两个参数：axesimage 对象（h）和新轴（cax）。\n",
    "\n",
    "#绘制散点图\n",
    "ax.plot(X_u_train[:,1], X_u_train[:,0], 'kx', label = 'Data (%d points)' % (X_u_train.shape[0]), markersize = 4, clip_on = False) #在ax上绘制散点图，前两个参数是散点的x坐标和y坐标；kx表示黑色的x（散点形状是x），label是散点的标签，clip_on表示散点可以绘制在轴的边界外\n",
    "\n",
    "\n",
    "#绘制三条虚线\n",
    "line = np.linspace(x.min(), x.max(), 2)[:,None] #生成了一个包含2个等间距的数值的数组，这些数值在 x.min() 到 x.max() 之间。[:,None] 是一个索引操作，用于将一维数组转换为二维数组。这里其实就是[-5;5]\n",
    "#第一个参数是虚线的x坐标，line是虚线y的坐标，第三个参数是虚线的样式，k表示黑色，--表示虚线，最后一个参数表示虚线的参数是1\n",
    "ax.plot(t[75]*np.ones((2,1)),line,'k--',linewidth=1) \n",
    "ax.plot(t[100]*np.ones((2,1)),line,'k--',linewidth=1)\n",
    "ax.plot(t[125]*np.ones((2,1)),line,'k--',linewidth=1)    \n",
    "\n",
    "#设置标签\n",
    "#设置ax子图的x轴的标签为t，y轴的标签为x。这里$t$和$x$是latex格式的文本，用于生成数学公式\n",
    "ax.set_xlabel('$t$')\n",
    "ax.set_ylabel('$x$')\n",
    "#设置子图ax的图例，frameon=False表示不显示图例的边框，loc='best'表示图例的位置是最佳位置，最后返回的leg是一个legend对象，表示图形的图例\n",
    "leg = ax.legend(frameon=False, loc = 'best')\n",
    "#    plt.setp(leg.get_texts(), color='w')   #用来设置图例中文本的颜色，这里是白色，取消注释后文本会变为白色\n",
    "ax.set_title('$|h(t,x)|$', fontsize = 10) #设置子图ax的标题为$|h(t,x)|$，表示latex格式的文本，用于生成数学公式，fontsize=10表示字体大小为10\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "####### Row 1: h(t,x) slices ##################    \n",
    "gs1 = gridspec.GridSpec(1,3) #创建一个1×3的网络，用于存放子图\n",
    "gs1.update(top=1-1/3, bottom=0, left=0.1, right=0.9, wspace=0.5) #更新该网络的参数，第一个表示子图的顶部位置为0.667，第二个参数表示子图的底部位置为0，第三个表示子图左侧的位置为0，第四个参数表示子图的右侧位置为0.9，第五个参数表示子图之间的宽度为0.5\n",
    "\n",
    "ax = plt.subplot(gs1[0,0])  #在gs1[0,0]指定的位置，也就是网格的第一行第一列，创建了一个子图，并将返回的axes对象赋值给ax。\n",
    "#绘制了两条线，一条表示精确值，一条表示预测值\n",
    "ax.plot(x,Exact_h[:,75], 'b-', linewidth = 2, label = 'Exact')      #第一个参数表示x轴上的坐标；第二个参数表示y轴上的坐标；第三个参数b-表示蓝色的实线；linewidth表示线的宽度为2；label表示线的标签\n",
    "ax.plot(x,H_pred[75,:], 'r--', linewidth = 2, label = 'Prediction') #同上\n",
    "#设置ax子图的x轴的标签为x，y轴的标签为|h(t,x)|。这里$x$和$|h(t,x)|$是latex格式的文本，用于生成数学公式\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$|h(t,x)|$')    \n",
    "#设置子图的标题，几个子图标题随着t的变化而变化，字体大小为10 \n",
    "ax.set_title('$t=%.2f$' % (t[75]), fontsize = 10)\n",
    "ax.axis('square') #设置子图的纵横比，使得x轴和y轴的单位长度相等，形成一个正方形的区域\n",
    "ax.set_xlim([-5.1,5.1]) #第一个子图的x轴范围是-5.1到5.1\n",
    "ax.set_ylim([-0.1,5.1]) #第一个子图的y轴范围是-0.1到5.1\n",
    "\n",
    "ax = plt.subplot(gs1[0, 1]) #在gs1[0,1]指定的位置，也就是网格的第一行第二列，创建了一个子图，并将返回的axes对象赋值给ax。\n",
    "#绘制了两条线，一条表示精确值，一条表示预测值\n",
    "ax.plot(x,Exact_h[:,100],'b-', linewidth = 2, label = 'Exact')        #第一个参数表示x轴上的坐标；第二个参数表示y轴上的坐标；第三个参数b-表示蓝色的实线；linewidth表示线的宽度为2；label表示线的标签\n",
    "ax.plot(x,H_pred[100,:],'r--', linewidth = 2, label = 'Prediction')   #同上\n",
    "ax.set_xlabel('$x$') #设置子图的x轴的标签为x\n",
    "ax.set_ylabel('$|h(t,x)|$') #设置子图的y轴的标签为|h(t,x)|\n",
    "ax.axis('square')   #设置子图的纵横比，使得x轴和y轴的单位长度相等，形成一个正方形的区域\n",
    "ax.set_xlim([-5.1,5.1])     #第二个子图的x轴范围是-5.1到5.1\n",
    "ax.set_ylim([-0.1,5.1])     #第二个子图的y轴范围是-0.1到5.1\n",
    "ax.set_title('$t = %.2f$' % (t[100]), fontsize = 10)        #设置第二个子图的标题，标题随着t的变化而变化，字体大小为10\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.8), ncol=5, frameon=False)  #设置第二个子图的图例，loc='upper center'表示图例的位置是上方中心，bbox_to_anchor=(0.5,-0.8)表示图例的中心位置是在子图的中间偏下方0.8的位置，ncol=5表示图例的列数是5，frameon=False表示不显示图例的边框\n",
    "\n",
    "ax = plt.subplot(gs1[0, 2]) #在gs1[0,2]指定的位置，也就是网格的第一行第三列，创建了一个子图，并将返回的axes对象赋值给ax。\n",
    "ax.plot(x,Exact_h[:,125], 'b-', linewidth = 2, label = 'Exact')        #第一个参数表示x轴上的坐标；第二个参数表示y轴上的坐标；第三个参数b-表示蓝色的实线；linewidth表示线的宽度为2；label表示线的标签\n",
    "ax.plot(x,H_pred[125,:], 'r--', linewidth = 2, label = 'Prediction')    #同上\n",
    "ax.set_xlabel('$x$') #设置子图的x轴的标签为x\n",
    "ax.set_ylabel('$|h(t,x)|$') #设置子图的y轴的标签为|h(t,x)|\n",
    "ax.axis('square')    #设置子图的纵横比，使得x轴和y轴的单位长度相等，形成一个正方形的区域\n",
    "ax.set_xlim([-5.1,5.1])    #第三个子图的x轴范围是-5.1到5.1\n",
    "ax.set_ylim([-0.1,5.1])    #第三个子图的y轴范围是-0.1到5.1\n",
    "ax.set_title('$t = %.2f$' % (t[125]), fontsize = 10)    #设置第三个子图的标题，标题随着t的变化而变化，字体大小为10\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorchgpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
