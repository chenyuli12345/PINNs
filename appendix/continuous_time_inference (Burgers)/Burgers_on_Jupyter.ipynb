{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\cheny\\AppData\\Local\\Temp\\ipykernel_14588\\3719892129.py:31: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#-*-coding:utf-8-*-\n",
    "\n",
    "#import the necessary libraries, set the gpu and random seed\n",
    "\n",
    "#下面这行代码，是为了把自己编写的代码文件当作一共模块导入，这里是把Utilities文件夹中的plotting.py文件当作python的模块导入，对应的是下面的from plotting import newfig, savefig。路径要随着不同设备的系统做相应的修改\n",
    "import sys #导入sys模块。sys模块提供了一些变量和函数，用于与 Python解释器进行交互和访问。例如，sys.path 是一个 Python 在导入模块时会查找的路径列表，sys.argv 是一个包含命令行参数的列表，sys.exit() 函数可以用于退出 Python 程序。导入 sys 模块后，你就可以在你的程序中使用这些变量和函数了。\n",
    "sys.path.insert(0, 'C:/Users/cheny/Documents/GitHub/PINNs/Utilities/')  #在 Python的sys.path列表中插入一个新的路径。sys.path是一个 Python 在导入模块时会查找的路径列表。新的路径'../../Utilities/'相对于当前脚本的路径。当你尝试导入一个模块时，Python 会在 sys.path 列表中的路径下查找这个模块。通过在列表开始位置插入一个路径，你可以让 Python 优先在这个路径下查找模块。这在你需要导入自定义模块或者不在 Python 标准库中的模块时非常有用。\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#下面的`scipy`是一个用于科学计算和技术计算的Python库，提供了许多高级的数学函数和便利的操作，包括数值积分、插值、优化、图像处理、统计等。\n",
    "import scipy.io #导入了scipy库中的io模块。scipy.io模块包含了一些用于文件输入/输出的函数，例如读取和写入.mat文件（MATLAB格式）。\n",
    "from scipy.interpolate import griddata #`scipy.interpolate`是`scipy`库中的一个模块，提供了许多插值工具，用于在给定的离散数据点之间进行插值和拟合。`griddata`是这个模块中的一个函数，用于在无规则的数据点上进行插值。\n",
    "from pyDOE import lhs #`pyDOE`是一个Python库，用于设计实验。它提供了一些函数来生成各种设计，如因子设计、拉丁超立方设计等。`lhs`是库中的一个函数，全名为\"Latin Hypercube Sampling\"，拉丁超立方采样。这是一种统计方法，用于生成一个近似均匀分布的多维样本点集。它在参数空间中生成一个非常均匀的样本，这对于高维数值优化问题非常有用，因为它可以更好地覆盖参数空间。\n",
    "from plotting import newfig, savefig  #从自定义的plotting.py文件中导入了newfig和savefig函数。这两个函数用于创建和保存图形。这两个函数的定义在plotting.py文件中\n",
    "from mpl_toolkits.mplot3d import Axes3D #`mpl_toolkits.mplot3d`是`matplotlib`库的一个模块，用于创建三维图形。`Axes3D`是`mpl_toolkits.mplot3d`模块中的一个类，用于创建一个三维的坐标轴。可以在这个坐标轴上绘制三维的图形，如曲线、曲面等。\n",
    "import time #一个内置模块，用于处理时间相关的操作。\n",
    "import matplotlib.gridspec as gridspec #是`matplotlib`库的一个模块，用于创建一个网格布局来放置子图。在`matplotlib`中可以创建一个或多个子图（subplot），每个子图都有自己的坐标轴，并可以在其中绘制图形。`gridspec`模块提供了一个灵活的方式来创建和放置子图。\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable #`mpl_toolkits.axes_grid1`是`matplotlib`库的一个模块，提供了一些高级的工具来控制matplotlib图形中的坐标轴和颜色条。`make_axes_locatable`是模块中的一个函数，用于创建一个可分割的坐标轴。可以在这个坐标轴的四个方向（上、下、左、右）添加新的坐标轴或颜色条。\n",
    "\n",
    "\n",
    "\n",
    "# #下面两行是指定索引为哪一块的gpu进行训练，这里使用索引为1的，即第二块gpu\n",
    "# import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1，0\"\n",
    "\n",
    "\n",
    "#NumPy和TensorFlow都有自己的随机数生成器，它们是独立的，互不影响。也就是说，设置NumPy的随机数种子不会影响TensorFlow的随机数生成，反之亦然\n",
    "np.random.seed(1234) #设置了NumPy的随机数生成器的种子。设置随机数生成器的种子可以确保每次运行程序时，NumPy生成的随机数序列都是一样的。\n",
    "tf.set_random_seed(1234) #设置了TensorFlow的随机数生成器的种子。设置随机数生成器的种子可以确保每次运行程序时，TensorFlow生成的随机数序列都是一样的\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the class of PINN\n",
    "\n",
    "#定义了一个名为`PhysicsInformedNN'的类，用于实现基于物理的神经网络。\n",
    "class PhysicsInformedNN:\n",
    "    # Initialize the class\n",
    "    def __init__(self, X_u, u, X_f, layers, lb, ub, nu): #这个类包含的第一个方法__init__，这是一个特殊的方法，也就是这个类的构造函数，用于初始化新创建的对象，接受了几个参数\n",
    "        \n",
    "        #Python使用self关键字来表示类的实例。当在类的方法中定义一个变量时，例如lb和ub，这些变量只在该方法内部可见，也就是说它们的作用域仅限于该方法。当方法执行完毕后，这些变量就会被销毁，无法在其他方法中访问它们。但如果希望在类的其他方法中也能访问这些变量就需要将它们保存为类的实例属性。这就是self.lb和self.ub的作用。\n",
    "            #通过将lb和ub赋值给self.lb和self.ub，就可以在类的其他方法中通过self.lb和self.ub来访问这些值。总的来说，self.lb和self.ub是类的实例属性，它们的作用域是整个类，而不仅仅是定义它们的方法。\n",
    "        self.lb = lb  #将传入的lb和ub参数的值存储在实例中，以便后续使用。这样可以在类的其他方法中通过self.lb和self.ub来访问这些值。\n",
    "        self.ub = ub\n",
    "    \n",
    "        self.x_u = X_u[:,0:1] #将X_u的第一列赋值给self.x_u，将X_u的第二列赋值给self.t_u。\n",
    "        self.t_u = X_u[:,1:2]\n",
    "        \n",
    "        self.x_f = X_f[:,0:1] #将X_f的第一列赋值给self.x_f，将X_f的第二列赋值给self.t_f。\n",
    "        self.t_f = X_f[:,1:2]\n",
    "        \n",
    "        self.u = u\n",
    "        \n",
    "        self.layers = layers #将传入的layers参数的值存储在实例中，以便后续使用。这样可以在类的其他方法中通过self.layers来访问这些值。\n",
    "        self.nu = nu\n",
    "        \n",
    "        # Initialize NNs\n",
    "        self.weights, self.biases = self.initialize_NN(layers) #调用了initialize_NN方法，用于初始化神经网络的权重和偏置。这个方法接受一个参数layers，它是一个列表，包含了神经网络的层数和每一层的神经元数量。例如，layers=[2, 100, 100, 100, 100, 2]表示神经网络有5个隐藏层，每个隐藏层有100个神经元，输入层和输出层分别有2个神经元。这个方法返回了神经网络的权重和偏置（具体见下面），分别存储在self.weights和self.biases中。\n",
    "        \n",
    "        # tf placeholders and graph\n",
    "\n",
    "        #使用tf.Session创建了一个名为tf.sess的会话。接受一个config参数，这是一个tf.ConfigProto对象，用于设置会话的配置选项。这里设置了两个选项：\n",
    "              #allow_soft_placement：如果设置为True，那么当某些操作无法在GPU上执行时，TensorFlow会自动将它们放在CPU上执行；\n",
    "              #log_device_placement：如果设置为True，那么在日志中会记录每个节点被安排在哪个设备上执行\n",
    "        self.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True,\n",
    "                                                     log_device_placement=True))\n",
    "        \n",
    "        #使用TensorFlow库创建占位符，用于存储输入和输出数据。占位符是TensorFlow中的一个特殊对象，允许在运行时将数据传递给TensorFlow计算图。可以将占位符看作是一个变量，但不需要提供初始值。相反只需要在运行计算图时提供一个值。使用feed_dict参数来为占位符提供值。\n",
    "            #例如，如果x是一个占位符，可以使用feed_dict={x: 1.0}来为它提供值。这个值可以是一个单独的数字，也可以是一个数组。        \n",
    "        #每个占位符都使用了tf.placeholder函数进行创建。该函数接受两个参数,第一个是数据类型,这里都是tf.float32,表示占位符的数据类型是浮点数。第二个参数是形状,这里都是[None,self.x0.shape[1]]这样的形式，其中None表示这一维的长度可以是任意的，self.x0.shape[1]表示这一维的长度是self.x0的列数。\n",
    "        self.x_u_tf = tf.placeholder(tf.float32, shape=[None, self.x_u.shape[1]])\n",
    "        self.t_u_tf = tf.placeholder(tf.float32, shape=[None, self.t_u.shape[1]])        \n",
    "        self.u_tf = tf.placeholder(tf.float32, shape=[None, self.u.shape[1]])\n",
    "        \n",
    "        self.x_f_tf = tf.placeholder(tf.float32, shape=[None, self.x_f.shape[1]])\n",
    "        self.t_f_tf = tf.placeholder(tf.float32, shape=[None, self.t_f.shape[1]])        \n",
    "\n",
    "        # tf Graphs，这里是使用TensorFlow库进行神经网络前向传播的部分（预测部分）。        \n",
    "        self.u_pred = self.net_u(self.x_u_tf, self.t_u_tf) #是调用net_u函数,将self.x_u_tf和self.t_u_tf作为参数传入,然后将返回的结果赋值给self.u_pred\n",
    "        self.f_pred = self.net_f(self.x_f_tf, self.t_f_tf) #是调用net_f函数,将self.x_f_tf和self.t_f_tf作为参数传入,然后将返回的结果赋值给self.f_pred\n",
    "\n",
    "\n",
    "        # Loss，这里是使用TensorFlow库计算损失函数的部分，训练目标是最小化损失函数，这里的损失函数由两部分组成，分别是初始条件、边界条件、微分方程两边的残差。每一部分都是预测值与真实值之间的差的平方的均值（均方误差）\n",
    "        #tf.reduce_mean是TensorFlow库中的一个函数，用于计算张量的均值。它接受一个参数，即张量，可以是一个一维数组，也可以是一个多维数组。它会返回一个标量，即这个张量的均值。\n",
    "        #tf.square是TensorFlow库中的一个函数，用于计算张量的平方。它接受一个参数，即张量，可以是一个一维数组，也可以是一个多维数组。它会返回一个与输入张量形状相同的张量，其中每个元素都是输入张量对应元素的平方。\n",
    "        #这里的+ \\表示将两行代码连接成一行，这是Python中的行连接符，用于将一行代码分成多行书写。\n",
    "        self.loss = tf.reduce_mean(tf.square(self.u_tf - self.u_pred)) + \\\n",
    "                    tf.reduce_mean(tf.square(self.f_pred))\n",
    "               \n",
    "\n",
    "        # Optimizers，这里是使用TensorFlow库进行优化的部分，使用了两种优化器，分别是L-BFGS-B和Adam。L-BFGS-B是一种基于梯度的优化方法，它使用了拟牛顿法来寻找损失函数的最小值。Adam是一种基于梯度的优化方法，它使用了自适应学习率来寻找损失函数的最小值\n",
    "\n",
    "        #首先用tf.contrib.opt.ScipyOptimizerInterface函数创建了一个优化器self.optimizer，它使用了L-BFGS-B方法，最大迭代次数为50000次，最大函数调用次数为50000次，最大相关矩阵大小为50，最大线搜索次数为50，终止条件为1.0 * np.finfo(float).eps。\n",
    "              #tf.contrib.opt.ScipyOptimizerInterface是TensorFlow中的函数，提供了一个接口，可以使用优化算法来最小化TensorFlow的损失函数，接受三个参数，第一个参数是损失函数（这里是self.loss）,第二个参数是优化方法，这里是L-BFGS-B，第三个参数options是一个字典，用于指定优化器的参数。\n",
    "              #这里maxiter表示最大迭代次数，maxfun表示最大函数调用次数，maxcor表示每次迭代中使用的最大修正因子数量，maxls表示每次迭代中最大线搜索次数，ftol表示终止条件，这里是1.0*np.finfo(float).eps，其中np.finfo(float).eps表示浮点数的精度，1.0 * np.finfo(float).eps表示浮点数的精度乘以1.0。\n",
    "        self.optimizer = tf.contrib.opt.ScipyOptimizerInterface(self.loss, \n",
    "                                                                method = 'L-BFGS-B', \n",
    "                                                                options = {'maxiter': 50000,\n",
    "                                                                           'maxfun': 50000,\n",
    "                                                                           'maxcor': 50,\n",
    "                                                                           'maxls': 50,\n",
    "                                                                           'ftol' : 1.0 * np.finfo(float).eps})\n",
    "        \n",
    "        #使用tf.global_variables_initializer函数创建了一个初始化所有全局变量的操作（Tensorflow中所有变量在使用之前都需要进行初始化）\n",
    "        init = tf.global_variables_initializer()\n",
    "        #sess.run是会话的一个方法，用于执行图中的操作或计算张量的值。这里是执行初始化\n",
    "        self.sess.run(init)\n",
    "\n",
    "    #定义了一个名为`initialize_NN'的函数/方法，用于初始化神经网络的权重和偏置，最后返回权重和偏置。                 \n",
    "    def initialize_NN(self, layers):  #接受一个参数layers\n",
    "        #定义了两个空列表weights和biases，用于存储神经网络的权重和偏置      \n",
    "        weights = []\n",
    "        biases = []\n",
    "        num_layers = len(layers) #获取神经网络的层数（由输入参数给出）并将其赋值给num_layers\n",
    "        for l in range(0,num_layers-1): #使用循环遍历神经网络的每一层（除了输出层）\n",
    "            W = self.xavier_init(size=[layers[l], layers[l+1]]) #初始化该层的权重，使用了xavier_init函数(该函数定义在下面)进行Xaiver初始化，权重的形状是layers[l]*layers[l+1]，其中layers[l]是上一层的神经元数量，layers[l+1]是当前层的神经元数量。\n",
    "              #tf.Variable(initial_value, dtype=None, name=None)函数用于创建一个变量，接受三个参数，第一个参数initial_value是变量的初始值，第二个参数dtype是变量的数据类型，第三个参数name是变量的名称。\n",
    "            b = tf.Variable(tf.zeros([1,layers[l+1]], dtype=tf.float32), dtype=tf.float32) #初始化该层的偏置b，使用了tf.Variable函数，将偏置b初始化为一个形状为[1,layers[l+1]]的全0数组，其中layers[l+1]是当前层的神经元数量\n",
    "              #list.append(element)是Python中的一个方法，用于向列表中添加元素（添加到列表末尾）。list是要添加元素的列表，element是要添加的元素（然和类型的参数均可）。\n",
    "            weights.append(W) #将初始化的权重和偏置添加到weights和biases列表中\n",
    "            biases.append(b)        \n",
    "        return weights, biases\n",
    "        \n",
    "\n",
    "    #定义了一个名为xavier_init的函数/方法，用于初始化神经网络的权重(在神经网络参数初始化中实现，见上面)。这个函数使用了Xavier初始化方法，这是一种常用的权重初始化方法，可以帮助我们在训练深度神经网络时保持每一层的激活值的分布相对稳定。    \n",
    "    def xavier_init(self, size): #接受一个参数size，这是一个列表，包含了权重的形状\n",
    "        in_dim = size[0] #输入维度是size的第一个数\n",
    "        out_dim = size[1]   #输出维度是size的第二个数     \n",
    "        xavier_stddev = np.sqrt(2/(in_dim + out_dim)) #计算标准差\n",
    "        return tf.Variable(tf.truncated_normal([in_dim, out_dim], stddev=xavier_stddev), dtype=tf.float32) #返回一个变量，类型为32位浮点，初始值为截断正态分布，标准差为xavier_stddev，形状为[in_dim, out_dim]，其中in_dim和out_dim分别是输入维度和输出维度\n",
    "    \n",
    "    #定义了一个名为neural_net的函数/方法，用于实现神经网络的输出。这个方法接受三个参数，分别是X，weights和biases，其中X是输入数据，weights和biases是神经网络的权重和偏置。\n",
    "    def neural_net(self,X,weights,biases):\n",
    "        num_layers=len(weights)+1 #计算神经网络的层数并返回到num_layers，其值位权重矩阵的长度（行数）加1\n",
    "        \n",
    "        H=2.0*(X-self.lb)/(self.ub-self.lb)-1.0 #这里H是X经过归一化处理后的结果，将X映射到了[-1,1]区间内\n",
    "        for l in range(0,num_layers-2): #使用循环遍历神经网络的每一层（除了输出层）\n",
    "            W=weights[l] #获取当前层的权重\n",
    "            b=biases[l] #获取当前层的偏置\n",
    "            H=tf.tanh(tf.add(tf.matmul(H,W),b)) #计算当前层的输出，使用了tf.matmul函数计算矩阵乘法，tf.add函数计算矩阵加法，tf.tanh函数计算双曲正切函数\n",
    "        W=weights[-1] #获取输出层的权重,这里的weights[-1]表示列表weights的最后一个元素\n",
    "        b=biases[-1] #获取输出层的偏置\n",
    "        Y=tf.add(tf.matmul(H,W),b)  #计算输出层的输出H*W+b\n",
    "        return Y #返回输出层的输出Y\n",
    "    \n",
    "\n",
    "    #定义了一个名为net_u的函数/方法，用于计算神经网络的输出。这个方法接受两个参数，分别是x和t，其中x是输入数据，t是时间数据。最后返回神经网络的输出。        \n",
    "    def net_u(self, x, t):\n",
    "        u = self.neural_net(tf.concat([x,t],1), self.weights, self.biases)  #（第一个参数将输入的两个参数x和t在第二个维度（列）上进行拼接，形成一个新的张量）调用之前定义的neural_net函数，根据两个参数权重和偏置，以及新得到的张量，计算神经网络的输出u\n",
    "        return u\n",
    "    \n",
    "\n",
    "    #定义了一个名为net_f的函数/方法，用于计算论文中的f。这个方法接受两个参数，分别是x和t，其中x是输入数据，t是时间数据。最后返回计算得到的f_u和f_v。\n",
    "    def net_f(self, x,t):\n",
    "        u = self.net_u(x,t) #调用上面的net_u函数，计算神经网络的输出u\n",
    "        u_t = tf.gradients(u, t)[0] #计算u关于t的梯度，也就是u关于t的导数\n",
    "        u_x = tf.gradients(u, x)[0] #计算u关于x的梯度，也就是u关于x的导数\n",
    "        u_xx = tf.gradients(u_x, x)[0] #计算u关于x的二阶梯度，也就是u关于x的二阶导数\n",
    "        f = u_t + u*u_x - self.nu*u_xx #计算f，定义见论文\n",
    "        \n",
    "        return f\n",
    "    \n",
    "    def callback(self, loss): #定义了一个名为callback的函数/方法，打印损失值\n",
    "        print('Loss:', loss)\n",
    "\n",
    "    #定义了一个名为train的函数/方法，用于训练神经网络。但这个方法不接受任何参数（如迭代次数nIter）。    \n",
    "    def train(self):\n",
    "         #创建一个名为tf_dict的字典，该字典将TensorFlow占位符映射到它们对应的数据。创建tf_dict的目的是为了在运行TensorFlow的计算图时，能够将数据传递给占位符。例如，当运行self.sess.run(self.train_op_Adam, tf_dict)时，tf_dict中的数据就会被传递给对应的占位符，然后在计算图中使用\n",
    "            #字典语法：dict = {key1: value1, key2: value2, ...}：key1、key2等是字典的键，value1、value2等是对应的值（这里键是占位符，值是对应的数据）。\n",
    "        tf_dict = {self.x_u_tf: self.x_u, self.t_u_tf: self.t_u, self.u_tf: self.u,\n",
    "                   self.x_f_tf: self.x_f, self.t_f_tf: self.t_f}\n",
    "\n",
    "         #使用优化器来最小化损失函数，第一个参数表示TensorFlow会话，第二个参数表示将数据传递给占位符的字典，fetches表示要获取的结果（这里只获取了损失函数的值），最后一个参数表示每次优化迭代后调用的回调参数，这里是之前定义的，用来打印损失函数                                                                                                                  \n",
    "        self.optimizer.minimize(self.sess, \n",
    "                                feed_dict = tf_dict,         \n",
    "                                fetches = [self.loss], \n",
    "                                loss_callback = self.callback)        \n",
    "                                    \n",
    "    #定义了一个名为predict的函数/方法，用于预测神经网络的输出。这个方法接受一个参数X_star，表示输入数据。最后返回预测的两个输出。\n",
    "    def predict(self, X_star):\n",
    "        #括号内第二个参数是创建一个字典，其中包含了两个键值对，键是 TensorFlow 的占位符（self.x_u_tf和self.t_u_tf），值是X_star中的对应列。这个字典将被用于在下面的TensorFlow 会话中给placeholder赋值，用以运行计算图\n",
    "        #使用 self.sess.run方法来给placeholder赋值并运行计算图，获取self.u_pred(神经网络的输出)的计算结果。并将结果保存在u_star中        \n",
    "        u_star = self.sess.run(self.u_pred, {self.x_u_tf: X_star[:,0:1], self.t_u_tf: X_star[:,1:2]})  \n",
    "        #括号内第二个参数是创建一个字典，其中包含了两个键值对，键是 TensorFlow 的占位符（self.x_f_tf和self.t_f_tf），值是X_star中的对应列。这个字典将被用于在下面的TensorFlow 会话中给placeholder赋值，用以运行计算图\n",
    "        #使用 self.sess.run方法来给placeholder赋值并运行计算图，获取self.f_pred(神经网络的输出)的计算结果。并将结果保存在f_star中\n",
    "        f_star = self.sess.run(self.f_pred, {self.x_f_tf: X_star[:,0:1], self.t_f_tf: X_star[:,1:2]})\n",
    "        return u_star, f_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\cheny\\AppData\\Local\\Temp\\ipykernel_14588\\1841064808.py:97: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\cheny\\AppData\\Local\\Temp\\ipykernel_14588\\1841064808.py:32: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\cheny\\AppData\\Local\\Temp\\ipykernel_14588\\1841064808.py:32: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:52:00.0, compute capability: 7.5\n",
      "/job:localhost/replica:0/task:0/device:GPU:1 -> device: 1, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:53:00.0, compute capability: 7.5\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\cheny\\AppData\\Local\\Temp\\ipykernel_14588\\1841064808.py:38: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\cheny\\AppData\\Local\\Temp\\ipykernel_14588\\1841064808.py:72: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n",
      "Loss: 0.28520364\n",
      "Loss: 0.6429156\n",
      "Loss: 0.25300643\n",
      "Loss: 0.23689014\n",
      "Loss: 0.23533313\n",
      "Loss: 0.23134337\n",
      "Loss: 0.2298152\n",
      "Loss: 0.22796476\n",
      "Loss: 0.22094162\n",
      "Loss: 0.2188291\n",
      "Loss: 0.21366788\n",
      "Loss: 0.20448838\n",
      "Loss: 0.19408073\n",
      "Loss: 0.19082873\n",
      "Loss: 0.21185881\n",
      "Loss: 0.18022722\n",
      "Loss: 0.17486882\n",
      "Loss: 0.16278063\n",
      "Loss: 0.1549454\n",
      "Loss: 0.15105098\n",
      "Loss: 0.15107861\n",
      "Loss: 0.14876363\n",
      "Loss: 0.14754027\n",
      "Loss: 0.14399624\n",
      "Loss: 0.14017177\n",
      "Loss: 0.13778931\n",
      "Loss: 0.1910006\n",
      "Loss: 0.13325173\n",
      "Loss: 0.12933895\n",
      "Loss: 0.11708557\n",
      "Loss: 0.117156416\n",
      "Loss: 0.10798283\n",
      "Loss: 0.1227922\n",
      "Loss: 0.10553168\n",
      "Loss: 0.10331823\n",
      "Loss: 0.10231429\n",
      "Loss: 0.101671726\n",
      "Loss: 0.10049777\n",
      "Loss: 0.09787995\n",
      "Loss: 0.09652282\n",
      "Loss: 0.093380265\n",
      "Loss: 0.09248446\n",
      "Loss: 0.09213564\n",
      "Loss: 0.09200102\n",
      "Loss: 0.091012955\n",
      "Loss: 0.08898944\n",
      "Loss: 0.08801428\n",
      "Loss: 0.08749437\n",
      "Loss: 0.08714557\n",
      "Loss: 0.08692239\n",
      "Loss: 0.08677237\n",
      "Loss: 0.08644222\n",
      "Loss: 0.08601413\n",
      "Loss: 0.085286275\n",
      "Loss: 0.08477032\n",
      "Loss: 0.08423219\n",
      "Loss: 0.08386435\n",
      "Loss: 0.08349851\n",
      "Loss: 0.08274147\n",
      "Loss: 0.081688665\n",
      "Loss: 0.0812217\n",
      "Loss: 0.08108814\n",
      "Loss: 0.080937356\n",
      "Loss: 0.080752425\n",
      "Loss: 0.08020541\n",
      "Loss: 0.079241395\n",
      "Loss: 0.07864504\n",
      "Loss: 0.078279525\n",
      "Loss: 0.07796102\n",
      "Loss: 0.07772226\n",
      "Loss: 0.0775319\n",
      "Loss: 0.07721747\n",
      "Loss: 0.07656198\n",
      "Loss: 0.075753376\n",
      "Loss: 0.07547359\n",
      "Loss: 0.08937627\n",
      "Loss: 0.07483712\n",
      "Loss: 0.07472243\n",
      "Loss: 0.074238464\n",
      "Loss: 0.07409091\n",
      "Loss: 0.07382105\n",
      "Loss: 0.07326262\n",
      "Loss: 0.07250642\n",
      "Loss: 0.07180643\n",
      "Loss: 0.07160707\n",
      "Loss: 0.07120408\n",
      "Loss: 0.07092651\n",
      "Loss: 0.07057987\n",
      "Loss: 0.06990187\n",
      "Loss: 0.06884535\n",
      "Loss: 0.06726571\n",
      "Loss: 0.06643801\n",
      "Loss: 0.066337205\n",
      "Loss: 0.066112846\n",
      "Loss: 0.06599295\n",
      "Loss: 0.0657713\n",
      "Loss: 0.06537744\n",
      "Loss: 0.064888984\n",
      "Loss: 0.06457164\n",
      "Loss: 0.06437364\n",
      "Loss: 0.06425037\n",
      "Loss: 0.063969925\n",
      "Loss: 0.06345151\n",
      "Loss: 0.06263773\n",
      "Loss: 0.062297218\n",
      "Loss: 0.061812017\n",
      "Loss: 0.061636437\n",
      "Loss: 0.06125386\n",
      "Loss: 0.060494974\n",
      "Loss: 0.059288863\n",
      "Loss: 0.058339886\n",
      "Loss: 0.057638302\n",
      "Loss: 0.05730053\n",
      "Loss: 0.057118632\n",
      "Loss: 0.056956664\n",
      "Loss: 0.056793228\n",
      "Loss: 0.056553014\n",
      "Loss: 0.056073055\n",
      "Loss: 0.055220254\n",
      "Loss: 0.054318525\n",
      "Loss: 0.053720914\n",
      "Loss: 0.053093918\n",
      "Loss: 0.05343184\n",
      "Loss: 0.052730467\n",
      "Loss: 0.052261192\n",
      "Loss: 0.051731713\n",
      "Loss: 0.050994236\n",
      "Loss: 0.05031819\n",
      "Loss: 0.049700007\n",
      "Loss: 0.049155153\n",
      "Loss: 0.048655905\n",
      "Loss: 0.04845625\n",
      "Loss: 0.048252717\n",
      "Loss: 0.047669657\n",
      "Loss: 0.04656107\n",
      "Loss: 0.045734935\n",
      "Loss: 0.045322143\n",
      "Loss: 0.043810662\n",
      "Loss: 0.042885993\n",
      "Loss: 0.04251539\n",
      "Loss: 0.04210134\n",
      "Loss: 0.041963268\n",
      "Loss: 0.041818094\n",
      "Loss: 0.041624032\n",
      "Loss: 0.041346185\n",
      "Loss: 0.04093061\n",
      "Loss: 0.04023961\n",
      "Loss: 0.03943311\n",
      "Loss: 0.038774565\n",
      "Loss: 0.038277566\n",
      "Loss: 0.037600193\n",
      "Loss: 0.037089005\n",
      "Loss: 0.03653718\n",
      "Loss: 0.035872348\n",
      "Loss: 0.035155203\n",
      "Loss: 0.03477458\n",
      "Loss: 0.034140922\n",
      "Loss: 0.03361527\n",
      "Loss: 0.033334866\n",
      "Loss: 0.03310108\n",
      "Loss: 0.032808706\n",
      "Loss: 0.032440454\n",
      "Loss: 0.031953633\n",
      "Loss: 0.031589057\n",
      "Loss: 0.03117477\n",
      "Loss: 0.030505184\n",
      "Loss: 0.030103045\n",
      "Loss: 0.02990146\n",
      "Loss: 0.02963237\n",
      "Loss: 0.029396148\n",
      "Loss: 0.029154219\n",
      "Loss: 0.028962793\n",
      "Loss: 0.028834347\n",
      "Loss: 0.028700475\n",
      "Loss: 0.028514804\n",
      "Loss: 0.0282549\n",
      "Loss: 0.027945273\n",
      "Loss: 0.027508957\n",
      "Loss: 0.026763968\n",
      "Loss: 0.026095236\n",
      "Loss: 0.025967037\n",
      "Loss: 0.025420308\n",
      "Loss: 0.025252637\n",
      "Loss: 0.025114164\n",
      "Loss: 0.025041679\n",
      "Loss: 0.025000744\n",
      "Loss: 0.02496033\n",
      "Loss: 0.024860349\n",
      "Loss: 0.0246252\n",
      "Loss: 0.024395443\n",
      "Loss: 0.024018683\n",
      "Loss: 0.023535736\n",
      "Loss: 0.023211163\n",
      "Loss: 0.022990916\n",
      "Loss: 0.022893492\n",
      "Loss: 0.022825595\n",
      "Loss: 0.022800505\n",
      "Loss: 0.022677068\n",
      "Loss: 0.02224923\n",
      "Loss: 0.02165802\n",
      "Loss: 0.021830466\n",
      "Loss: 0.021162177\n",
      "Loss: 0.020635892\n",
      "Loss: 0.020025885\n",
      "Loss: 0.019772165\n",
      "Loss: 0.019611951\n",
      "Loss: 0.0194882\n",
      "Loss: 0.019102586\n",
      "Loss: 0.018448818\n",
      "Loss: 0.017930195\n",
      "Loss: 0.017159645\n",
      "Loss: 0.016315186\n",
      "Loss: 0.016182467\n",
      "Loss: 0.016611561\n",
      "Loss: 0.015872398\n",
      "Loss: 0.015880337\n",
      "Loss: 0.015610702\n",
      "Loss: 0.015492876\n",
      "Loss: 0.015379119\n",
      "Loss: 0.015303852\n",
      "Loss: 0.015245872\n",
      "Loss: 0.015147018\n",
      "Loss: 0.015039341\n",
      "Loss: 0.014928514\n",
      "Loss: 0.014810244\n",
      "Loss: 0.014675655\n",
      "Loss: 0.014458927\n",
      "Loss: 0.015506176\n",
      "Loss: 0.014379719\n",
      "Loss: 0.014115192\n",
      "Loss: 0.013841135\n",
      "Loss: 0.013707127\n",
      "Loss: 0.013586417\n",
      "Loss: 0.013474915\n",
      "Loss: 0.013269709\n",
      "Loss: 0.013082521\n",
      "Loss: 0.013099194\n",
      "Loss: 0.012949729\n",
      "Loss: 0.01270194\n",
      "Loss: 0.0124576595\n",
      "Loss: 0.01234452\n",
      "Loss: 0.0122274235\n",
      "Loss: 0.012095079\n",
      "Loss: 0.011904193\n",
      "Loss: 0.011743884\n",
      "Loss: 0.011642206\n",
      "Loss: 0.0115601495\n",
      "Loss: 0.011505995\n",
      "Loss: 0.011464563\n",
      "Loss: 0.011385966\n",
      "Loss: 0.0112326965\n",
      "Loss: 0.01112856\n",
      "Loss: 0.011030815\n",
      "Loss: 0.010947006\n",
      "Loss: 0.010904072\n",
      "Loss: 0.010797425\n",
      "Loss: 0.010651708\n",
      "Loss: 0.010504157\n",
      "Loss: 0.010388091\n",
      "Loss: 0.0102810655\n",
      "Loss: 0.010230519\n",
      "Loss: 0.010192022\n",
      "Loss: 0.010124107\n",
      "Loss: 0.010570997\n",
      "Loss: 0.010044704\n",
      "Loss: 0.0098065995\n",
      "Loss: 0.009635566\n",
      "Loss: 0.009442491\n",
      "Loss: 0.0093367295\n",
      "Loss: 0.009270057\n",
      "Loss: 0.0091670025\n",
      "Loss: 0.009094488\n",
      "Loss: 0.008988617\n",
      "Loss: 0.008878445\n",
      "Loss: 0.0087906085\n",
      "Loss: 0.00875142\n",
      "Loss: 0.008712434\n",
      "Loss: 0.008669402\n",
      "Loss: 0.008632797\n",
      "Loss: 0.008573787\n",
      "Loss: 0.008477135\n",
      "Loss: 0.008414121\n",
      "Loss: 0.008412982\n",
      "Loss: 0.008314989\n",
      "Loss: 0.008249769\n",
      "Loss: 0.008177913\n",
      "Loss: 0.008030426\n",
      "Loss: 0.008055874\n",
      "Loss: 0.007942443\n",
      "Loss: 0.007857432\n",
      "Loss: 0.0076691187\n",
      "Loss: 0.0075957943\n",
      "Loss: 0.007550696\n",
      "Loss: 0.007498407\n",
      "Loss: 0.007374349\n",
      "Loss: 0.007291033\n",
      "Loss: 0.0072113825\n",
      "Loss: 0.0071460665\n",
      "Loss: 0.0070585515\n",
      "Loss: 0.006902473\n",
      "Loss: 0.008084875\n",
      "Loss: 0.0068373024\n",
      "Loss: 0.006826171\n",
      "Loss: 0.006954539\n",
      "Loss: 0.006797843\n",
      "Loss: 0.006752281\n",
      "Loss: 0.0067071086\n",
      "Loss: 0.0066419924\n",
      "Loss: 0.006607976\n",
      "Loss: 0.006549393\n",
      "Loss: 0.0064917244\n",
      "Loss: 0.0064232503\n",
      "Loss: 0.006351463\n",
      "Loss: 0.006292844\n",
      "Loss: 0.0062621976\n",
      "Loss: 0.006225519\n",
      "Loss: 0.00620224\n",
      "Loss: 0.006171534\n",
      "Loss: 0.006137318\n",
      "Loss: 0.006111864\n",
      "Loss: 0.0060769785\n",
      "Loss: 0.006030675\n",
      "Loss: 0.005959506\n",
      "Loss: 0.0059064995\n",
      "Loss: 0.00586903\n",
      "Loss: 0.005819847\n",
      "Loss: 0.005762566\n",
      "Loss: 0.005702896\n",
      "Loss: 0.005655328\n",
      "Loss: 0.005624114\n",
      "Loss: 0.00556228\n",
      "Loss: 0.005548305\n",
      "Loss: 0.0054926053\n",
      "Loss: 0.0053853225\n",
      "Loss: 0.0052690995\n",
      "Loss: 0.0052040624\n",
      "Loss: 0.0051645404\n",
      "Loss: 0.005143306\n",
      "Loss: 0.005126968\n",
      "Loss: 0.005099369\n",
      "Loss: 0.005072344\n",
      "Loss: 0.005046392\n",
      "Loss: 0.005015435\n",
      "Loss: 0.0049827583\n",
      "Loss: 0.004976891\n",
      "Loss: 0.004932454\n",
      "Loss: 0.004899206\n",
      "Loss: 0.004850423\n",
      "Loss: 0.004818149\n",
      "Loss: 0.0047476385\n",
      "Loss: 0.005957725\n",
      "Loss: 0.0047291843\n",
      "Loss: 0.004658468\n",
      "Loss: 0.004626991\n",
      "Loss: 0.00460737\n",
      "Loss: 0.0045850053\n",
      "Loss: 0.004542875\n",
      "Loss: 0.0048148255\n",
      "Loss: 0.004482093\n",
      "Loss: 0.00518156\n",
      "Loss: 0.0044524274\n",
      "Loss: 0.0043882397\n",
      "Loss: 0.004322612\n",
      "Loss: 0.0044057914\n",
      "Loss: 0.004285124\n",
      "Loss: 0.0043723295\n",
      "Loss: 0.004264502\n",
      "Loss: 0.004247112\n",
      "Loss: 0.004207089\n",
      "Loss: 0.00415509\n",
      "Loss: 0.004131302\n",
      "Loss: 0.0041160355\n",
      "Loss: 0.00410118\n",
      "Loss: 0.004090031\n",
      "Loss: 0.004080595\n",
      "Loss: 0.00407286\n",
      "Loss: 0.004070917\n",
      "Loss: 0.004059742\n",
      "Loss: 0.0040391567\n",
      "Loss: 0.00399347\n",
      "Loss: 0.003941247\n",
      "Loss: 0.003932727\n",
      "Loss: 0.003887083\n",
      "Loss: 0.0038529998\n",
      "Loss: 0.0038004941\n",
      "Loss: 0.0037179429\n",
      "Loss: 0.0036526583\n",
      "Loss: 0.003596806\n",
      "Loss: 0.0035550771\n",
      "Loss: 0.0035211123\n",
      "Loss: 0.003488903\n",
      "Loss: 0.0034671985\n",
      "Loss: 0.0034520072\n",
      "Loss: 0.003468057\n",
      "Loss: 0.0034462328\n",
      "Loss: 0.003432474\n",
      "Loss: 0.0034191795\n",
      "Loss: 0.0033952738\n",
      "Loss: 0.0033754343\n",
      "Loss: 0.0033617956\n",
      "Loss: 0.0033431738\n",
      "Loss: 0.0033085786\n",
      "Loss: 0.0032679632\n",
      "Loss: 0.0032410156\n",
      "Loss: 0.0031990106\n",
      "Loss: 0.0031713303\n",
      "Loss: 0.003131863\n",
      "Loss: 0.0030830055\n",
      "Loss: 0.0030528582\n",
      "Loss: 0.0029916544\n",
      "Loss: 0.0029416229\n",
      "Loss: 0.0029174292\n",
      "Loss: 0.0028956756\n",
      "Loss: 0.0028792443\n",
      "Loss: 0.002862364\n",
      "Loss: 0.002889831\n",
      "Loss: 0.002851314\n",
      "Loss: 0.0028289007\n",
      "Loss: 0.002815769\n",
      "Loss: 0.002805472\n",
      "Loss: 0.0027815797\n",
      "Loss: 0.0027550901\n",
      "Loss: 0.0028717164\n",
      "Loss: 0.0027360187\n",
      "Loss: 0.0027019337\n",
      "Loss: 0.0026912526\n",
      "Loss: 0.0026755596\n",
      "Loss: 0.0026505624\n",
      "Loss: 0.002636604\n",
      "Loss: 0.0026219205\n",
      "Loss: 0.002600919\n",
      "Loss: 0.0025799195\n",
      "Loss: 0.0027472335\n",
      "Loss: 0.0025741928\n",
      "Loss: 0.0025516313\n",
      "Loss: 0.0025353192\n",
      "Loss: 0.002521073\n",
      "Loss: 0.0025127719\n",
      "Loss: 0.002500724\n",
      "Loss: 0.0024811937\n",
      "Loss: 0.002467106\n",
      "Loss: 0.0024529244\n",
      "Loss: 0.0024686744\n",
      "Loss: 0.0024495353\n",
      "Loss: 0.002435803\n",
      "Loss: 0.0024248348\n",
      "Loss: 0.0024128503\n",
      "Loss: 0.0024058304\n",
      "Loss: 0.0023917286\n",
      "Loss: 0.0023764074\n",
      "Loss: 0.0023679659\n",
      "Loss: 0.002352267\n",
      "Loss: 0.0023371947\n",
      "Loss: 0.002323796\n",
      "Loss: 0.0023114327\n",
      "Loss: 0.0022978885\n",
      "Loss: 0.0022786593\n",
      "Loss: 0.0022565122\n",
      "Loss: 0.0022402273\n",
      "Loss: 0.002224562\n",
      "Loss: 0.0022114113\n",
      "Loss: 0.0022208574\n",
      "Loss: 0.0022085544\n",
      "Loss: 0.00220341\n",
      "Loss: 0.002190875\n",
      "Loss: 0.0021701027\n",
      "Loss: 0.002135281\n",
      "Loss: 0.0021050256\n",
      "Loss: 0.0020784438\n",
      "Loss: 0.002061972\n",
      "Loss: 0.002050335\n",
      "Loss: 0.0020428894\n",
      "Loss: 0.002035934\n",
      "Loss: 0.0020448063\n",
      "Loss: 0.0020298122\n",
      "Loss: 0.0020163346\n",
      "Loss: 0.0020014183\n",
      "Loss: 0.0019822877\n",
      "Loss: 0.0019656243\n",
      "Loss: 0.0019548906\n",
      "Loss: 0.0019482868\n",
      "Loss: 0.0019436501\n",
      "Loss: 0.0019405414\n",
      "Loss: 0.0019394802\n",
      "Loss: 0.0019312572\n",
      "Loss: 0.0019243618\n",
      "Loss: 0.0019150513\n",
      "Loss: 0.0019085074\n",
      "Loss: 0.0018890032\n",
      "Loss: 0.0018706663\n",
      "Loss: 0.0018604659\n",
      "Loss: 0.0018538677\n",
      "Loss: 0.0018497761\n",
      "Loss: 0.0018449968\n",
      "Loss: 0.0018388254\n",
      "Loss: 0.0018329683\n",
      "Loss: 0.0018259564\n",
      "Loss: 0.0018198342\n",
      "Loss: 0.00181581\n",
      "Loss: 0.0018100473\n",
      "Loss: 0.001802003\n",
      "Loss: 0.0017874495\n",
      "Loss: 0.0017763108\n",
      "Loss: 0.0017855484\n",
      "Loss: 0.0017645289\n",
      "Loss: 0.0017493548\n",
      "Loss: 0.0017420254\n",
      "Loss: 0.0017316232\n",
      "Loss: 0.001723897\n",
      "Loss: 0.0017179728\n",
      "Loss: 0.0017136661\n",
      "Loss: 0.001709918\n",
      "Loss: 0.0016992382\n",
      "Loss: 0.0016807928\n",
      "Loss: 0.0016680381\n",
      "Loss: 0.001643178\n",
      "Loss: 0.0016345977\n",
      "Loss: 0.0016275656\n",
      "Loss: 0.0016169564\n",
      "Loss: 0.0016029767\n",
      "Loss: 0.0015892552\n",
      "Loss: 0.0015825205\n",
      "Loss: 0.0015790872\n",
      "Loss: 0.0015758632\n",
      "Loss: 0.001571757\n",
      "Loss: 0.0015687693\n",
      "Loss: 0.0015636655\n",
      "Loss: 0.0022336964\n",
      "Loss: 0.0015613728\n",
      "Loss: 0.0015542501\n",
      "Loss: 0.0015427911\n",
      "Loss: 0.0015335107\n",
      "Loss: 0.0015239253\n",
      "Loss: 0.0015138953\n",
      "Loss: 0.00151608\n",
      "Loss: 0.0015067178\n",
      "Loss: 0.0014853069\n",
      "Loss: 0.0014705248\n",
      "Loss: 0.0014559454\n",
      "Loss: 0.0014400829\n",
      "Loss: 0.0014230117\n",
      "Loss: 0.001414195\n",
      "Loss: 0.0014061367\n",
      "Loss: 0.0014269754\n",
      "Loss: 0.0014025627\n",
      "Loss: 0.0013936621\n",
      "Loss: 0.0013875462\n",
      "Loss: 0.0013975757\n",
      "Loss: 0.0013831304\n",
      "Loss: 0.0013729935\n",
      "Loss: 0.0013625049\n",
      "Loss: 0.0013506319\n",
      "Loss: 0.0013472314\n",
      "Loss: 0.0013435504\n",
      "Loss: 0.0013399095\n",
      "Loss: 0.0013377672\n",
      "Loss: 0.0013306638\n",
      "Loss: 0.001323024\n",
      "Loss: 0.0013136251\n",
      "Loss: 0.0013060153\n",
      "Loss: 0.0012955094\n",
      "Loss: 0.0012820298\n",
      "Loss: 0.0012738496\n",
      "Loss: 0.0012757262\n",
      "Loss: 0.001269645\n",
      "Loss: 0.0012607268\n",
      "Loss: 0.0012515117\n",
      "Loss: 0.0012434127\n",
      "Loss: 0.0012397029\n",
      "Loss: 0.0012355619\n",
      "Loss: 0.0012327333\n",
      "Loss: 0.0012306017\n",
      "Loss: 0.0012285766\n",
      "Loss: 0.0012251514\n",
      "Loss: 0.0012187972\n",
      "Loss: 0.0012136768\n",
      "Loss: 0.0012080411\n",
      "Loss: 0.0012043008\n",
      "Loss: 0.001197844\n",
      "Loss: 0.0011907265\n",
      "Loss: 0.0011853947\n",
      "Loss: 0.0011824088\n",
      "Loss: 0.0011806199\n",
      "Loss: 0.0011789233\n",
      "Loss: 0.0011758793\n",
      "Loss: 0.0011724638\n",
      "Loss: 0.0011685689\n",
      "Loss: 0.0011613593\n",
      "Loss: 0.0011539664\n",
      "Loss: 0.0011457828\n",
      "Loss: 0.0011376488\n",
      "Loss: 0.0011463485\n",
      "Loss: 0.0011347923\n",
      "Loss: 0.0011295277\n",
      "Loss: 0.0011260288\n",
      "Loss: 0.0011224693\n",
      "Loss: 0.001114605\n",
      "Loss: 0.0011074981\n",
      "Loss: 0.0011010617\n",
      "Loss: 0.0010935571\n",
      "Loss: 0.0010838846\n",
      "Loss: 0.0010768691\n",
      "Loss: 0.0010709113\n",
      "Loss: 0.0010661952\n",
      "Loss: 0.0010593134\n",
      "Loss: 0.001050261\n",
      "Loss: 0.0010550224\n",
      "Loss: 0.001046333\n",
      "Loss: 0.0010516184\n",
      "Loss: 0.001041151\n",
      "Loss: 0.0010371389\n",
      "Loss: 0.001030636\n",
      "Loss: 0.0010280908\n",
      "Loss: 0.0010258612\n",
      "Loss: 0.0010235574\n",
      "Loss: 0.00102132\n",
      "Loss: 0.0010192165\n",
      "Loss: 0.0010168351\n",
      "Loss: 0.0010144324\n",
      "Loss: 0.0010121204\n",
      "Loss: 0.001010675\n",
      "Loss: 0.0010080348\n",
      "Loss: 0.0010053859\n",
      "Loss: 0.0010034969\n",
      "Loss: 0.0010002634\n",
      "Loss: 0.000997932\n",
      "Loss: 0.0009934756\n",
      "Loss: 0.0009843527\n",
      "Loss: 0.0009772719\n",
      "Loss: 0.0009751495\n",
      "Loss: 0.00096768263\n",
      "Loss: 0.0009633753\n",
      "Loss: 0.00095961266\n",
      "Loss: 0.00095381297\n",
      "Loss: 0.0009650027\n",
      "Loss: 0.0009525777\n",
      "Loss: 0.00094874785\n",
      "Loss: 0.0009410228\n",
      "Loss: 0.00093032134\n",
      "Loss: 0.0009255492\n",
      "Loss: 0.0009228955\n",
      "Loss: 0.00092041\n",
      "Loss: 0.0009158262\n",
      "Loss: 0.0009121695\n",
      "Loss: 0.00090695627\n",
      "Loss: 0.0009039978\n",
      "Loss: 0.00090146344\n",
      "Loss: 0.00090046076\n",
      "Loss: 0.0009000921\n",
      "Loss: 0.0008978396\n",
      "Loss: 0.0008969596\n",
      "Loss: 0.0008950407\n",
      "Loss: 0.00089282054\n",
      "Loss: 0.000890648\n",
      "Loss: 0.0008874273\n",
      "Loss: 0.00088388444\n",
      "Loss: 0.000880054\n",
      "Loss: 0.0008754862\n",
      "Loss: 0.00087128393\n",
      "Loss: 0.000869464\n",
      "Loss: 0.00086333917\n",
      "Loss: 0.0008595171\n",
      "Loss: 0.0008550231\n",
      "Loss: 0.0008501614\n",
      "Loss: 0.0008456253\n",
      "Loss: 0.00085044274\n",
      "Loss: 0.0008434168\n",
      "Loss: 0.0008405937\n",
      "Loss: 0.0008384638\n",
      "Loss: 0.0008368965\n",
      "Loss: 0.0008348129\n",
      "Loss: 0.0008359096\n",
      "Loss: 0.00083292724\n",
      "Loss: 0.00082797464\n",
      "Loss: 0.000822935\n",
      "Loss: 0.0008192954\n",
      "Loss: 0.00081473426\n",
      "Loss: 0.0008082746\n",
      "Loss: 0.0008015998\n",
      "Loss: 0.00079454214\n",
      "Loss: 0.0007837672\n",
      "Loss: 0.0007758416\n",
      "Loss: 0.000768265\n",
      "Loss: 0.00076240744\n",
      "Loss: 0.00076039525\n",
      "Loss: 0.00075816916\n",
      "Loss: 0.00075179496\n",
      "Loss: 0.0007459694\n",
      "Loss: 0.0007417114\n",
      "Loss: 0.00073770195\n",
      "Loss: 0.0007329459\n",
      "Loss: 0.0007313554\n",
      "Loss: 0.0007293435\n",
      "Loss: 0.00072765053\n",
      "Loss: 0.00072582765\n",
      "Loss: 0.00090039836\n",
      "Loss: 0.0007250689\n",
      "Loss: 0.00072317116\n",
      "Loss: 0.00072023325\n",
      "Loss: 0.00071747706\n",
      "Loss: 0.00071494863\n",
      "Loss: 0.00071157597\n",
      "Loss: 0.00070824916\n",
      "Loss: 0.00070488424\n",
      "Loss: 0.0007049418\n",
      "Loss: 0.00070187624\n",
      "Loss: 0.000697082\n",
      "Loss: 0.0006905987\n",
      "Loss: 0.0006840606\n",
      "Loss: 0.0006818792\n",
      "Loss: 0.0006766722\n",
      "Loss: 0.0006741743\n",
      "Loss: 0.00067187037\n",
      "Loss: 0.0006698413\n",
      "Loss: 0.00066706946\n",
      "Loss: 0.00066403317\n",
      "Loss: 0.0006632982\n",
      "Loss: 0.0006609357\n",
      "Loss: 0.00065763836\n",
      "Loss: 0.0006532652\n",
      "Loss: 0.0006479507\n",
      "Loss: 0.00071899\n",
      "Loss: 0.0006462558\n",
      "Loss: 0.0006420478\n",
      "Loss: 0.00063884316\n",
      "Loss: 0.000637377\n",
      "Loss: 0.00063533185\n",
      "Loss: 0.000634248\n",
      "Loss: 0.00062981754\n",
      "Loss: 0.0006276409\n",
      "Loss: 0.00062463223\n",
      "Loss: 0.0006217677\n",
      "Loss: 0.00061797374\n",
      "Loss: 0.00061247527\n",
      "Loss: 0.0006082092\n",
      "Loss: 0.00060482154\n",
      "Loss: 0.00060231495\n",
      "Loss: 0.000600203\n",
      "Loss: 0.00059760595\n",
      "Loss: 0.0005947958\n",
      "Loss: 0.00079162396\n",
      "Loss: 0.0005939406\n",
      "Loss: 0.0005908203\n",
      "Loss: 0.00058830477\n",
      "Loss: 0.0005864685\n",
      "Loss: 0.00058560143\n",
      "Loss: 0.0005838397\n",
      "Loss: 0.0005824523\n",
      "Loss: 0.00058139826\n",
      "Loss: 0.00058037497\n",
      "Loss: 0.0007003721\n",
      "Loss: 0.00058018084\n",
      "Loss: 0.00057883805\n",
      "Loss: 0.00057767594\n",
      "Loss: 0.00057642313\n",
      "Loss: 0.00057565095\n",
      "Loss: 0.00057455525\n",
      "Loss: 0.0005734271\n",
      "Loss: 0.0005723661\n",
      "Loss: 0.0005711738\n",
      "Loss: 0.00056982005\n",
      "Loss: 0.00056692417\n",
      "Loss: 0.0005944089\n",
      "Loss: 0.0005662315\n",
      "Loss: 0.0005634464\n",
      "Loss: 0.00056203356\n",
      "Loss: 0.0006060911\n",
      "Loss: 0.0005601047\n",
      "Loss: 0.0005583167\n",
      "Loss: 0.0005554472\n",
      "Loss: 0.0005522198\n",
      "Loss: 0.00054969336\n",
      "Loss: 0.0005479754\n",
      "Loss: 0.0005465343\n",
      "Loss: 0.00054537435\n",
      "Loss: 0.00054461614\n",
      "Loss: 0.0005440414\n",
      "Loss: 0.00054257736\n",
      "Loss: 0.0005400077\n",
      "Loss: 0.000546408\n",
      "Loss: 0.000539203\n",
      "Loss: 0.0005347699\n",
      "Loss: 0.0005316696\n",
      "Loss: 0.00052823836\n",
      "Loss: 0.00052686845\n",
      "Loss: 0.00052523415\n",
      "Loss: 0.000522459\n",
      "Loss: 0.00052134984\n",
      "Loss: 0.0005197391\n",
      "Loss: 0.0005184049\n",
      "Loss: 0.00051995955\n",
      "Loss: 0.00051756634\n",
      "Loss: 0.00051628205\n",
      "Loss: 0.0005154741\n",
      "Loss: 0.00051474775\n",
      "Loss: 0.00051400095\n",
      "Loss: 0.0005126318\n",
      "Loss: 0.00051150937\n",
      "Loss: 0.0005097052\n",
      "Loss: 0.0005073814\n",
      "Loss: 0.000506579\n",
      "Loss: 0.0005029595\n",
      "Loss: 0.000501986\n",
      "Loss: 0.00050082925\n",
      "Loss: 0.0005029801\n",
      "Loss: 0.000500048\n",
      "Loss: 0.000498242\n",
      "Loss: 0.0004964396\n",
      "Loss: 0.00049477513\n",
      "Loss: 0.00049252715\n",
      "Loss: 0.0004904244\n",
      "Loss: 0.00048640784\n",
      "Loss: 0.00048238167\n",
      "Loss: 0.0004792025\n",
      "Loss: 0.00047988223\n",
      "Loss: 0.00047795527\n",
      "Loss: 0.00047645537\n",
      "Loss: 0.00047537245\n",
      "Loss: 0.00047446106\n",
      "Loss: 0.00047294123\n",
      "Loss: 0.00047157684\n",
      "Loss: 0.00046936746\n",
      "Loss: 0.00046494053\n",
      "Loss: 0.00046200684\n",
      "Loss: 0.00048053078\n",
      "Loss: 0.00045774682\n",
      "Loss: 0.0004542809\n",
      "Loss: 0.00045129034\n",
      "Loss: 0.00044759118\n",
      "Loss: 0.00044322756\n",
      "Loss: 0.00044066797\n",
      "Loss: 0.00044110528\n",
      "Loss: 0.00043966947\n",
      "Loss: 0.00043752103\n",
      "Loss: 0.00043585716\n",
      "Loss: 0.00043406\n",
      "Loss: 0.00043001876\n",
      "Loss: 0.00042576506\n",
      "Loss: 0.0004233719\n",
      "Loss: 0.0004220241\n",
      "Loss: 0.00042089558\n",
      "Loss: 0.00041972712\n",
      "Loss: 0.0004182917\n",
      "Loss: 0.00041673606\n",
      "Loss: 0.00041528116\n",
      "Loss: 0.0004138523\n",
      "Loss: 0.0004119344\n",
      "Loss: 0.00041279022\n",
      "Loss: 0.0004111633\n",
      "Loss: 0.00041016372\n",
      "Loss: 0.00040898452\n",
      "Loss: 0.00040756963\n",
      "Loss: 0.00040576686\n",
      "Loss: 0.00041360958\n",
      "Loss: 0.000404894\n",
      "Loss: 0.00040288456\n",
      "Loss: 0.0004018531\n",
      "Loss: 0.0004007648\n",
      "Loss: 0.00039969402\n",
      "Loss: 0.0003985064\n",
      "Loss: 0.00039661475\n",
      "Loss: 0.0003952891\n",
      "Loss: 0.00039280576\n",
      "Loss: 0.00039207205\n",
      "Loss: 0.00039014753\n",
      "Loss: 0.00038834894\n",
      "Loss: 0.0003866619\n",
      "Loss: 0.00038376416\n",
      "Loss: 0.00038100302\n",
      "Loss: 0.0003791773\n",
      "Loss: 0.00037934186\n",
      "Loss: 0.0003786886\n",
      "Loss: 0.0003811961\n",
      "Loss: 0.000378403\n",
      "Loss: 0.0003778477\n",
      "Loss: 0.00038089824\n",
      "Loss: 0.00037717557\n",
      "Loss: 0.0003763547\n",
      "Loss: 0.00037519366\n",
      "Loss: 0.0003746537\n",
      "Loss: 0.00037428667\n",
      "Loss: 0.00037399592\n",
      "Loss: 0.0003736413\n",
      "Loss: 0.0003731442\n",
      "Loss: 0.00037241098\n",
      "Loss: 0.00037196133\n",
      "Loss: 0.00037056103\n",
      "Loss: 0.0003698788\n",
      "Loss: 0.00036917152\n",
      "Loss: 0.000368468\n",
      "Loss: 0.00036701228\n",
      "Loss: 0.0003661416\n",
      "Loss: 0.0003653937\n",
      "Loss: 0.00036461555\n",
      "Loss: 0.00036410883\n",
      "Loss: 0.00036334374\n",
      "Loss: 0.00036237252\n",
      "Loss: 0.000360584\n",
      "Loss: 0.00039714391\n",
      "Loss: 0.00036028854\n",
      "Loss: 0.00035947122\n",
      "Loss: 0.00035846554\n",
      "Loss: 0.00035785633\n",
      "Loss: 0.00035768826\n",
      "Loss: 0.000356588\n",
      "Loss: 0.00035592355\n",
      "Loss: 0.00035521868\n",
      "Loss: 0.00035555544\n",
      "Loss: 0.00035480142\n",
      "Loss: 0.00035405354\n",
      "Loss: 0.00035281022\n",
      "Loss: 0.00035171577\n",
      "Loss: 0.0003505566\n",
      "Loss: 0.00035011827\n",
      "Loss: 0.0003840616\n",
      "Loss: 0.00034969844\n",
      "Loss: 0.00034930033\n",
      "Loss: 0.00034800262\n",
      "Loss: 0.00034695645\n",
      "Loss: 0.00034568063\n",
      "Loss: 0.00036063086\n",
      "Loss: 0.0003452881\n",
      "Loss: 0.00034409142\n",
      "Loss: 0.0003465226\n",
      "Loss: 0.00034363213\n",
      "Loss: 0.00034260936\n",
      "Loss: 0.00034187644\n",
      "Loss: 0.00034154547\n",
      "Loss: 0.0003413271\n",
      "Loss: 0.0003402401\n",
      "Loss: 0.00033981324\n",
      "Loss: 0.0003393215\n",
      "Loss: 0.00033890182\n",
      "Loss: 0.0003380011\n",
      "Loss: 0.00033707946\n",
      "Loss: 0.0003365517\n",
      "Loss: 0.00036597077\n",
      "Loss: 0.00033642727\n",
      "Loss: 0.0003357915\n",
      "Loss: 0.00033536096\n",
      "Loss: 0.00033466244\n",
      "Loss: 0.00033321892\n",
      "Loss: 0.0003320717\n",
      "Loss: 0.00033105092\n",
      "Loss: 0.00033024227\n",
      "Loss: 0.00032971217\n",
      "Loss: 0.00032900914\n",
      "Loss: 0.00032805436\n",
      "Loss: 0.0003261955\n",
      "Loss: 0.0003254862\n",
      "Loss: 0.00032466918\n",
      "Loss: 0.00032412075\n",
      "Loss: 0.0003234679\n",
      "Loss: 0.00032291835\n",
      "Loss: 0.0003224186\n",
      "Loss: 0.00032181668\n",
      "Loss: 0.00032125635\n",
      "Loss: 0.00032072648\n",
      "Loss: 0.00032007345\n",
      "Loss: 0.00032382604\n",
      "Loss: 0.0003196984\n",
      "Loss: 0.0003189441\n",
      "Loss: 0.00031797498\n",
      "Loss: 0.0003168752\n",
      "Loss: 0.0003182634\n",
      "Loss: 0.00031658492\n",
      "Loss: 0.00031579906\n",
      "Loss: 0.00031510298\n",
      "Loss: 0.00031444128\n",
      "Loss: 0.00031325454\n",
      "Loss: 0.00031283655\n",
      "Loss: 0.00031755603\n",
      "Loss: 0.00031214274\n",
      "Loss: 0.00031131887\n",
      "Loss: 0.00031100475\n",
      "Loss: 0.00031048863\n",
      "Loss: 0.00030979182\n",
      "Loss: 0.00030876024\n",
      "Loss: 0.00030715409\n",
      "Loss: 0.00030509036\n",
      "Loss: 0.000303203\n",
      "Loss: 0.00030244698\n",
      "Loss: 0.00030188158\n",
      "Loss: 0.00030135238\n",
      "Loss: 0.00030049734\n",
      "Loss: 0.00030004903\n",
      "Loss: 0.00029914052\n",
      "Loss: 0.0003118404\n",
      "Loss: 0.00029871636\n",
      "Loss: 0.00029767127\n",
      "Loss: 0.0002961136\n",
      "Loss: 0.0002954485\n",
      "Loss: 0.00029480123\n",
      "Loss: 0.00029431662\n",
      "Loss: 0.0002975873\n",
      "Loss: 0.00029320866\n",
      "Loss: 0.00029217408\n",
      "Loss: 0.00029125216\n",
      "Loss: 0.00029047814\n",
      "Loss: 0.00029021426\n",
      "Loss: 0.00028965276\n",
      "Loss: 0.00028903031\n",
      "Loss: 0.00032770622\n",
      "Loss: 0.00028890063\n",
      "Loss: 0.00028829125\n",
      "Loss: 0.00028763124\n",
      "Loss: 0.00028685242\n",
      "Loss: 0.00028645247\n",
      "Loss: 0.0002862878\n",
      "Loss: 0.00028636958\n",
      "Loss: 0.00028616117\n",
      "Loss: 0.00028596816\n",
      "Loss: 0.00028537624\n",
      "Loss: 0.0002850768\n",
      "Loss: 0.00028469448\n",
      "Loss: 0.00028433418\n",
      "Loss: 0.00028405408\n",
      "Loss: 0.00028391992\n",
      "Loss: 0.00028365507\n",
      "Loss: 0.00028314226\n",
      "Loss: 0.0002821018\n",
      "Loss: 0.0002819653\n",
      "Loss: 0.00028118357\n",
      "Loss: 0.0002808476\n",
      "Loss: 0.0002803807\n",
      "Loss: 0.00027931237\n",
      "Loss: 0.000279057\n",
      "Loss: 0.00027879365\n",
      "Loss: 0.00027846187\n",
      "Loss: 0.00027772886\n",
      "Loss: 0.00027663918\n",
      "Loss: 0.00027587803\n",
      "Loss: 0.00027506394\n",
      "Loss: 0.00027416917\n",
      "Loss: 0.00027319265\n",
      "Loss: 0.00027280973\n",
      "Loss: 0.000272458\n",
      "Loss: 0.0002723062\n",
      "Loss: 0.00027217885\n",
      "Loss: 0.00027181068\n",
      "Loss: 0.0002725869\n",
      "Loss: 0.00027146385\n",
      "Loss: 0.00027090946\n",
      "Loss: 0.00027036713\n",
      "Loss: 0.00027006134\n",
      "Loss: 0.00026955604\n",
      "Loss: 0.00026929544\n",
      "Loss: 0.00026882658\n",
      "Loss: 0.00026852638\n",
      "Loss: 0.00026824104\n",
      "Loss: 0.0002680576\n",
      "Loss: 0.00026771036\n",
      "Loss: 0.00026763516\n",
      "Loss: 0.0002674251\n",
      "Loss: 0.00026685366\n",
      "Loss: 0.00026651376\n",
      "Loss: 0.00026603628\n",
      "Loss: 0.00026572667\n",
      "Loss: 0.00026520254\n",
      "Loss: 0.0002645853\n",
      "Loss: 0.00026449392\n",
      "Loss: 0.0002639117\n",
      "Loss: 0.0002634481\n",
      "Loss: 0.0002628533\n",
      "Loss: 0.00026221471\n",
      "Loss: 0.00026154864\n",
      "Loss: 0.00026102978\n",
      "Loss: 0.00026063464\n",
      "Loss: 0.00026032247\n",
      "Loss: 0.00026011723\n",
      "Loss: 0.00025993667\n",
      "Loss: 0.0002597414\n",
      "Loss: 0.00025945398\n",
      "Loss: 0.00026006417\n",
      "Loss: 0.0002592433\n",
      "Loss: 0.00025870476\n",
      "Loss: 0.00025816506\n",
      "Loss: 0.0002576942\n",
      "Loss: 0.00025701185\n",
      "Loss: 0.00025712894\n",
      "Loss: 0.0002566124\n",
      "Loss: 0.00025637553\n",
      "Loss: 0.0002562565\n",
      "Loss: 0.00025612657\n",
      "Loss: 0.00025591502\n",
      "Loss: 0.00025559874\n",
      "Loss: 0.0002553421\n",
      "Loss: 0.00025494368\n",
      "Loss: 0.00025462243\n",
      "Loss: 0.00025403214\n",
      "Loss: 0.00025340385\n",
      "Loss: 0.00025288766\n",
      "Loss: 0.00025237122\n",
      "Loss: 0.00025177712\n",
      "Loss: 0.00025621458\n",
      "Loss: 0.0002516216\n",
      "Loss: 0.00025107912\n",
      "Loss: 0.0002502689\n",
      "Loss: 0.00024894872\n",
      "Loss: 0.00024780896\n",
      "Loss: 0.0002463419\n",
      "Loss: 0.00024529273\n",
      "Loss: 0.0002447744\n",
      "Loss: 0.00024397325\n",
      "Loss: 0.00024320654\n",
      "Loss: 0.00024251931\n",
      "Loss: 0.00024199538\n",
      "Loss: 0.00024150795\n",
      "Loss: 0.00024093711\n",
      "Loss: 0.00024057228\n",
      "Loss: 0.00032630152\n",
      "Loss: 0.00023970439\n",
      "Loss: 0.00023894571\n",
      "Loss: 0.00023865956\n",
      "Loss: 0.00023724008\n",
      "Loss: 0.00023641213\n",
      "Loss: 0.00023585107\n",
      "Loss: 0.00023496692\n",
      "Loss: 0.00023407432\n",
      "Loss: 0.00023313663\n",
      "Loss: 0.00023226989\n",
      "Loss: 0.00023181803\n",
      "Loss: 0.00023136023\n",
      "Loss: 0.00023095925\n",
      "Loss: 0.0002302685\n",
      "Loss: 0.00022968263\n",
      "Loss: 0.00022918064\n",
      "Loss: 0.00023116931\n",
      "Loss: 0.0002289151\n",
      "Loss: 0.00022806501\n",
      "Loss: 0.00022745293\n",
      "Loss: 0.0002260964\n",
      "Loss: 0.00022512811\n",
      "Loss: 0.00022443743\n",
      "Loss: 0.00022403234\n",
      "Loss: 0.00022369323\n",
      "Loss: 0.00022338556\n",
      "Loss: 0.00022298672\n",
      "Loss: 0.00022254\n",
      "Loss: 0.00024983534\n",
      "Loss: 0.0002222764\n",
      "Loss: 0.00022139103\n",
      "Loss: 0.00022044196\n",
      "Loss: 0.00021993277\n",
      "Loss: 0.00021933476\n",
      "Loss: 0.00021888781\n",
      "Loss: 0.00021860583\n",
      "Loss: 0.00021826422\n",
      "Loss: 0.00021784891\n",
      "Loss: 0.00021737981\n",
      "Loss: 0.00021767351\n",
      "Loss: 0.00021698978\n",
      "Loss: 0.0002166077\n",
      "Loss: 0.0002158634\n",
      "Loss: 0.00021531315\n",
      "Loss: 0.00021432986\n",
      "Loss: 0.00021604166\n",
      "Loss: 0.00021398434\n",
      "Loss: 0.00021369752\n",
      "Loss: 0.00021349995\n",
      "Loss: 0.00021336912\n",
      "Loss: 0.00021281748\n",
      "Loss: 0.00021249843\n",
      "Loss: 0.00021520752\n",
      "Loss: 0.00021231992\n",
      "Loss: 0.00021158115\n",
      "Loss: 0.00021079212\n",
      "Loss: 0.00021005419\n",
      "Loss: 0.00020936\n",
      "Loss: 0.0002085836\n",
      "Loss: 0.0002081036\n",
      "Loss: 0.00020759914\n",
      "Loss: 0.00020783243\n",
      "Loss: 0.00020741147\n",
      "Loss: 0.00020670542\n",
      "Loss: 0.000206409\n",
      "Loss: 0.0002062046\n",
      "Loss: 0.00020681228\n",
      "Loss: 0.00020615687\n",
      "Loss: 0.00020603178\n",
      "Loss: 0.00020885581\n",
      "Loss: 0.00020594426\n",
      "Loss: 0.00020562517\n",
      "Loss: 0.00020541786\n",
      "Loss: 0.00020499672\n",
      "Loss: 0.00020441583\n",
      "Loss: 0.00020425247\n",
      "Loss: 0.00020336165\n",
      "Loss: 0.0002029895\n",
      "Loss: 0.00020239182\n",
      "Loss: 0.00020181765\n",
      "Loss: 0.00020057391\n",
      "Loss: 0.00019970033\n",
      "Loss: 0.00019874965\n",
      "Loss: 0.00019826504\n",
      "Loss: 0.00019770449\n",
      "Loss: 0.00019712611\n",
      "Loss: 0.00019918746\n",
      "Loss: 0.00019659472\n",
      "Loss: 0.00019726477\n",
      "Loss: 0.0001960912\n",
      "Loss: 0.00019532721\n",
      "Loss: 0.00019505213\n",
      "Loss: 0.00019476202\n",
      "Loss: 0.00019455602\n",
      "Loss: 0.00019408097\n",
      "Loss: 0.00019762409\n",
      "Loss: 0.0001939382\n",
      "Loss: 0.0001930639\n",
      "Loss: 0.00019257204\n",
      "Loss: 0.00019215108\n",
      "Loss: 0.00019184721\n",
      "Loss: 0.0001959097\n",
      "Loss: 0.00019132727\n",
      "Loss: 0.00019087947\n",
      "Loss: 0.0001899764\n",
      "Loss: 0.00018911524\n",
      "Loss: 0.00018809608\n",
      "Loss: 0.0001877393\n",
      "Loss: 0.00018745431\n",
      "Loss: 0.00018728356\n",
      "Loss: 0.00018690588\n",
      "Loss: 0.00020012795\n",
      "Loss: 0.0001868285\n",
      "Loss: 0.00018635782\n",
      "Loss: 0.00018577444\n",
      "Loss: 0.00018530806\n",
      "Loss: 0.00018485694\n",
      "Loss: 0.00018438612\n",
      "Loss: 0.0001839177\n",
      "Loss: 0.00018341254\n",
      "Loss: 0.0001829089\n",
      "Loss: 0.00018251826\n",
      "Loss: 0.0001819228\n",
      "Loss: 0.00018147976\n",
      "Loss: 0.00018089716\n",
      "Loss: 0.00018025714\n",
      "Loss: 0.00017974904\n",
      "Loss: 0.0001791068\n",
      "Loss: 0.00017891722\n",
      "Loss: 0.00017866885\n",
      "Loss: 0.00017843209\n",
      "Loss: 0.00017800766\n",
      "Loss: 0.00017772402\n",
      "Loss: 0.00017742957\n",
      "Loss: 0.00017719473\n",
      "Loss: 0.0001770331\n",
      "Loss: 0.00017693172\n",
      "Loss: 0.00017679803\n",
      "Loss: 0.00017661665\n",
      "Loss: 0.00017623843\n",
      "Loss: 0.00017673356\n",
      "Loss: 0.000176084\n",
      "Loss: 0.00017565339\n",
      "Loss: 0.0001751849\n",
      "Loss: 0.00017493327\n",
      "Loss: 0.00017464506\n",
      "Loss: 0.00017410035\n",
      "Loss: 0.0001864752\n",
      "Loss: 0.00017370467\n",
      "Loss: 0.00017292198\n",
      "Loss: 0.00017228466\n",
      "Loss: 0.0001718429\n",
      "Loss: 0.00017115897\n",
      "Loss: 0.0001703832\n",
      "Loss: 0.00016976373\n",
      "Loss: 0.00016938204\n",
      "Loss: 0.00016909733\n",
      "Loss: 0.00016887728\n",
      "Loss: 0.00016856246\n",
      "Loss: 0.00016822273\n",
      "Loss: 0.00016798504\n",
      "Loss: 0.00016779677\n",
      "Loss: 0.00016866715\n",
      "Loss: 0.00016773154\n",
      "Loss: 0.00016739049\n",
      "Loss: 0.00016709046\n",
      "Loss: 0.00016673752\n",
      "Loss: 0.00016642819\n",
      "Loss: 0.00016600887\n",
      "Loss: 0.0001656232\n",
      "Loss: 0.00016575672\n",
      "Loss: 0.0001653457\n",
      "Loss: 0.00016510685\n",
      "Loss: 0.00016480206\n",
      "Loss: 0.00016463565\n",
      "Loss: 0.00016421695\n",
      "Loss: 0.00016381989\n",
      "Loss: 0.00016344587\n",
      "Loss: 0.00016324266\n",
      "Loss: 0.00016307319\n",
      "Loss: 0.00016284054\n",
      "Loss: 0.0001625547\n",
      "Loss: 0.00016235135\n",
      "Loss: 0.0001627836\n",
      "Loss: 0.00016211659\n",
      "Loss: 0.00016183303\n",
      "Loss: 0.00016148102\n",
      "Loss: 0.0001612789\n",
      "Loss: 0.00016094744\n",
      "Loss: 0.00016070664\n",
      "Loss: 0.00016055861\n",
      "Loss: 0.00016035474\n",
      "Loss: 0.00016013508\n",
      "Loss: 0.0001600724\n",
      "Loss: 0.0001597514\n",
      "Loss: 0.0001594175\n",
      "Loss: 0.00015896969\n",
      "Loss: 0.00015871343\n",
      "Loss: 0.0001583638\n",
      "Loss: 0.00015805836\n",
      "Loss: 0.00015878811\n",
      "Loss: 0.00015788837\n",
      "Loss: 0.00015772319\n",
      "Loss: 0.00015704507\n",
      "Loss: 0.00015659933\n",
      "Loss: 0.00015621819\n",
      "Loss: 0.0001558185\n",
      "Loss: 0.0001554511\n",
      "Loss: 0.00015492375\n",
      "Loss: 0.00015444953\n",
      "Loss: 0.00015441056\n",
      "Loss: 0.00015408821\n",
      "Loss: 0.00015371694\n",
      "Loss: 0.00015316514\n",
      "Loss: 0.00015290665\n",
      "Loss: 0.0001529787\n",
      "Loss: 0.00015275547\n",
      "Loss: 0.00015252893\n",
      "Loss: 0.0001524218\n",
      "Loss: 0.00015192834\n",
      "Loss: 0.00015163832\n",
      "Loss: 0.00015104092\n",
      "Loss: 0.00015064777\n",
      "Loss: 0.00015024698\n",
      "Loss: 0.00014988714\n",
      "Loss: 0.00014962474\n",
      "Loss: 0.00014939904\n",
      "Loss: 0.00014923181\n",
      "Loss: 0.0001488833\n",
      "Loss: 0.00014849208\n",
      "Loss: 0.00014808119\n",
      "Loss: 0.00014794178\n",
      "Loss: 0.00014782077\n",
      "Loss: 0.00014808276\n",
      "Loss: 0.0001477137\n",
      "Loss: 0.0001474274\n",
      "Loss: 0.00014727635\n",
      "Loss: 0.00014708575\n",
      "Loss: 0.00014694093\n",
      "Loss: 0.00014665903\n",
      "Loss: 0.00014644169\n",
      "Loss: 0.0001607591\n",
      "Loss: 0.00014638287\n",
      "Loss: 0.00014600651\n",
      "Loss: 0.00014565057\n",
      "Loss: 0.000145387\n",
      "Loss: 0.00014537317\n",
      "Loss: 0.00014530413\n",
      "Loss: 0.00014516026\n",
      "Loss: 0.00014508066\n",
      "Loss: 0.00014478793\n",
      "Loss: 0.0001443652\n",
      "Loss: 0.00014430734\n",
      "Loss: 0.00014385305\n",
      "Loss: 0.00014365249\n",
      "Loss: 0.00014344392\n",
      "Loss: 0.00014326646\n",
      "Loss: 0.00014307829\n",
      "Loss: 0.00014761424\n",
      "Loss: 0.00014300215\n",
      "Loss: 0.00014285685\n",
      "Loss: 0.00014268485\n",
      "Loss: 0.00014244176\n",
      "Loss: 0.00014225187\n",
      "Loss: 0.00014207125\n",
      "Loss: 0.00014161841\n",
      "Loss: 0.00014130402\n",
      "Loss: 0.00014091066\n",
      "Loss: 0.00014058611\n",
      "Loss: 0.00014047445\n",
      "Loss: 0.00014029196\n",
      "Loss: 0.00014014464\n",
      "Loss: 0.00014003747\n",
      "Loss: 0.00013993529\n",
      "Loss: 0.00013989667\n",
      "Loss: 0.00013990887\n",
      "Loss: 0.00013979482\n",
      "Loss: 0.00013989882\n",
      "Loss: 0.0001397279\n",
      "Loss: 0.00013961302\n",
      "Loss: 0.00013945482\n",
      "Loss: 0.00013920125\n",
      "Loss: 0.00013884908\n",
      "Loss: 0.00013866616\n",
      "Loss: 0.00014312049\n",
      "Loss: 0.00013856473\n",
      "Loss: 0.0001383884\n",
      "Loss: 0.00013926564\n",
      "Loss: 0.00013822234\n",
      "Loss: 0.00013801212\n",
      "Loss: 0.00013782599\n",
      "Loss: 0.00013767697\n",
      "Loss: 0.00013788816\n",
      "Loss: 0.00013759297\n",
      "Loss: 0.00013745723\n",
      "Loss: 0.00013727768\n",
      "Loss: 0.00013718368\n",
      "Loss: 0.00013704179\n",
      "Loss: 0.00013746409\n",
      "Loss: 0.00013695909\n",
      "Loss: 0.00013680529\n",
      "Loss: 0.0001364914\n",
      "Loss: 0.00013631128\n",
      "Loss: 0.00013597905\n",
      "Loss: 0.00013555403\n",
      "Loss: 0.00013547044\n",
      "Loss: 0.00013529335\n",
      "Loss: 0.00013522423\n",
      "Loss: 0.00013511117\n",
      "Loss: 0.00013495481\n",
      "Loss: 0.00013603478\n",
      "Loss: 0.00013481863\n",
      "Loss: 0.00013455896\n",
      "Loss: 0.00013391982\n",
      "Loss: 0.00013373925\n",
      "Loss: 0.00013348812\n",
      "Loss: 0.00013324083\n",
      "Loss: 0.0001330173\n",
      "Loss: 0.00013278966\n",
      "Loss: 0.00013433711\n",
      "Loss: 0.00013259744\n",
      "Loss: 0.00013232313\n",
      "Loss: 0.00013195863\n",
      "Loss: 0.00013181087\n",
      "Loss: 0.0001316002\n",
      "Loss: 0.0001315579\n",
      "Loss: 0.0001312917\n",
      "Loss: 0.00013116456\n",
      "Loss: 0.00013100338\n",
      "Loss: 0.00013085586\n",
      "Loss: 0.00013186557\n",
      "Loss: 0.00013079937\n",
      "Loss: 0.000130587\n",
      "Loss: 0.00013033254\n",
      "Loss: 0.00013001965\n",
      "Loss: 0.00012976563\n",
      "Loss: 0.00012958221\n",
      "Loss: 0.00012918784\n",
      "Loss: 0.00012904528\n",
      "Loss: 0.0001289233\n",
      "Loss: 0.00012883212\n",
      "Loss: 0.00012871469\n",
      "Loss: 0.00012846498\n",
      "Loss: 0.00012833922\n",
      "Loss: 0.00012802855\n",
      "Loss: 0.00012872154\n",
      "Loss: 0.00012788526\n",
      "Loss: 0.00012763502\n",
      "Loss: 0.00012743931\n",
      "Loss: 0.00012729812\n",
      "Loss: 0.00012714253\n",
      "Loss: 0.00012697678\n",
      "Loss: 0.00012678343\n",
      "Loss: 0.00012668723\n",
      "Loss: 0.00012655336\n",
      "Loss: 0.0001267672\n",
      "Loss: 0.00012649005\n",
      "Loss: 0.00012628855\n",
      "Loss: 0.00012616426\n",
      "Loss: 0.0001260878\n",
      "Loss: 0.00012595687\n",
      "Loss: 0.00012578095\n",
      "Loss: 0.00012731658\n",
      "Loss: 0.0001257561\n",
      "Loss: 0.0001256992\n",
      "Loss: 0.00012558963\n",
      "Loss: 0.00012549246\n",
      "Loss: 0.0001253793\n",
      "Loss: 0.0001251954\n",
      "Loss: 0.00012498046\n",
      "Loss: 0.00012479314\n",
      "Loss: 0.00012466034\n",
      "Loss: 0.00012437394\n",
      "Loss: 0.00012419165\n",
      "Loss: 0.00012404972\n",
      "Loss: 0.00012397801\n",
      "Loss: 0.00012391571\n",
      "Loss: 0.00012385995\n",
      "Loss: 0.00012376004\n",
      "Loss: 0.00012364294\n",
      "Loss: 0.00012350087\n",
      "Loss: 0.00012340245\n",
      "Loss: 0.00012319896\n",
      "Loss: 0.00012300666\n",
      "Loss: 0.00012284181\n",
      "Loss: 0.00012267267\n",
      "Loss: 0.00012251877\n",
      "Loss: 0.00012312412\n",
      "Loss: 0.00012241155\n",
      "Loss: 0.00012231048\n",
      "Loss: 0.00012217597\n",
      "Loss: 0.00012211347\n",
      "Loss: 0.00012193038\n",
      "Loss: 0.00012255191\n",
      "Loss: 0.000121875826\n",
      "Loss: 0.00012166635\n",
      "Loss: 0.000121469064\n",
      "Loss: 0.00012168105\n",
      "Loss: 0.000121368765\n",
      "Loss: 0.00012122502\n",
      "Loss: 0.00012110796\n",
      "Loss: 0.000120859266\n",
      "Loss: 0.000120626224\n",
      "Loss: 0.00012038321\n",
      "Loss: 0.000120076904\n",
      "Loss: 0.000119798395\n",
      "Loss: 0.00011950829\n",
      "Loss: 0.000119364355\n",
      "Loss: 0.00011911127\n",
      "Loss: 0.000118852884\n",
      "Loss: 0.000118786906\n",
      "Loss: 0.00011841458\n",
      "Loss: 0.00011832248\n",
      "Loss: 0.0001182034\n",
      "Loss: 0.00011813253\n",
      "Loss: 0.00011804071\n",
      "Loss: 0.00011791356\n",
      "Loss: 0.000118501324\n",
      "Loss: 0.00011788597\n",
      "Loss: 0.000117768766\n",
      "Loss: 0.00011762537\n",
      "Loss: 0.00011755909\n",
      "Loss: 0.00011742835\n",
      "Loss: 0.000117291354\n",
      "Loss: 0.000116977244\n",
      "Loss: 0.00011667605\n",
      "Loss: 0.000116442665\n",
      "Loss: 0.00011608738\n",
      "Loss: 0.000115846\n",
      "Loss: 0.000115408635\n",
      "Loss: 0.00011519533\n",
      "Loss: 0.00011493846\n",
      "Loss: 0.00011490674\n",
      "Loss: 0.00011478025\n",
      "Loss: 0.00011470413\n",
      "Loss: 0.00011461413\n",
      "Loss: 0.00011447672\n",
      "Loss: 0.00011432938\n",
      "Loss: 0.00011423855\n",
      "Loss: 0.000114110626\n",
      "Loss: 0.00011391606\n",
      "Loss: 0.00011403453\n",
      "Loss: 0.0001138006\n",
      "Loss: 0.00011360372\n",
      "Loss: 0.00011344644\n",
      "Loss: 0.00011329029\n",
      "Loss: 0.00011309047\n",
      "Loss: 0.00011275752\n",
      "Loss: 0.00011237215\n",
      "Loss: 0.00011217651\n",
      "Loss: 0.000112040085\n",
      "Loss: 0.00011194544\n",
      "Loss: 0.000111779154\n",
      "Loss: 0.000111650435\n",
      "Loss: 0.0001115043\n",
      "Loss: 0.00011139331\n",
      "Loss: 0.00011126812\n",
      "Loss: 0.00011115689\n",
      "Loss: 0.00011101676\n",
      "Loss: 0.000110854075\n",
      "Loss: 0.0001106383\n",
      "Loss: 0.000114826995\n",
      "Loss: 0.00011038221\n",
      "Loss: 0.00011002738\n",
      "Loss: 0.00010949829\n",
      "Loss: 0.00010926164\n",
      "Loss: 0.00010883815\n",
      "Loss: 0.000109234505\n",
      "Loss: 0.00010867492\n",
      "Loss: 0.00010856558\n",
      "Loss: 0.00010817414\n",
      "Loss: 0.00010803976\n",
      "Loss: 0.000107816384\n",
      "Loss: 0.00010762425\n",
      "Loss: 0.0001073695\n",
      "Loss: 0.00013015102\n",
      "Loss: 0.00010721761\n",
      "Loss: 0.000107020605\n",
      "Loss: 0.00010683733\n",
      "Loss: 0.00010667928\n",
      "Loss: 0.00010652967\n",
      "Loss: 0.000106331645\n",
      "Loss: 0.00010613074\n",
      "Loss: 0.00010594858\n",
      "Loss: 0.0001057792\n",
      "Loss: 0.000105613464\n",
      "Loss: 0.00010546476\n",
      "Loss: 0.00010531717\n",
      "Loss: 0.00010514497\n",
      "Loss: 0.0001052404\n",
      "Loss: 0.00010505246\n",
      "Loss: 0.00010494487\n",
      "Loss: 0.00010477604\n",
      "Loss: 0.000104589766\n",
      "Loss: 0.00010426581\n",
      "Loss: 0.00010395227\n",
      "Loss: 0.00012233848\n",
      "Loss: 0.000103686136\n",
      "Loss: 0.00010352583\n",
      "Loss: 0.00010329081\n",
      "Loss: 0.00010316785\n",
      "Loss: 0.000103068676\n",
      "Loss: 0.00010297487\n",
      "Loss: 0.00010276113\n",
      "Loss: 0.00010437949\n",
      "Loss: 0.00010266467\n",
      "Loss: 0.000102332735\n",
      "Loss: 0.00010213309\n",
      "Loss: 0.00010184926\n",
      "Loss: 0.0001017937\n",
      "Loss: 0.00010170743\n",
      "Loss: 0.000101272104\n",
      "Loss: 0.00010115856\n",
      "Loss: 0.000100982834\n",
      "Loss: 0.000100925325\n",
      "Loss: 0.000100820966\n",
      "Loss: 0.00010074781\n",
      "Loss: 0.00010055989\n",
      "Loss: 0.00010024835\n",
      "Loss: 9.987909e-05\n",
      "Loss: 9.944856e-05\n",
      "Loss: 0.00010155878\n",
      "Loss: 9.933236e-05\n",
      "Loss: 9.9118995e-05\n",
      "Loss: 9.875619e-05\n",
      "Loss: 9.850207e-05\n",
      "Loss: 9.829548e-05\n",
      "Loss: 9.815405e-05\n",
      "Loss: 9.804836e-05\n",
      "Loss: 9.7948156e-05\n",
      "Loss: 9.782683e-05\n",
      "Loss: 9.768283e-05\n",
      "Loss: 9.7543176e-05\n",
      "Loss: 9.805515e-05\n",
      "Loss: 9.7476106e-05\n",
      "Loss: 9.7370954e-05\n",
      "Loss: 9.727487e-05\n",
      "Loss: 9.718563e-05\n",
      "Loss: 9.705653e-05\n",
      "Loss: 9.6976866e-05\n",
      "Loss: 9.6898984e-05\n",
      "Loss: 9.6768665e-05\n",
      "Loss: 9.6558266e-05\n",
      "Loss: 9.6336094e-05\n",
      "Loss: 9.6221454e-05\n",
      "Loss: 9.61675e-05\n",
      "Loss: 9.6139105e-05\n",
      "Loss: 9.611428e-05\n",
      "Loss: 9.608745e-05\n",
      "Loss: 9.6036805e-05\n",
      "Loss: 9.595124e-05\n",
      "Loss: 9.604657e-05\n",
      "Loss: 9.5873125e-05\n",
      "Loss: 9.5724754e-05\n",
      "Loss: 9.560964e-05\n",
      "Loss: 9.549088e-05\n",
      "Loss: 9.53837e-05\n",
      "Loss: 9.689937e-05\n",
      "Loss: 9.532663e-05\n",
      "Loss: 9.509468e-05\n",
      "Loss: 9.490471e-05\n",
      "Loss: 9.475092e-05\n",
      "Loss: 9.4633506e-05\n",
      "Loss: 9.446196e-05\n",
      "Loss: 9.4340285e-05\n",
      "Loss: 9.416368e-05\n",
      "Loss: 9.389113e-05\n",
      "Loss: 9.363396e-05\n",
      "Loss: 9.344144e-05\n",
      "Loss: 9.448838e-05\n",
      "Loss: 9.336243e-05\n",
      "Loss: 9.316864e-05\n",
      "Loss: 9.301024e-05\n",
      "Loss: 9.29143e-05\n",
      "Loss: 9.281266e-05\n",
      "Loss: 9.25834e-05\n",
      "Loss: 9.282844e-05\n",
      "Loss: 9.2423834e-05\n",
      "Loss: 9.220445e-05\n",
      "Loss: 9.199763e-05\n",
      "Loss: 9.182222e-05\n",
      "Loss: 9.1721544e-05\n",
      "Loss: 9.161701e-05\n",
      "Loss: 9.150924e-05\n",
      "Loss: 9.143741e-05\n",
      "Loss: 9.2199305e-05\n",
      "Loss: 9.1408234e-05\n",
      "Loss: 9.132239e-05\n",
      "Loss: 9.125583e-05\n",
      "Loss: 9.1188725e-05\n",
      "Loss: 9.108758e-05\n",
      "Loss: 9.1012604e-05\n",
      "Loss: 9.087356e-05\n",
      "Loss: 9.077527e-05\n",
      "Loss: 9.067966e-05\n",
      "Loss: 9.050344e-05\n",
      "Loss: 9.036024e-05\n",
      "Loss: 9.023978e-05\n",
      "Loss: 9.0122376e-05\n",
      "Loss: 8.993028e-05\n",
      "Loss: 8.9764595e-05\n",
      "Loss: 8.959304e-05\n",
      "Loss: 8.942881e-05\n",
      "Loss: 8.933385e-05\n",
      "Loss: 8.919989e-05\n",
      "Loss: 8.904224e-05\n",
      "Loss: 8.8910085e-05\n",
      "Loss: 8.881338e-05\n",
      "Loss: 0.0001590259\n",
      "Loss: 8.878426e-05\n",
      "Loss: 8.867089e-05\n",
      "Loss: 8.849538e-05\n",
      "Loss: 8.834695e-05\n",
      "Loss: 8.818968e-05\n",
      "Loss: 8.795399e-05\n",
      "Loss: 8.769123e-05\n",
      "Loss: 8.749521e-05\n",
      "Loss: 8.726101e-05\n",
      "Loss: 8.711404e-05\n",
      "Loss: 8.693379e-05\n",
      "Loss: 8.8572844e-05\n",
      "Loss: 8.688425e-05\n",
      "Loss: 8.675165e-05\n",
      "Loss: 8.659391e-05\n",
      "Loss: 8.6459375e-05\n",
      "Loss: 8.638043e-05\n",
      "Loss: 8.625038e-05\n",
      "Loss: 8.607701e-05\n",
      "Loss: 8.593414e-05\n",
      "Loss: 8.5797335e-05\n",
      "Loss: 8.565759e-05\n",
      "Loss: 8.554208e-05\n",
      "Loss: 8.535158e-05\n",
      "Loss: 8.519639e-05\n",
      "Loss: 8.509719e-05\n",
      "Loss: 8.503344e-05\n",
      "Loss: 8.4972286e-05\n",
      "Loss: 8.491293e-05\n",
      "Loss: 8.4828855e-05\n",
      "Loss: 8.4726315e-05\n",
      "Loss: 8.4794214e-05\n",
      "Loss: 8.467506e-05\n",
      "Loss: 8.4561936e-05\n",
      "Loss: 8.4473664e-05\n",
      "Loss: 8.441438e-05\n",
      "Loss: 8.4377956e-05\n",
      "Loss: 8.4308616e-05\n",
      "Loss: 8.421969e-05\n",
      "Loss: 8.447277e-05\n",
      "Loss: 8.4051775e-05\n",
      "Loss: 8.3831765e-05\n",
      "Loss: 8.359801e-05\n",
      "Loss: 8.334253e-05\n",
      "Loss: 8.3153594e-05\n",
      "Loss: 8.2942526e-05\n",
      "Loss: 8.2719474e-05\n",
      "Loss: 8.2566105e-05\n",
      "Loss: 8.243989e-05\n",
      "Loss: 8.231376e-05\n",
      "Loss: 8.2222265e-05\n",
      "Loss: 8.216553e-05\n",
      "Loss: 8.206806e-05\n",
      "Loss: 8.186987e-05\n",
      "Loss: 0.00017161237\n",
      "Loss: 8.184678e-05\n",
      "Loss: 8.1731494e-05\n",
      "Loss: 8.1676786e-05\n",
      "Loss: 8.1625956e-05\n",
      "Loss: 8.153093e-05\n",
      "Loss: 8.1323546e-05\n",
      "Loss: 8.109018e-05\n",
      "Loss: 8.096044e-05\n",
      "Loss: 8.056062e-05\n",
      "Loss: 8.039892e-05\n",
      "Loss: 8.0164806e-05\n",
      "Loss: 7.989378e-05\n",
      "Loss: 7.967474e-05\n",
      "Loss: 7.943953e-05\n",
      "Loss: 7.925651e-05\n",
      "Loss: 7.914362e-05\n",
      "Loss: 7.90607e-05\n",
      "Loss: 7.892664e-05\n",
      "Loss: 7.8724974e-05\n",
      "Loss: 7.868558e-05\n",
      "Loss: 7.8358265e-05\n",
      "Loss: 7.8329176e-05\n",
      "Loss: 7.8151876e-05\n",
      "Loss: 7.8042794e-05\n",
      "Loss: 7.792097e-05\n",
      "Loss: 7.777206e-05\n",
      "Loss: 7.759998e-05\n",
      "Loss: 7.940043e-05\n",
      "Loss: 7.749814e-05\n",
      "Loss: 7.740251e-05\n",
      "Loss: 7.713677e-05\n",
      "Loss: 7.7063436e-05\n",
      "Loss: 7.699955e-05\n",
      "Loss: 7.6882294e-05\n",
      "Loss: 7.663664e-05\n",
      "Loss: 7.654402e-05\n",
      "Loss: 7.6470686e-05\n",
      "Loss: 7.6427285e-05\n",
      "Loss: 7.638367e-05\n",
      "Loss: 7.632893e-05\n",
      "Loss: 7.626157e-05\n",
      "Loss: 7.620397e-05\n",
      "Loss: 7.609675e-05\n",
      "Loss: 7.6038e-05\n",
      "Loss: 7.5996184e-05\n",
      "Loss: 7.592538e-05\n",
      "Loss: 7.585384e-05\n",
      "Loss: 7.5790194e-05\n",
      "Loss: 7.576032e-05\n",
      "Loss: 7.562925e-05\n",
      "Loss: 7.5541524e-05\n",
      "Loss: 7.544415e-05\n",
      "Loss: 7.5317526e-05\n",
      "Loss: 7.5102515e-05\n",
      "Loss: 7.5018674e-05\n",
      "Loss: 7.483114e-05\n",
      "Loss: 7.468884e-05\n",
      "Loss: 7.460517e-05\n",
      "Loss: 7.450681e-05\n",
      "Loss: 7.435527e-05\n",
      "Loss: 7.448351e-05\n",
      "Loss: 7.4283846e-05\n",
      "Loss: 7.4135976e-05\n",
      "Loss: 7.404718e-05\n",
      "Loss: 7.397846e-05\n",
      "Loss: 7.393662e-05\n",
      "Loss: 7.384765e-05\n",
      "Loss: 7.375088e-05\n",
      "Loss: 7.415307e-05\n",
      "Loss: 7.370054e-05\n",
      "Loss: 7.360727e-05\n",
      "Loss: 7.33605e-05\n",
      "Loss: 7.321157e-05\n",
      "Loss: 7.30419e-05\n",
      "Loss: 7.287355e-05\n",
      "Loss: 7.254424e-05\n",
      "Loss: 7.2362076e-05\n",
      "Loss: 7.222297e-05\n",
      "Loss: 7.213646e-05\n",
      "Loss: 7.205595e-05\n",
      "Loss: 7.197124e-05\n",
      "Loss: 7.206025e-05\n",
      "Loss: 7.1927956e-05\n",
      "Loss: 7.4022355e-05\n",
      "Loss: 7.179234e-05\n",
      "Loss: 7.1685376e-05\n",
      "Loss: 7.155012e-05\n",
      "Loss: 7.139507e-05\n",
      "Loss: 7.138406e-05\n",
      "Loss: 7.1259565e-05\n",
      "Loss: 7.0991744e-05\n",
      "Loss: 7.07087e-05\n",
      "Loss: 7.046671e-05\n",
      "Loss: 7.0749506e-05\n",
      "Loss: 7.038042e-05\n",
      "Loss: 7.0122085e-05\n",
      "Loss: 6.9853784e-05\n",
      "Loss: 6.970894e-05\n",
      "Loss: 6.9601454e-05\n",
      "Loss: 6.954635e-05\n",
      "Loss: 6.939157e-05\n",
      "Loss: 6.932783e-05\n",
      "Loss: 6.924501e-05\n",
      "Loss: 6.911908e-05\n",
      "Loss: 6.9348054e-05\n",
      "Loss: 6.90269e-05\n",
      "Loss: 6.8835485e-05\n",
      "Loss: 6.861175e-05\n",
      "Loss: 6.85142e-05\n",
      "Loss: 6.845647e-05\n",
      "Loss: 6.8353154e-05\n",
      "Loss: 6.806421e-05\n",
      "Loss: 6.788505e-05\n",
      "Loss: 6.74896e-05\n",
      "Loss: 6.7329776e-05\n",
      "Loss: 6.7154375e-05\n",
      "Loss: 6.698044e-05\n",
      "Loss: 6.681749e-05\n",
      "Loss: 6.672734e-05\n",
      "Loss: 6.660406e-05\n",
      "Loss: 6.669721e-05\n",
      "Loss: 6.657255e-05\n",
      "Loss: 6.651484e-05\n",
      "Loss: 6.6458786e-05\n",
      "Loss: 6.634755e-05\n",
      "Loss: 6.610308e-05\n",
      "Loss: 6.589221e-05\n",
      "Loss: 6.8964786e-05\n",
      "Loss: 6.5821805e-05\n",
      "Loss: 6.564188e-05\n",
      "Loss: 6.548538e-05\n",
      "Loss: 6.532194e-05\n",
      "Loss: 6.519345e-05\n",
      "Loss: 6.4943e-05\n",
      "Loss: 6.4849344e-05\n",
      "Loss: 6.4741274e-05\n",
      "Loss: 6.496533e-05\n",
      "Loss: 6.469255e-05\n",
      "Loss: 6.458861e-05\n",
      "Loss: 6.445461e-05\n",
      "Loss: 6.4298496e-05\n",
      "Loss: 6.4328386e-05\n",
      "Loss: 6.4189604e-05\n",
      "Loss: 6.4095635e-05\n",
      "Loss: 6.4002335e-05\n",
      "Loss: 6.389576e-05\n",
      "Loss: 6.380006e-05\n",
      "Loss: 6.365214e-05\n",
      "Loss: 6.351401e-05\n",
      "Loss: 6.339837e-05\n",
      "Loss: 6.326734e-05\n",
      "Loss: 6.390966e-05\n",
      "Loss: 6.321733e-05\n",
      "Loss: 6.309039e-05\n",
      "Loss: 6.347758e-05\n",
      "Loss: 6.3044405e-05\n",
      "Loss: 6.4791326e-05\n",
      "Loss: 6.300533e-05\n",
      "Loss: 6.290352e-05\n",
      "Loss: 6.282712e-05\n",
      "Loss: 6.302225e-05\n",
      "Loss: 6.277583e-05\n",
      "Loss: 6.261959e-05\n",
      "Loss: 6.253958e-05\n",
      "Loss: 6.243467e-05\n",
      "Loss: 6.240283e-05\n",
      "Loss: 6.228465e-05\n",
      "Loss: 6.217607e-05\n",
      "Loss: 6.321599e-05\n",
      "Loss: 6.211131e-05\n",
      "Loss: 6.19712e-05\n",
      "Loss: 6.186029e-05\n",
      "Loss: 6.1758255e-05\n",
      "Loss: 6.1657985e-05\n",
      "Loss: 6.16018e-05\n",
      "Loss: 6.156682e-05\n",
      "Loss: 6.154587e-05\n",
      "Loss: 6.1515566e-05\n",
      "Loss: 6.1485895e-05\n",
      "Loss: 6.142708e-05\n",
      "Loss: 6.136467e-05\n",
      "Loss: 6.130534e-05\n",
      "Loss: 6.119891e-05\n",
      "Loss: 6.1095605e-05\n",
      "Loss: 6.102649e-05\n",
      "Loss: 6.096452e-05\n",
      "Loss: 6.0882165e-05\n",
      "Loss: 6.154706e-05\n",
      "Loss: 6.083952e-05\n",
      "Loss: 6.072678e-05\n",
      "Loss: 6.0653772e-05\n",
      "Loss: 6.0595357e-05\n",
      "Loss: 6.0467e-05\n",
      "Loss: 6.034282e-05\n",
      "Loss: 6.0248916e-05\n",
      "Loss: 6.0179525e-05\n",
      "Loss: 6.0124206e-05\n",
      "Loss: 6.0064078e-05\n",
      "Loss: 5.9989798e-05\n",
      "Loss: 5.990096e-05\n",
      "Loss: 5.9821177e-05\n",
      "Loss: 5.974564e-05\n",
      "Loss: 5.9696045e-05\n",
      "Loss: 5.9652917e-05\n",
      "Loss: 5.9599734e-05\n",
      "Loss: 5.9509708e-05\n",
      "Loss: 5.9352293e-05\n",
      "Loss: 5.9198348e-05\n",
      "Loss: 5.9141803e-05\n",
      "Loss: 5.9100086e-05\n",
      "Loss: 5.902508e-05\n",
      "Loss: 5.891105e-05\n",
      "Loss: 5.878966e-05\n",
      "Loss: 5.8691756e-05\n",
      "Loss: 5.867305e-05\n",
      "Loss: 5.855058e-05\n",
      "Loss: 5.8366815e-05\n",
      "Loss: 5.8233636e-05\n",
      "Loss: 5.81301e-05\n",
      "Loss: 5.803356e-05\n",
      "Loss: 5.796673e-05\n",
      "Loss: 5.783436e-05\n",
      "Loss: 5.7718913e-05\n",
      "Loss: 5.7583144e-05\n",
      "Loss: 5.7597215e-05\n",
      "Loss: 5.7529865e-05\n",
      "Loss: 5.7456684e-05\n",
      "Loss: 5.7343597e-05\n",
      "Loss: 5.7242796e-05\n",
      "Loss: 5.7089932e-05\n",
      "Loss: 7.0454844e-05\n",
      "Loss: 5.706462e-05\n",
      "Loss: 5.6948753e-05\n",
      "Loss: 5.6780646e-05\n",
      "Loss: 5.669768e-05\n",
      "Loss: 5.6621473e-05\n",
      "Loss: 5.6573077e-05\n",
      "Loss: 5.653139e-05\n",
      "Loss: 5.6438912e-05\n",
      "Loss: 5.653681e-05\n",
      "Loss: 5.636556e-05\n",
      "Loss: 5.6264507e-05\n",
      "Loss: 5.615022e-05\n",
      "Loss: 5.6050136e-05\n",
      "Loss: 5.599959e-05\n",
      "Loss: 5.5812117e-05\n",
      "Loss: 5.5716075e-05\n",
      "Loss: 5.562475e-05\n",
      "Loss: 5.5559838e-05\n",
      "Loss: 5.5511333e-05\n",
      "Loss: 5.545661e-05\n",
      "Loss: 5.5383138e-05\n",
      "Loss: 5.5298042e-05\n",
      "Loss: 5.5192355e-05\n",
      "Loss: 5.512785e-05\n",
      "Loss: 5.5066805e-05\n",
      "Loss: 5.5796823e-05\n",
      "Loss: 5.504272e-05\n",
      "Loss: 5.4995133e-05\n",
      "Loss: 5.496553e-05\n",
      "Loss: 5.4852833e-05\n",
      "Loss: 5.481358e-05\n",
      "Loss: 5.470847e-05\n",
      "Loss: 5.5061155e-05\n",
      "Loss: 5.4637625e-05\n",
      "Loss: 5.4554595e-05\n",
      "Loss: 5.4380806e-05\n",
      "Loss: 5.431868e-05\n",
      "Loss: 5.4263033e-05\n",
      "Loss: 5.4192573e-05\n",
      "Loss: 5.413226e-05\n",
      "Loss: 5.403073e-05\n",
      "Loss: 5.3977255e-05\n",
      "Loss: 5.39288e-05\n",
      "Loss: 5.390563e-05\n",
      "Loss: 5.3883483e-05\n",
      "Loss: 5.3856715e-05\n",
      "Loss: 5.3836826e-05\n",
      "Loss: 5.3810625e-05\n",
      "Loss: 5.376932e-05\n",
      "Loss: 5.370808e-05\n",
      "Loss: 5.362078e-05\n",
      "Loss: 5.3809883e-05\n",
      "Loss: 5.3573887e-05\n",
      "Loss: 5.3459284e-05\n",
      "Loss: 5.3336094e-05\n",
      "Loss: 5.324212e-05\n",
      "Loss: 5.317929e-05\n",
      "Loss: 5.314043e-05\n",
      "Loss: 5.309757e-05\n",
      "Loss: 5.3061995e-05\n",
      "Loss: 5.2988267e-05\n",
      "Loss: 5.295209e-05\n",
      "Loss: 5.2899166e-05\n",
      "Loss: 5.2844207e-05\n",
      "Loss: 5.280367e-05\n",
      "Loss: 5.272873e-05\n",
      "Loss: 5.262311e-05\n",
      "Loss: 5.302932e-05\n",
      "Loss: 5.2584503e-05\n",
      "Loss: 5.2466938e-05\n",
      "Loss: 5.237221e-05\n",
      "Loss: 5.231188e-05\n",
      "Loss: 5.2259948e-05\n",
      "Loss: 5.2186504e-05\n",
      "Loss: 5.2067895e-05\n",
      "Loss: 5.197322e-05\n",
      "Loss: 5.188818e-05\n",
      "Loss: 5.183256e-05\n",
      "Loss: 5.1784515e-05\n",
      "Loss: 5.1743275e-05\n",
      "Loss: 5.1660172e-05\n",
      "Loss: 5.164517e-05\n",
      "Loss: 5.1846433e-05\n",
      "Loss: 5.1540606e-05\n",
      "Loss: 5.1440282e-05\n",
      "Loss: 5.1374038e-05\n",
      "Loss: 5.1237024e-05\n",
      "Loss: 5.1139992e-05\n",
      "Loss: 5.106959e-05\n",
      "Loss: 5.1031275e-05\n",
      "Loss: 5.1002255e-05\n",
      "Loss: 5.0970626e-05\n",
      "Loss: 5.0912204e-05\n",
      "Loss: 5.218542e-05\n",
      "Loss: 5.0895353e-05\n",
      "Loss: 5.0867347e-05\n",
      "Loss: 5.081501e-05\n",
      "Loss: 5.0781928e-05\n",
      "Loss: 5.0751372e-05\n",
      "Loss: 5.0709426e-05\n",
      "Loss: 5.066395e-05\n",
      "Loss: 5.0631723e-05\n",
      "Loss: 5.0545037e-05\n",
      "Loss: 5.0518924e-05\n",
      "Loss: 5.0451476e-05\n",
      "Loss: 5.0393064e-05\n",
      "Loss: 5.0305367e-05\n",
      "Loss: 5.0253893e-05\n",
      "Loss: 5.0211784e-05\n",
      "Loss: 5.0165636e-05\n",
      "Loss: 5.0111794e-05\n",
      "Loss: 5.0047365e-05\n",
      "Loss: 5.010548e-05\n",
      "Loss: 5.001516e-05\n",
      "Loss: 4.9949762e-05\n",
      "Loss: 4.9883354e-05\n",
      "Loss: 4.9823266e-05\n",
      "Loss: 4.9759044e-05\n",
      "Loss: 4.9706254e-05\n",
      "Loss: 4.994867e-05\n",
      "Loss: 4.9693394e-05\n",
      "Loss: 4.9642324e-05\n",
      "Loss: 4.9583312e-05\n",
      "Loss: 4.952627e-05\n",
      "Loss: 4.94704e-05\n",
      "Loss: 4.9426453e-05\n",
      "Loss: 4.9385395e-05\n",
      "Loss: 4.9346243e-05\n",
      "Loss: 4.92732e-05\n",
      "Loss: 4.923424e-05\n",
      "Loss: 4.917406e-05\n",
      "Loss: 4.912555e-05\n",
      "Loss: 4.9097252e-05\n",
      "Loss: 4.8909864e-05\n",
      "Loss: 4.8832764e-05\n",
      "Loss: 4.8742164e-05\n",
      "Loss: 4.8637477e-05\n",
      "Loss: 4.8583424e-05\n",
      "Loss: 4.8498918e-05\n",
      "Loss: 4.8463568e-05\n",
      "Loss: 4.8384925e-05\n",
      "Loss: 4.83231e-05\n",
      "Loss: 4.8277398e-05\n",
      "Loss: 4.8254467e-05\n",
      "Loss: 4.8220136e-05\n",
      "Loss: 4.8185466e-05\n",
      "Loss: 4.8136608e-05\n",
      "Loss: 4.8082635e-05\n",
      "Loss: 4.8386453e-05\n",
      "Loss: 4.805904e-05\n",
      "Loss: 4.8010566e-05\n",
      "Loss: 4.797222e-05\n",
      "Loss: 4.7934744e-05\n",
      "Loss: 4.7892412e-05\n",
      "Loss: 4.783132e-05\n",
      "Loss: 4.777487e-05\n",
      "Loss: 4.773606e-05\n",
      "Loss: 4.766912e-05\n",
      "Loss: 4.7649562e-05\n",
      "Loss: 4.754532e-05\n",
      "Loss: 4.7443566e-05\n",
      "Loss: 4.7339487e-05\n",
      "Loss: 4.7258363e-05\n",
      "Loss: 4.7190708e-05\n",
      "Loss: 4.708757e-05\n",
      "Loss: 4.702157e-05\n",
      "Loss: 4.6895577e-05\n",
      "Loss: 4.6845398e-05\n",
      "Loss: 4.679978e-05\n",
      "Loss: 4.677276e-05\n",
      "Loss: 4.6742225e-05\n",
      "Loss: 4.668322e-05\n",
      "Loss: 4.662415e-05\n",
      "Loss: 4.657532e-05\n",
      "Loss: 4.650204e-05\n",
      "Loss: 4.6448855e-05\n",
      "Loss: 4.6399036e-05\n",
      "Loss: 4.6347457e-05\n",
      "Loss: 4.6328467e-05\n",
      "Loss: 4.630791e-05\n",
      "Loss: 4.629496e-05\n",
      "Loss: 4.625177e-05\n",
      "Loss: 4.6451038e-05\n",
      "Loss: 4.622462e-05\n",
      "Loss: 4.6131518e-05\n",
      "Loss: 4.6028315e-05\n",
      "Loss: 4.592647e-05\n",
      "Loss: 4.5815217e-05\n",
      "Loss: 4.566827e-05\n",
      "Loss: 4.5591947e-05\n",
      "Loss: 4.544668e-05\n",
      "Loss: 4.5411885e-05\n",
      "Loss: 4.5350338e-05\n",
      "Loss: 4.5285517e-05\n",
      "Loss: 4.5208937e-05\n",
      "Loss: 4.5303535e-05\n",
      "Loss: 4.516867e-05\n",
      "Loss: 4.5063676e-05\n",
      "Loss: 4.4943677e-05\n",
      "Loss: 4.4832326e-05\n",
      "Loss: 4.467747e-05\n",
      "Loss: 4.4590874e-05\n",
      "Loss: 4.4556513e-05\n",
      "Loss: 4.452872e-05\n",
      "Loss: 4.4488916e-05\n",
      "Loss: 4.4429737e-05\n",
      "Loss: 4.474866e-05\n",
      "Loss: 4.440551e-05\n",
      "Loss: 4.4358683e-05\n",
      "Loss: 4.4279055e-05\n",
      "Loss: 4.4323264e-05\n",
      "Loss: 4.424296e-05\n",
      "Loss: 4.4145578e-05\n",
      "Loss: 4.4019747e-05\n",
      "Loss: 4.3920987e-05\n",
      "Loss: 4.3856504e-05\n",
      "Loss: 4.3818964e-05\n",
      "Loss: 4.3768487e-05\n",
      "Loss: 4.372117e-05\n",
      "Loss: 4.3696742e-05\n",
      "Loss: 4.3672353e-05\n",
      "Loss: 4.3633885e-05\n",
      "Loss: 4.369771e-05\n",
      "Loss: 4.3601656e-05\n",
      "Loss: 4.3530625e-05\n",
      "Loss: 4.3506647e-05\n",
      "Loss: 4.344774e-05\n",
      "Loss: 4.3411008e-05\n",
      "Loss: 4.335557e-05\n",
      "Loss: 4.3270302e-05\n",
      "Loss: 4.3195905e-05\n",
      "Loss: 4.3135893e-05\n",
      "Loss: 4.30867e-05\n",
      "Loss: 4.313308e-05\n",
      "Loss: 4.307536e-05\n",
      "Loss: 4.304844e-05\n",
      "Loss: 4.301779e-05\n",
      "Loss: 4.2956985e-05\n",
      "Loss: 4.2895503e-05\n",
      "Loss: 4.291553e-05\n",
      "Loss: 4.285718e-05\n",
      "Loss: 4.2802698e-05\n",
      "Loss: 4.2725995e-05\n",
      "Loss: 4.2657553e-05\n",
      "Loss: 4.2590134e-05\n",
      "Loss: 4.2535015e-05\n",
      "Loss: 4.2457286e-05\n",
      "Loss: 4.2397136e-05\n",
      "Loss: 4.2361866e-05\n",
      "Loss: 4.2386193e-05\n",
      "Loss: 4.234166e-05\n",
      "Loss: 4.2314718e-05\n",
      "Loss: 4.2270887e-05\n",
      "Loss: 4.221824e-05\n",
      "Loss: 4.2174404e-05\n",
      "Loss: 4.2131345e-05\n",
      "Loss: 4.2065287e-05\n",
      "Loss: 4.198773e-05\n",
      "Loss: 4.1937652e-05\n",
      "Loss: 4.1892366e-05\n",
      "Loss: 4.18522e-05\n",
      "Loss: 4.1833842e-05\n",
      "Loss: 4.178191e-05\n",
      "Loss: 4.175281e-05\n",
      "Loss: 4.1717798e-05\n",
      "Loss: 4.1648687e-05\n",
      "Loss: 4.1573265e-05\n",
      "Loss: 4.145929e-05\n",
      "Loss: 4.138404e-05\n",
      "Loss: 4.1543273e-05\n",
      "Loss: 4.1310705e-05\n",
      "Loss: 4.1152027e-05\n",
      "Loss: 4.0944145e-05\n",
      "Loss: 4.0861873e-05\n",
      "Loss: 4.0812138e-05\n",
      "Loss: 4.0789208e-05\n",
      "Loss: 4.077673e-05\n",
      "Loss: 4.0759005e-05\n",
      "Loss: 4.07403e-05\n",
      "Loss: 4.071065e-05\n",
      "Loss: 4.0665906e-05\n",
      "Loss: 4.0557024e-05\n",
      "Loss: 4.04747e-05\n",
      "Loss: 4.0373496e-05\n",
      "Loss: 4.030042e-05\n",
      "Loss: 4.023926e-05\n",
      "Loss: 4.0199433e-05\n",
      "Loss: 4.0125415e-05\n",
      "Loss: 4.0090246e-05\n",
      "Loss: 4.0000534e-05\n",
      "Loss: 3.993592e-05\n",
      "Loss: 3.9999555e-05\n",
      "Loss: 3.989779e-05\n",
      "Loss: 3.9849863e-05\n",
      "Loss: 3.9795068e-05\n",
      "Loss: 3.9763258e-05\n",
      "Loss: 3.9720464e-05\n",
      "Loss: 3.9657643e-05\n",
      "Loss: 3.9595376e-05\n",
      "Loss: 3.9476876e-05\n",
      "Loss: 3.9414655e-05\n",
      "Loss: 3.9380513e-05\n",
      "Loss: 3.949442e-05\n",
      "Loss: 3.935946e-05\n",
      "Loss: 3.9413586e-05\n",
      "Loss: 3.9343955e-05\n",
      "Loss: 3.9317398e-05\n",
      "Loss: 3.9262508e-05\n",
      "Loss: 3.9218052e-05\n",
      "Loss: 3.9161743e-05\n",
      "Loss: 3.910325e-05\n",
      "Loss: 3.9031e-05\n",
      "Loss: 3.8948834e-05\n",
      "Loss: 3.890053e-05\n",
      "Loss: 3.8841616e-05\n",
      "Loss: 3.8819264e-05\n",
      "Loss: 3.880475e-05\n",
      "Loss: 3.878601e-05\n",
      "Loss: 3.876634e-05\n",
      "Loss: 3.8727027e-05\n",
      "Loss: 3.865421e-05\n",
      "Loss: 3.8551083e-05\n",
      "Loss: 3.8435195e-05\n",
      "Loss: 3.8297745e-05\n",
      "Loss: 3.8206585e-05\n",
      "Loss: 3.8135175e-05\n",
      "Loss: 3.8141712e-05\n",
      "Loss: 3.810292e-05\n",
      "Loss: 3.8063223e-05\n",
      "Loss: 3.8043556e-05\n",
      "Loss: 3.8030525e-05\n",
      "Loss: 3.8014467e-05\n",
      "Loss: 3.797569e-05\n",
      "Loss: 3.787267e-05\n",
      "Loss: 3.7810645e-05\n",
      "Loss: 3.7746813e-05\n",
      "Loss: 3.7687772e-05\n",
      "Loss: 3.763922e-05\n",
      "Loss: 3.7602727e-05\n",
      "Loss: 3.7583734e-05\n",
      "Loss: 3.7562393e-05\n",
      "Loss: 3.7479553e-05\n",
      "Loss: 3.7421178e-05\n",
      "Loss: 3.735267e-05\n",
      "Loss: 3.7308957e-05\n",
      "Loss: 3.7248563e-05\n",
      "Loss: 3.7196653e-05\n",
      "Loss: 3.718308e-05\n",
      "Loss: 3.711768e-05\n",
      "Loss: 3.7105106e-05\n",
      "Loss: 3.7080405e-05\n",
      "Loss: 3.7062207e-05\n",
      "Loss: 3.7039612e-05\n",
      "Loss: 3.6998514e-05\n",
      "Loss: 3.700159e-05\n",
      "Loss: 3.6975263e-05\n",
      "Loss: 3.6928497e-05\n",
      "Loss: 3.6871035e-05\n",
      "Loss: 3.683206e-05\n",
      "Loss: 3.6968537e-05\n",
      "Loss: 3.681511e-05\n",
      "Loss: 3.675029e-05\n",
      "Loss: 3.6666537e-05\n",
      "Loss: 3.6572204e-05\n",
      "Loss: 3.6479018e-05\n",
      "Loss: 3.6442063e-05\n",
      "Loss: 3.6415047e-05\n",
      "Loss: 3.638422e-05\n",
      "Loss: 3.6347312e-05\n",
      "Loss: 3.631076e-05\n",
      "Loss: 3.626302e-05\n",
      "Loss: 3.623228e-05\n",
      "Loss: 3.6252215e-05\n",
      "Loss: 3.6207864e-05\n",
      "Loss: 3.618677e-05\n",
      "Loss: 3.6150777e-05\n",
      "Loss: 3.6126436e-05\n",
      "Loss: 3.6083853e-05\n",
      "Loss: 3.6066154e-05\n",
      "Loss: 3.6009926e-05\n",
      "Loss: 3.5980866e-05\n",
      "Loss: 3.595223e-05\n",
      "Loss: 3.5927384e-05\n",
      "Loss: 3.5892648e-05\n",
      "Loss: 3.62419e-05\n",
      "Loss: 3.5883655e-05\n",
      "Loss: 3.5857585e-05\n",
      "Loss: 3.596104e-05\n",
      "Loss: 3.5845496e-05\n",
      "Loss: 3.5813475e-05\n",
      "Loss: 3.5775895e-05\n",
      "Loss: 3.5745237e-05\n",
      "Loss: 3.5697656e-05\n",
      "Loss: 3.5661425e-05\n",
      "Loss: 3.5624722e-05\n",
      "Loss: 3.559088e-05\n",
      "Loss: 3.5547557e-05\n",
      "Loss: 3.5672805e-05\n",
      "Loss: 3.5517285e-05\n",
      "Loss: 3.547709e-05\n",
      "Loss: 3.5440076e-05\n",
      "Loss: 3.5406352e-05\n",
      "Loss: 3.5354184e-05\n",
      "Loss: 3.5564124e-05\n",
      "Loss: 3.5318863e-05\n",
      "Loss: 3.529746e-05\n",
      "Loss: 3.528216e-05\n",
      "Loss: 3.5266487e-05\n",
      "Loss: 3.52555e-05\n",
      "Loss: 3.523272e-05\n",
      "Loss: 3.520979e-05\n",
      "Loss: 3.534377e-05\n",
      "Loss: 3.5194415e-05\n",
      "Loss: 3.5174686e-05\n",
      "Loss: 3.514564e-05\n",
      "Loss: 3.5130342e-05\n",
      "Loss: 3.5106903e-05\n",
      "Loss: 3.5069294e-05\n",
      "Loss: 3.503953e-05\n",
      "Loss: 3.5018125e-05\n",
      "Loss: 3.4981258e-05\n",
      "Loss: 3.4957593e-05\n",
      "Loss: 3.4918434e-05\n",
      "Loss: 3.4885947e-05\n",
      "Loss: 3.4840694e-05\n",
      "Loss: 3.480123e-05\n",
      "Loss: 3.5257457e-05\n",
      "Loss: 3.477304e-05\n",
      "Loss: 3.4748795e-05\n",
      "Loss: 3.472428e-05\n",
      "Loss: 3.471064e-05\n",
      "Loss: 3.4675606e-05\n",
      "Loss: 3.4640078e-05\n",
      "Loss: 3.4602916e-05\n",
      "Loss: 3.4579058e-05\n",
      "Loss: 3.4531942e-05\n",
      "Loss: 3.449656e-05\n",
      "Loss: 3.445209e-05\n",
      "Loss: 3.4390494e-05\n",
      "Loss: 3.4403416e-05\n",
      "Loss: 3.4355573e-05\n",
      "Loss: 3.4290097e-05\n",
      "Loss: 3.4259938e-05\n",
      "Loss: 3.4216628e-05\n",
      "Loss: 3.419991e-05\n",
      "Loss: 3.4175508e-05\n",
      "Loss: 3.413756e-05\n",
      "Loss: 3.4119665e-05\n",
      "Loss: 3.4051183e-05\n",
      "Loss: 3.3994864e-05\n",
      "Loss: 3.392157e-05\n",
      "Loss: 3.3877324e-05\n",
      "Loss: 3.385431e-05\n",
      "Loss: 3.3817683e-05\n",
      "Loss: 3.381211e-05\n",
      "Loss: 3.3784396e-05\n",
      "Loss: 3.373181e-05\n",
      "Loss: 3.3690452e-05\n",
      "Loss: 3.3666267e-05\n",
      "Loss: 3.3641518e-05\n",
      "Loss: 3.3567125e-05\n",
      "Loss: 3.3516197e-05\n",
      "Loss: 3.3504184e-05\n",
      "Loss: 3.3438722e-05\n",
      "Loss: 3.340305e-05\n",
      "Loss: 3.3355944e-05\n",
      "Loss: 3.3288597e-05\n",
      "Loss: 3.324795e-05\n",
      "Loss: 3.330279e-05\n",
      "Loss: 3.323575e-05\n",
      "Loss: 3.321083e-05\n",
      "Loss: 3.318095e-05\n",
      "Loss: 3.3158693e-05\n",
      "Loss: 3.3136188e-05\n",
      "Loss: 3.3096043e-05\n",
      "Loss: 3.3081546e-05\n",
      "Loss: 3.3028115e-05\n",
      "Loss: 3.300098e-05\n",
      "Loss: 3.2958054e-05\n",
      "Loss: 3.2856762e-05\n",
      "Loss: 3.278668e-05\n",
      "Loss: 3.275638e-05\n",
      "Loss: 3.2683707e-05\n",
      "Loss: 3.2628195e-05\n",
      "Loss: 3.256495e-05\n",
      "Loss: 3.2530363e-05\n",
      "Loss: 3.2511765e-05\n",
      "Loss: 3.2488304e-05\n",
      "Loss: 3.246386e-05\n",
      "Loss: 3.243534e-05\n",
      "Loss: 3.2403095e-05\n",
      "Loss: 3.2364056e-05\n",
      "Loss: 3.233407e-05\n",
      "Loss: 3.2294432e-05\n",
      "Loss: 3.2241784e-05\n",
      "Loss: 3.2464086e-05\n",
      "Loss: 3.2225136e-05\n",
      "Loss: 3.2539312e-05\n",
      "Loss: 3.220501e-05\n",
      "Loss: 3.217042e-05\n",
      "Loss: 3.214654e-05\n",
      "Loss: 3.2138152e-05\n",
      "Loss: 3.2126143e-05\n",
      "Loss: 3.2115822e-05\n",
      "Loss: 3.208078e-05\n",
      "Loss: 3.2065487e-05\n",
      "Loss: 3.2037737e-05\n",
      "Loss: 3.2022497e-05\n",
      "Loss: 3.2009157e-05\n",
      "Loss: 3.1981042e-05\n",
      "Loss: 3.1950323e-05\n",
      "Loss: 3.191551e-05\n",
      "Loss: 3.1898606e-05\n",
      "Loss: 3.186785e-05\n",
      "Loss: 3.1837495e-05\n",
      "Loss: 3.179812e-05\n",
      "Loss: 3.1787375e-05\n",
      "Loss: 3.1741023e-05\n",
      "Loss: 3.167101e-05\n",
      "Loss: 3.162893e-05\n",
      "Loss: 3.1561107e-05\n",
      "Loss: 3.1531097e-05\n",
      "Loss: 3.1484367e-05\n",
      "Loss: 3.1467465e-05\n",
      "Loss: 3.145244e-05\n",
      "Loss: 3.1444302e-05\n",
      "Loss: 3.1410105e-05\n",
      "Loss: 3.1380034e-05\n",
      "Loss: 3.131554e-05\n",
      "Loss: 3.127963e-05\n",
      "Loss: 3.1241805e-05\n",
      "Loss: 3.120883e-05\n",
      "Loss: 3.117269e-05\n",
      "Loss: 3.1119078e-05\n",
      "Loss: 3.121466e-05\n",
      "Loss: 3.1103966e-05\n",
      "Loss: 3.1071017e-05\n",
      "Loss: 3.1042153e-05\n",
      "Loss: 3.102082e-05\n",
      "Loss: 3.0992807e-05\n",
      "Loss: 3.0962365e-05\n",
      "Loss: 3.255936e-05\n",
      "Loss: 3.095393e-05\n",
      "Loss: 3.0923893e-05\n",
      "Loss: 3.094652e-05\n",
      "Loss: 3.0899148e-05\n",
      "Loss: 3.0858097e-05\n",
      "Loss: 3.0976586e-05\n",
      "Loss: 3.0833595e-05\n",
      "Loss: 3.080711e-05\n",
      "Loss: 3.0777825e-05\n",
      "Loss: 3.074342e-05\n",
      "Loss: 3.0671425e-05\n",
      "Loss: 3.065146e-05\n",
      "Loss: 3.0618507e-05\n",
      "Loss: 3.0605137e-05\n",
      "Loss: 3.0584826e-05\n",
      "Loss: 3.0570685e-05\n",
      "Loss: 3.0593415e-05\n",
      "Loss: 3.0551815e-05\n",
      "Loss: 3.0521234e-05\n",
      "Loss: 3.0438703e-05\n",
      "Loss: 3.0417916e-05\n",
      "Loss: 3.0381552e-05\n",
      "Loss: 3.0477455e-05\n",
      "Loss: 3.0357676e-05\n",
      "Loss: 3.0337942e-05\n",
      "Loss: 3.0302588e-05\n",
      "Loss: 3.0332389e-05\n",
      "Loss: 3.0294175e-05\n",
      "Loss: 3.0278141e-05\n",
      "Loss: 3.0251762e-05\n",
      "Loss: 3.0233125e-05\n",
      "Loss: 3.0208297e-05\n",
      "Loss: 3.0169569e-05\n",
      "Loss: 3.010147e-05\n",
      "Loss: 3.0053365e-05\n",
      "Loss: 2.999934e-05\n",
      "Loss: 2.9954906e-05\n",
      "Loss: 2.9920775e-05\n",
      "Loss: 2.9900602e-05\n",
      "Loss: 2.985614e-05\n",
      "Loss: 2.9789191e-05\n",
      "Loss: 2.9742674e-05\n",
      "Loss: 2.9706043e-05\n",
      "Loss: 2.9780345e-05\n",
      "Loss: 2.9674728e-05\n",
      "Loss: 2.9646282e-05\n",
      "Loss: 2.9596467e-05\n",
      "Loss: 2.9576531e-05\n",
      "Loss: 2.9515712e-05\n",
      "Loss: 2.9480401e-05\n",
      "Loss: 2.9464056e-05\n",
      "Loss: 2.9455121e-05\n",
      "Loss: 2.9432915e-05\n",
      "Loss: 2.9402427e-05\n",
      "Loss: 2.9368413e-05\n",
      "Loss: 2.9328789e-05\n",
      "Loss: 2.9299636e-05\n",
      "Loss: 2.9281582e-05\n",
      "Loss: 2.934505e-05\n",
      "Loss: 2.927287e-05\n",
      "Loss: 2.924447e-05\n",
      "Loss: 2.9216806e-05\n",
      "Loss: 2.9198745e-05\n",
      "Loss: 2.9182214e-05\n",
      "Loss: 2.9158397e-05\n",
      "Loss: 2.9197552e-05\n",
      "Loss: 2.9141052e-05\n",
      "Loss: 2.9107163e-05\n",
      "Loss: 2.906692e-05\n",
      "Loss: 2.9043627e-05\n",
      "Loss: 3.1468382e-05\n",
      "Loss: 2.9032062e-05\n",
      "Loss: 2.901041e-05\n",
      "Loss: 2.8986356e-05\n",
      "Loss: 2.8974064e-05\n",
      "Loss: 2.8946815e-05\n",
      "Loss: 2.890323e-05\n",
      "Loss: 2.8817012e-05\n",
      "Loss: 2.8825521e-05\n",
      "Loss: 2.8767556e-05\n",
      "Loss: 2.8703242e-05\n",
      "Loss: 2.8709937e-05\n",
      "Loss: 2.8680597e-05\n",
      "Loss: 2.8661936e-05\n",
      "Loss: 2.8646542e-05\n",
      "Loss: 2.861822e-05\n",
      "Loss: 2.858142e-05\n",
      "Loss: 2.8538023e-05\n",
      "Loss: 2.8495539e-05\n",
      "Loss: 2.8481016e-05\n",
      "Loss: 2.8437797e-05\n",
      "Loss: 2.840793e-05\n",
      "Loss: 2.8388298e-05\n",
      "Loss: 2.8371562e-05\n",
      "Loss: 2.8354683e-05\n",
      "Loss: 2.830883e-05\n",
      "Loss: 2.845307e-05\n",
      "Loss: 2.8301514e-05\n",
      "Loss: 2.8269962e-05\n",
      "Loss: 2.8245144e-05\n",
      "Loss: 2.8227094e-05\n",
      "Loss: 2.820898e-05\n",
      "Loss: 2.818157e-05\n",
      "Loss: 2.8253155e-05\n",
      "Loss: 2.8170018e-05\n",
      "Loss: 2.8137609e-05\n",
      "Loss: 2.8099843e-05\n",
      "Loss: 2.8068389e-05\n",
      "Loss: 2.8048167e-05\n",
      "Loss: 2.803168e-05\n",
      "Loss: 2.801172e-05\n",
      "Loss: 2.7984886e-05\n",
      "Loss: 2.7936263e-05\n",
      "Loss: 2.7917216e-05\n",
      "Loss: 2.7903596e-05\n",
      "Loss: 2.7886515e-05\n",
      "Loss: 2.7847784e-05\n",
      "Loss: 2.7829785e-05\n",
      "Loss: 2.7773476e-05\n",
      "Loss: 2.7745202e-05\n",
      "Loss: 2.7696522e-05\n",
      "Loss: 2.7674958e-05\n",
      "Loss: 2.7616737e-05\n",
      "Loss: 2.7590204e-05\n",
      "Loss: 2.7561255e-05\n",
      "Loss: 2.7525008e-05\n",
      "Loss: 2.7479382e-05\n",
      "Loss: 2.7432732e-05\n",
      "Loss: 2.739417e-05\n",
      "Loss: 2.8216064e-05\n",
      "Loss: 2.7380214e-05\n",
      "Loss: 2.7330598e-05\n",
      "Loss: 2.7297787e-05\n",
      "Loss: 2.7269276e-05\n",
      "Loss: 2.7225948e-05\n",
      "Loss: 2.7187802e-05\n",
      "Loss: 2.7163143e-05\n",
      "Loss: 2.713147e-05\n",
      "Loss: 2.7117349e-05\n",
      "Loss: 2.7108374e-05\n",
      "Loss: 2.709905e-05\n",
      "Loss: 2.7087406e-05\n",
      "Loss: 2.7069922e-05\n",
      "Loss: 2.7033117e-05\n",
      "Loss: 2.6996515e-05\n",
      "Loss: 2.6976035e-05\n",
      "Loss: 2.6952404e-05\n",
      "Loss: 2.6967138e-05\n",
      "Loss: 2.6935213e-05\n",
      "Loss: 2.6889975e-05\n",
      "Loss: 2.6839314e-05\n",
      "Loss: 2.680036e-05\n",
      "Loss: 2.6757174e-05\n",
      "Loss: 2.7179882e-05\n",
      "Loss: 2.6747734e-05\n",
      "Loss: 2.6703356e-05\n",
      "Loss: 2.665395e-05\n",
      "Loss: 2.6605492e-05\n",
      "Loss: 2.6581289e-05\n",
      "Loss: 2.6566322e-05\n",
      "Loss: 2.6541584e-05\n",
      "Loss: 2.6513273e-05\n",
      "Loss: 2.6497451e-05\n",
      "Loss: 2.6480073e-05\n",
      "Loss: 2.6488045e-05\n",
      "Loss: 2.6454485e-05\n",
      "Loss: 2.643897e-05\n",
      "Loss: 2.6420837e-05\n",
      "Loss: 2.6402646e-05\n",
      "Loss: 2.6431639e-05\n",
      "Loss: 2.6387119e-05\n",
      "Loss: 2.635192e-05\n",
      "Loss: 2.6324062e-05\n",
      "Loss: 2.6292155e-05\n",
      "Loss: 2.6260446e-05\n",
      "Loss: 2.6247239e-05\n",
      "Loss: 2.6188363e-05\n",
      "Loss: 2.6161882e-05\n",
      "Loss: 2.6140446e-05\n",
      "Loss: 2.6396323e-05\n",
      "Loss: 2.6124879e-05\n",
      "Loss: 2.6092777e-05\n",
      "Loss: 2.6024474e-05\n",
      "Loss: 2.5973293e-05\n",
      "Loss: 2.5918132e-05\n",
      "Loss: 2.5880488e-05\n",
      "Loss: 2.5847878e-05\n",
      "Loss: 2.5819632e-05\n",
      "Loss: 2.5796835e-05\n",
      "Loss: 2.5852914e-05\n",
      "Loss: 2.578174e-05\n",
      "Loss: 2.5752033e-05\n",
      "Loss: 2.5705578e-05\n",
      "Loss: 2.565421e-05\n",
      "Loss: 2.5722184e-05\n",
      "Loss: 2.5634294e-05\n",
      "Loss: 2.5587324e-05\n",
      "Loss: 2.5523725e-05\n",
      "Loss: 2.579469e-05\n",
      "Loss: 2.5507685e-05\n",
      "Loss: 2.5448462e-05\n",
      "Loss: 2.5385265e-05\n",
      "Loss: 2.531349e-05\n",
      "Loss: 2.5291301e-05\n",
      "Loss: 2.5252033e-05\n",
      "Loss: 2.5234696e-05\n",
      "Loss: 2.520592e-05\n",
      "Loss: 2.5186135e-05\n",
      "Loss: 2.513597e-05\n",
      "Loss: 2.5111376e-05\n",
      "Loss: 2.5074994e-05\n",
      "Loss: 2.506006e-05\n",
      "Loss: 2.504616e-05\n",
      "Loss: 2.5022298e-05\n",
      "Loss: 2.4996383e-05\n",
      "Loss: 2.4983707e-05\n",
      "Loss: 2.4959443e-05\n",
      "Loss: 2.4939258e-05\n",
      "Loss: 2.4926307e-05\n",
      "Loss: 2.4913523e-05\n",
      "Loss: 2.4892914e-05\n",
      "Loss: 2.487124e-05\n",
      "Loss: 2.485123e-05\n",
      "Loss: 2.4819077e-05\n",
      "Loss: 2.4771078e-05\n",
      "Loss: 2.472308e-05\n",
      "Loss: 2.4686562e-05\n",
      "Loss: 2.4667635e-05\n",
      "Loss: 2.4638679e-05\n",
      "Loss: 2.4620958e-05\n",
      "Loss: 2.4604904e-05\n",
      "Loss: 2.4586427e-05\n",
      "Loss: 2.4554882e-05\n",
      "Loss: 2.4500987e-05\n",
      "Loss: 2.4437311e-05\n",
      "Loss: 2.4370254e-05\n",
      "Loss: 2.430544e-05\n",
      "Loss: 2.4323293e-05\n",
      "Loss: 2.429198e-05\n",
      "Loss: 2.4827037e-05\n",
      "Loss: 2.4284018e-05\n",
      "Loss: 2.4262114e-05\n",
      "Loss: 2.4246592e-05\n",
      "Loss: 2.4233945e-05\n",
      "Loss: 2.4221896e-05\n",
      "Loss: 2.420144e-05\n",
      "Loss: 2.417971e-05\n",
      "Loss: 2.4170009e-05\n",
      "Loss: 2.415682e-05\n",
      "Loss: 2.413983e-05\n",
      "Loss: 2.4399693e-05\n",
      "Loss: 2.4133311e-05\n",
      "Loss: 2.4112085e-05\n",
      "Loss: 2.4097453e-05\n",
      "Loss: 2.408231e-05\n",
      "Loss: 2.406896e-05\n",
      "Loss: 2.4041317e-05\n",
      "Loss: 2.3994184e-05\n",
      "Loss: 2.3973842e-05\n",
      "Loss: 2.4014778e-05\n",
      "Loss: 2.3953418e-05\n",
      "Loss: 2.3929406e-05\n",
      "Loss: 2.3915458e-05\n",
      "Loss: 2.3893921e-05\n",
      "Loss: 2.387287e-05\n",
      "Loss: 2.4153185e-05\n",
      "Loss: 2.386439e-05\n",
      "Loss: 2.385048e-05\n",
      "Loss: 2.3813162e-05\n",
      "Loss: 2.3781005e-05\n",
      "Loss: 2.3747521e-05\n",
      "Loss: 2.3755185e-05\n",
      "Loss: 2.3728022e-05\n",
      "Loss: 2.3703677e-05\n",
      "Loss: 2.3681852e-05\n",
      "Loss: 2.366427e-05\n",
      "Loss: 2.3634913e-05\n",
      "Loss: 2.366853e-05\n",
      "Loss: 2.3617384e-05\n",
      "Loss: 2.3593839e-05\n",
      "Loss: 2.3575652e-05\n",
      "Loss: 2.3550536e-05\n",
      "Loss: 2.3521025e-05\n",
      "Loss: 2.348129e-05\n",
      "Loss: 2.3440252e-05\n",
      "Loss: 2.3410219e-05\n",
      "Loss: 2.3379995e-05\n",
      "Loss: 2.3351218e-05\n",
      "Loss: 2.344598e-05\n",
      "Loss: 2.3332192e-05\n",
      "Loss: 2.328998e-05\n",
      "Loss: 2.3225688e-05\n",
      "Loss: 2.3188537e-05\n",
      "Loss: 2.315241e-05\n",
      "Loss: 2.3118942e-05\n",
      "Loss: 2.3063843e-05\n",
      "Loss: 2.296687e-05\n",
      "Loss: 2.3116228e-05\n",
      "Loss: 2.2956536e-05\n",
      "Loss: 2.2909928e-05\n",
      "Loss: 2.2889773e-05\n",
      "Loss: 2.2876078e-05\n",
      "Loss: 2.2847296e-05\n",
      "Loss: 2.2824834e-05\n",
      "Loss: 2.2797542e-05\n",
      "Loss: 2.2771379e-05\n",
      "Loss: 2.2765227e-05\n",
      "Loss: 2.2681295e-05\n",
      "Loss: 2.264289e-05\n",
      "Loss: 2.260049e-05\n",
      "Loss: 2.256703e-05\n",
      "Loss: 2.2590175e-05\n",
      "Loss: 2.2542166e-05\n",
      "Loss: 2.2485088e-05\n",
      "Loss: 2.2442971e-05\n",
      "Loss: 2.2389158e-05\n",
      "Loss: 2.2356497e-05\n",
      "Loss: 2.2320835e-05\n",
      "Loss: 2.2296203e-05\n",
      "Loss: 2.2282216e-05\n",
      "Loss: 2.2257835e-05\n",
      "Loss: 2.2240061e-05\n",
      "Loss: 2.221713e-05\n",
      "Loss: 2.2244942e-05\n",
      "Loss: 2.2208307e-05\n",
      "Loss: 2.2184862e-05\n",
      "Loss: 2.2160675e-05\n",
      "Loss: 2.2145705e-05\n",
      "Loss: 2.213568e-05\n",
      "Loss: 2.2126309e-05\n",
      "Loss: 2.2112712e-05\n",
      "Loss: 2.2097405e-05\n",
      "Loss: 2.2076942e-05\n",
      "Loss: 2.2056829e-05\n",
      "Loss: 2.2080148e-05\n",
      "Loss: 2.2043088e-05\n",
      "Loss: 2.2021122e-05\n",
      "Loss: 2.1963875e-05\n",
      "Loss: 2.1934924e-05\n",
      "Loss: 2.1907175e-05\n",
      "Loss: 2.1880136e-05\n",
      "Loss: 2.1846747e-05\n",
      "Loss: 2.182356e-05\n",
      "Loss: 2.179235e-05\n",
      "Loss: 2.1754524e-05\n",
      "Loss: 2.1726386e-05\n",
      "Loss: 2.169831e-05\n",
      "Loss: 2.1668751e-05\n",
      "Loss: 2.1642118e-05\n",
      "Loss: 2.1622453e-05\n",
      "Loss: 2.1564509e-05\n",
      "Loss: 2.1523214e-05\n",
      "Loss: 3.068003e-05\n",
      "Loss: 2.1517928e-05\n",
      "Loss: 2.1473712e-05\n",
      "Loss: 2.144852e-05\n",
      "Loss: 2.1427284e-05\n",
      "Loss: 2.1383225e-05\n",
      "Loss: 2.1388892e-05\n",
      "Loss: 2.135809e-05\n",
      "Loss: 2.1323936e-05\n",
      "Loss: 2.1310234e-05\n",
      "Loss: 2.1289357e-05\n",
      "Loss: 2.125374e-05\n",
      "Loss: 2.1218179e-05\n",
      "Loss: 2.119162e-05\n",
      "Loss: 2.1180465e-05\n",
      "Loss: 2.1148255e-05\n",
      "Loss: 2.1164256e-05\n",
      "Loss: 2.1116324e-05\n",
      "Loss: 2.1081665e-05\n",
      "Loss: 2.1053029e-05\n",
      "Loss: 2.1032405e-05\n",
      "Loss: 2.1022704e-05\n",
      "Loss: 2.1004686e-05\n",
      "Loss: 2.0966036e-05\n",
      "Loss: 2.0922027e-05\n",
      "Loss: 2.0907648e-05\n",
      "Loss: 2.083147e-05\n",
      "Loss: 2.0798492e-05\n",
      "Loss: 2.0755837e-05\n",
      "Loss: 2.0723119e-05\n",
      "Loss: 2.0704847e-05\n",
      "Loss: 2.0668207e-05\n",
      "Loss: 2.0640868e-05\n",
      "Loss: 2.0594274e-05\n",
      "Loss: 2.057101e-05\n",
      "Loss: 2.0547552e-05\n",
      "Loss: 2.0503307e-05\n",
      "Loss: 2.0474765e-05\n",
      "Loss: 2.045395e-05\n",
      "Loss: 2.0434629e-05\n",
      "Loss: 2.0404697e-05\n",
      "Loss: 2.0370117e-05\n",
      "Loss: 2.0327003e-05\n",
      "Loss: 2.027221e-05\n",
      "Loss: 2.0362922e-05\n",
      "Loss: 2.0238795e-05\n",
      "Loss: 2.0176873e-05\n",
      "Loss: 2.0119745e-05\n",
      "Loss: 2.0047379e-05\n",
      "Loss: 1.9997899e-05\n",
      "Loss: 1.9960982e-05\n",
      "Loss: 1.9927815e-05\n",
      "Loss: 1.9892712e-05\n",
      "Loss: 1.9862833e-05\n",
      "Loss: 1.9839274e-05\n",
      "Loss: 1.9803505e-05\n",
      "Loss: 2.108081e-05\n",
      "Loss: 1.9799116e-05\n",
      "Loss: 1.976284e-05\n",
      "Loss: 1.973366e-05\n",
      "Loss: 1.9707495e-05\n",
      "Loss: 1.9689936e-05\n",
      "Loss: 1.9685045e-05\n",
      "Loss: 1.9675184e-05\n",
      "Loss: 1.966558e-05\n",
      "Loss: 1.9653395e-05\n",
      "Loss: 1.9636005e-05\n",
      "Loss: 1.9796755e-05\n",
      "Loss: 1.9622554e-05\n",
      "Loss: 1.9604211e-05\n",
      "Loss: 1.9579367e-05\n",
      "Loss: 1.9539535e-05\n",
      "Loss: 1.9475918e-05\n",
      "Loss: 1.9433255e-05\n",
      "Loss: 1.9396653e-05\n",
      "Loss: 1.9362176e-05\n",
      "Loss: 1.9332416e-05\n",
      "Loss: 1.9307216e-05\n",
      "Loss: 1.9287378e-05\n",
      "Loss: 1.9262443e-05\n",
      "Loss: 1.9234998e-05\n",
      "Loss: 1.9213252e-05\n",
      "Loss: 1.9182018e-05\n",
      "Loss: 1.9163459e-05\n",
      "Loss: 1.9120898e-05\n",
      "Loss: 1.9094306e-05\n",
      "Loss: 1.9365703e-05\n",
      "Loss: 1.9079553e-05\n",
      "Loss: 1.903969e-05\n",
      "Loss: 1.9008989e-05\n",
      "Loss: 1.897474e-05\n",
      "Loss: 1.8947014e-05\n",
      "Loss: 1.8920706e-05\n",
      "Loss: 1.8896611e-05\n",
      "Loss: 1.887124e-05\n",
      "Loss: 1.883574e-05\n",
      "Loss: 1.8813524e-05\n",
      "Loss: 2.0835378e-05\n",
      "Loss: 1.8805049e-05\n",
      "Loss: 1.8773137e-05\n",
      "Loss: 1.8758634e-05\n",
      "Loss: 1.8750177e-05\n",
      "Loss: 1.8737468e-05\n",
      "Loss: 1.8718847e-05\n",
      "Loss: 1.870149e-05\n",
      "Loss: 1.8682533e-05\n",
      "Loss: 1.8661216e-05\n",
      "Loss: 1.863655e-05\n",
      "Loss: 1.8618248e-05\n",
      "Loss: 1.8600243e-05\n",
      "Loss: 1.8591607e-05\n",
      "Loss: 1.856574e-05\n",
      "Loss: 1.8559636e-05\n",
      "Loss: 1.8539555e-05\n",
      "Loss: 1.8527095e-05\n",
      "Loss: 1.8502797e-05\n",
      "Loss: 1.8486542e-05\n",
      "Loss: 1.8467736e-05\n",
      "Loss: 1.8453538e-05\n",
      "Loss: 1.8443961e-05\n",
      "Loss: 1.8429346e-05\n",
      "Loss: 1.841441e-05\n",
      "Loss: 1.8399509e-05\n",
      "Loss: 1.8377561e-05\n",
      "Loss: 1.83551e-05\n",
      "Loss: 1.830345e-05\n",
      "Loss: 1.8261988e-05\n",
      "Loss: 1.8232464e-05\n",
      "Loss: 1.8202729e-05\n",
      "Loss: 1.8191804e-05\n",
      "Loss: 1.81833e-05\n",
      "Loss: 1.8170931e-05\n",
      "Loss: 1.8145844e-05\n",
      "Loss: 1.813513e-05\n",
      "Loss: 1.810843e-05\n",
      "Loss: 1.8093591e-05\n",
      "Loss: 1.8066297e-05\n",
      "Loss: 1.8043085e-05\n",
      "Loss: 1.8015948e-05\n",
      "Loss: 1.8000568e-05\n",
      "Loss: 1.7988848e-05\n",
      "Loss: 1.7968503e-05\n",
      "Loss: 1.7990184e-05\n",
      "Loss: 1.7953893e-05\n",
      "Loss: 1.793045e-05\n",
      "Loss: 1.7910428e-05\n",
      "Loss: 1.7920771e-05\n",
      "Loss: 1.7905386e-05\n",
      "Loss: 1.7897124e-05\n",
      "Loss: 1.7886843e-05\n",
      "Loss: 1.7877697e-05\n",
      "Loss: 1.7864906e-05\n",
      "Loss: 1.7837783e-05\n",
      "Loss: 1.782901e-05\n",
      "Loss: 1.7783525e-05\n",
      "Loss: 1.776202e-05\n",
      "Loss: 1.7706874e-05\n",
      "Loss: 1.768176e-05\n",
      "Loss: 1.7659488e-05\n",
      "Loss: 1.764002e-05\n",
      "Loss: 1.7668774e-05\n",
      "Loss: 1.7628732e-05\n",
      "Loss: 1.7608727e-05\n",
      "Loss: 1.7585127e-05\n",
      "Loss: 1.7553184e-05\n",
      "Loss: 1.7523298e-05\n",
      "Loss: 1.7522312e-05\n",
      "Loss: 1.7499422e-05\n",
      "Loss: 1.7463928e-05\n",
      "Loss: 1.7443832e-05\n",
      "Loss: 1.7403821e-05\n",
      "Loss: 1.7376316e-05\n",
      "Loss: 1.770251e-05\n",
      "Loss: 1.7357175e-05\n",
      "Loss: 1.7322283e-05\n",
      "Loss: 1.7280603e-05\n",
      "Loss: 1.7258608e-05\n",
      "Loss: 1.7241015e-05\n",
      "Loss: 1.7226992e-05\n",
      "Loss: 1.7218472e-05\n",
      "Loss: 1.7213539e-05\n",
      "Loss: 1.7209228e-05\n",
      "Loss: 1.7199452e-05\n",
      "Loss: 1.7201593e-05\n",
      "Loss: 1.7189957e-05\n",
      "Loss: 1.7177554e-05\n",
      "Loss: 1.716769e-05\n",
      "Loss: 1.7156573e-05\n",
      "Loss: 1.7144923e-05\n",
      "Loss: 1.712865e-05\n",
      "Loss: 1.7120345e-05\n",
      "Loss: 1.7109625e-05\n",
      "Loss: 1.7091954e-05\n",
      "Loss: 1.712441e-05\n",
      "Loss: 1.708028e-05\n",
      "Loss: 1.7064063e-05\n",
      "Loss: 1.7055332e-05\n",
      "Loss: 1.706193e-05\n",
      "Loss: 1.7050112e-05\n",
      "Loss: 1.7043534e-05\n",
      "Loss: 1.7019303e-05\n",
      "Loss: 1.700231e-05\n",
      "Loss: 1.6992557e-05\n",
      "Loss: 1.6976841e-05\n",
      "Loss: 1.7232323e-05\n",
      "Loss: 1.6974876e-05\n",
      "Loss: 1.6953809e-05\n",
      "Loss: 1.6918337e-05\n",
      "Loss: 1.6884766e-05\n",
      "Loss: 1.6847878e-05\n",
      "Loss: 1.6830832e-05\n",
      "Loss: 1.6822813e-05\n",
      "Loss: 1.6811062e-05\n",
      "Loss: 1.678272e-05\n",
      "Loss: 1.675951e-05\n",
      "Loss: 1.6720835e-05\n",
      "Loss: 1.6690019e-05\n",
      "Loss: 1.6659958e-05\n",
      "Loss: 1.6653627e-05\n",
      "Loss: 1.6618289e-05\n",
      "Loss: 1.6609578e-05\n",
      "Loss: 1.6599566e-05\n",
      "Loss: 1.6581545e-05\n",
      "Loss: 1.6555596e-05\n",
      "Loss: 1.6523372e-05\n",
      "Loss: 1.6490807e-05\n",
      "Loss: 1.645878e-05\n",
      "Loss: 1.641098e-05\n",
      "Loss: 1.6385844e-05\n",
      "Loss: 1.6359163e-05\n",
      "Loss: 1.634152e-05\n",
      "Loss: 1.6324659e-05\n",
      "Loss: 1.6306654e-05\n",
      "Loss: 1.6417112e-05\n",
      "Loss: 1.6296675e-05\n",
      "Loss: 1.6278005e-05\n",
      "Loss: 1.627097e-05\n",
      "Loss: 1.62331e-05\n",
      "Loss: 1.6190903e-05\n",
      "Loss: 1.6131451e-05\n",
      "Loss: 1.6107002e-05\n",
      "Loss: 1.6092174e-05\n",
      "Loss: 1.6074187e-05\n",
      "Loss: 1.605667e-05\n",
      "Loss: 1.6013704e-05\n",
      "Loss: 1.598582e-05\n",
      "Loss: 1.6292744e-05\n",
      "Loss: 1.5973368e-05\n",
      "Loss: 1.5940846e-05\n",
      "Loss: 1.591145e-05\n",
      "Loss: 1.5891681e-05\n",
      "Loss: 1.5874257e-05\n",
      "Loss: 1.5860911e-05\n",
      "Loss: 1.5845475e-05\n",
      "Loss: 1.5846887e-05\n",
      "Loss: 1.5835361e-05\n",
      "Loss: 1.5820904e-05\n",
      "Loss: 1.5798294e-05\n",
      "Loss: 1.5781845e-05\n",
      "Loss: 1.5769965e-05\n",
      "Loss: 1.5757412e-05\n",
      "Loss: 1.5746435e-05\n",
      "Loss: 1.5736157e-05\n",
      "Loss: 1.5729822e-05\n",
      "Loss: 1.5726404e-05\n",
      "Loss: 1.5698653e-05\n",
      "Loss: 1.56818e-05\n",
      "Loss: 1.5660293e-05\n",
      "Loss: 1.5623915e-05\n",
      "Loss: 1.5585454e-05\n",
      "Loss: 1.5569052e-05\n",
      "Loss: 1.5548894e-05\n",
      "Loss: 1.553189e-05\n",
      "Loss: 1.5512838e-05\n",
      "Loss: 1.5488953e-05\n",
      "Loss: 1.5486727e-05\n",
      "Loss: 1.5466705e-05\n",
      "Loss: 1.5428674e-05\n",
      "Loss: 1.5399235e-05\n",
      "Loss: 1.538008e-05\n",
      "Loss: 1.5334906e-05\n",
      "Loss: 1.5314023e-05\n",
      "Loss: 1.5288038e-05\n",
      "Loss: 1.5267626e-05\n",
      "Loss: 1.5253033e-05\n",
      "Loss: 1.52353205e-05\n",
      "Loss: 1.5219371e-05\n",
      "Loss: 1.5194271e-05\n",
      "Loss: 1.5178704e-05\n",
      "Loss: 1.5152552e-05\n",
      "Loss: 1.51208915e-05\n",
      "Loss: 1.5094354e-05\n",
      "Loss: 1.50342275e-05\n",
      "Loss: 1.4990497e-05\n",
      "Loss: 1.4993953e-05\n",
      "Loss: 1.4961489e-05\n",
      "Loss: 1.4916978e-05\n",
      "Loss: 1.4881548e-05\n",
      "Loss: 1.4950986e-05\n",
      "Loss: 1.4874981e-05\n",
      "Loss: 1.4860718e-05\n",
      "Loss: 1.4844022e-05\n",
      "Loss: 1.4832721e-05\n",
      "Loss: 1.4863134e-05\n",
      "Loss: 1.481993e-05\n",
      "Loss: 1.4785378e-05\n",
      "Loss: 1.4761634e-05\n",
      "Loss: 1.4728361e-05\n",
      "Loss: 1.4831081e-05\n",
      "Loss: 1.4721078e-05\n",
      "Loss: 1.4708304e-05\n",
      "Loss: 1.4676374e-05\n",
      "Loss: 1.4661965e-05\n",
      "Loss: 1.4646398e-05\n",
      "Loss: 1.4625695e-05\n",
      "Loss: 1.4619608e-05\n",
      "Loss: 1.4666461e-05\n",
      "Loss: 1.4617035e-05\n",
      "Loss: 1.4610921e-05\n",
      "Loss: 1.46040775e-05\n",
      "Loss: 1.45948925e-05\n",
      "Loss: 1.4615001e-05\n",
      "Loss: 1.4589538e-05\n",
      "Loss: 1.4581703e-05\n",
      "Loss: 1.4576842e-05\n",
      "Loss: 1.4570191e-05\n",
      "Loss: 1.4556428e-05\n",
      "Loss: 1.4534417e-05\n",
      "Loss: 1.4518102e-05\n",
      "Loss: 1.4510288e-05\n",
      "Loss: 1.4505005e-05\n",
      "Loss: 1.4500101e-05\n",
      "Loss: 1.4565385e-05\n",
      "Loss: 1.4491316e-05\n",
      "Loss: 1.4482919e-05\n",
      "Loss: 1.4468258e-05\n",
      "Loss: 1.4450695e-05\n",
      "Loss: 1.442473e-05\n",
      "Loss: 1.4400789e-05\n",
      "Loss: 1.4378643e-05\n",
      "Loss: 1.4348819e-05\n",
      "Loss: 1.4325233e-05\n",
      "Loss: 1.4312074e-05\n",
      "Loss: 1.43030275e-05\n",
      "Loss: 1.42962635e-05\n",
      "Loss: 1.4284023e-05\n",
      "Loss: 1.4268775e-05\n",
      "Loss: 1.4258933e-05\n",
      "Loss: 1.4233934e-05\n",
      "Loss: 1.4219098e-05\n",
      "Loss: 1.4208272e-05\n",
      "Loss: 1.4200688e-05\n",
      "Loss: 1.418365e-05\n",
      "Loss: 1.4156563e-05\n",
      "Loss: 1.4145082e-05\n",
      "Loss: 1.41176615e-05\n",
      "Loss: 1.4096255e-05\n",
      "Loss: 1.4082372e-05\n",
      "Loss: 1.407446e-05\n",
      "Loss: 1.4062556e-05\n",
      "Loss: 1.4056341e-05\n",
      "Loss: 1.4048805e-05\n",
      "Loss: 1.40353795e-05\n",
      "Loss: 1.4104177e-05\n",
      "Loss: 1.4029617e-05\n",
      "Loss: 1.4009779e-05\n",
      "Loss: 1.3997518e-05\n",
      "Loss: 1.3986909e-05\n",
      "Loss: 1.3980213e-05\n",
      "Loss: 1.3969684e-05\n",
      "Loss: 1.3959793e-05\n",
      "Loss: 1.3949781e-05\n",
      "Loss: 1.4007378e-05\n",
      "Loss: 1.394762e-05\n",
      "Loss: 1.39375825e-05\n",
      "Loss: 1.3922793e-05\n",
      "Loss: 1.3912154e-05\n",
      "Loss: 1.3903889e-05\n",
      "Loss: 1.3899025e-05\n",
      "Loss: 1.3881051e-05\n",
      "Loss: 1.3867825e-05\n",
      "Loss: 1.3855641e-05\n",
      "Loss: 1.3847761e-05\n",
      "Loss: 1.38522155e-05\n",
      "Loss: 1.3843586e-05\n",
      "Loss: 1.3831746e-05\n",
      "Loss: 1.3824283e-05\n",
      "Loss: 1.3817417e-05\n",
      "Loss: 1.3804477e-05\n",
      "Loss: 1.37899915e-05\n",
      "Loss: 1.3784594e-05\n",
      "Loss: 1.37743455e-05\n",
      "Loss: 1.3765606e-05\n",
      "Loss: 1.3769992e-05\n",
      "Loss: 1.3753054e-05\n",
      "Loss: 1.37320185e-05\n",
      "Loss: 1.370905e-05\n",
      "Loss: 1.3694214e-05\n",
      "Loss: 1.3677232e-05\n",
      "Loss: 1.3713035e-05\n",
      "Loss: 1.367066e-05\n",
      "Loss: 1.3658884e-05\n",
      "Loss: 1.364538e-05\n",
      "Loss: 1.3635949e-05\n",
      "Loss: 1.3622214e-05\n",
      "Loss: 1.3603913e-05\n",
      "Loss: 1.35915725e-05\n",
      "Loss: 1.3581337e-05\n",
      "Loss: 1.3573632e-05\n",
      "Loss: 1.356863e-05\n",
      "Loss: 1.356018e-05\n",
      "Loss: 1.3551606e-05\n",
      "Loss: 1.3547545e-05\n",
      "Loss: 1.3534531e-05\n",
      "Loss: 1.3528982e-05\n",
      "Loss: 1.3523978e-05\n",
      "Loss: 1.3515786e-05\n",
      "Loss: 1.3505958e-05\n",
      "Loss: 1.3496988e-05\n",
      "Loss: 1.34882675e-05\n",
      "Loss: 1.3481463e-05\n",
      "Loss: 1.3479035e-05\n",
      "Loss: 1.3480628e-05\n",
      "Loss: 1.3475661e-05\n",
      "Loss: 1.3468603e-05\n",
      "Loss: 1.3464568e-05\n",
      "Loss: 1.3455876e-05\n",
      "Loss: 1.3446206e-05\n",
      "Loss: 1.3458467e-05\n",
      "Loss: 1.34420225e-05\n",
      "Loss: 1.3437766e-05\n",
      "Loss: 1.3435439e-05\n",
      "Loss: 1.343107e-05\n",
      "Loss: 1.34234215e-05\n",
      "Loss: 1.3412846e-05\n",
      "Loss: 1.3405541e-05\n",
      "Loss: 1.3398646e-05\n",
      "Loss: 1.3390781e-05\n",
      "Loss: 1.3379702e-05\n",
      "Loss: 1.3366229e-05\n",
      "Loss: 1.3355627e-05\n",
      "Loss: 1.3340532e-05\n",
      "Loss: 1.3338369e-05\n",
      "Loss: 1.3327912e-05\n",
      "Loss: 1.3324667e-05\n",
      "Loss: 1.3316017e-05\n",
      "Loss: 1.3300019e-05\n",
      "Loss: 1.32839e-05\n",
      "Loss: 1.3270117e-05\n",
      "Loss: 1.325684e-05\n",
      "Loss: 1.3244717e-05\n",
      "Loss: 1.3234387e-05\n",
      "Loss: 1.32107625e-05\n",
      "Loss: 1.31959205e-05\n",
      "Loss: 1.3238112e-05\n",
      "Loss: 1.3189602e-05\n",
      "Loss: 1.3170974e-05\n",
      "Loss: 1.3150187e-05\n",
      "Loss: 1.312912e-05\n",
      "Loss: 1.3105937e-05\n",
      "Loss: 1.3123484e-05\n",
      "Loss: 1.30938115e-05\n",
      "Loss: 1.309331e-05\n",
      "Loss: 1.3070188e-05\n",
      "Loss: 1.3055254e-05\n",
      "Loss: 1.3040227e-05\n",
      "Loss: 1.3040528e-05\n",
      "Loss: 1.30296885e-05\n",
      "Loss: 1.3010789e-05\n",
      "Loss: 1.3000577e-05\n",
      "Loss: 1.2980264e-05\n",
      "Loss: 1.296853e-05\n",
      "Loss: 1.2948912e-05\n",
      "Loss: 1.29289665e-05\n",
      "Loss: 1.2916766e-05\n",
      "Loss: 1.2910221e-05\n",
      "Loss: 1.2897854e-05\n",
      "Loss: 1.2888024e-05\n",
      "Loss: 1.28759775e-05\n",
      "Loss: 1.2861799e-05\n",
      "Loss: 1.2864867e-05\n",
      "Loss: 1.2857886e-05\n",
      "Loss: 1.2851486e-05\n",
      "Loss: 1.2842549e-05\n",
      "Loss: 1.2832765e-05\n",
      "Loss: 1.28168795e-05\n",
      "Loss: 1.320443e-05\n",
      "Loss: 1.2815969e-05\n",
      "Loss: 1.280304e-05\n",
      "Loss: 1.2793771e-05\n",
      "Loss: 1.27811445e-05\n",
      "Loss: 1.2767001e-05\n",
      "Loss: 1.2737289e-05\n",
      "Loss: 1.27686435e-05\n",
      "Loss: 1.27231915e-05\n",
      "Loss: 1.26935865e-05\n",
      "Loss: 1.2685918e-05\n",
      "Loss: 1.2674302e-05\n",
      "Loss: 1.2670345e-05\n",
      "Loss: 1.2666472e-05\n",
      "Loss: 1.2662711e-05\n",
      "Loss: 1.265944e-05\n",
      "Loss: 1.2651582e-05\n",
      "Loss: 1.2642171e-05\n",
      "Loss: 1.262725e-05\n",
      "Loss: 1.2618389e-05\n",
      "Loss: 1.2597393e-05\n",
      "Loss: 1.258691e-05\n",
      "Loss: 1.2557581e-05\n",
      "Loss: 1.2547003e-05\n",
      "Loss: 1.25395445e-05\n",
      "Loss: 1.25312945e-05\n",
      "Loss: 1.2535401e-05\n",
      "Loss: 1.25250235e-05\n",
      "Loss: 1.2516938e-05\n",
      "Loss: 1.2512304e-05\n",
      "Loss: 1.2503448e-05\n",
      "Loss: 1.2492299e-05\n",
      "Loss: 1.2484057e-05\n",
      "Loss: 1.247592e-05\n",
      "Loss: 1.2468768e-05\n",
      "Loss: 1.2459626e-05\n",
      "Loss: 1.2451546e-05\n",
      "Loss: 1.2432304e-05\n",
      "Loss: 1.2420456e-05\n",
      "Loss: 1.24041535e-05\n",
      "Loss: 1.2368759e-05\n",
      "Loss: 1.2344052e-05\n",
      "Loss: 1.2331998e-05\n",
      "Loss: 1.2308526e-05\n",
      "Loss: 1.2285762e-05\n",
      "Loss: 1.23448635e-05\n",
      "Loss: 1.2278355e-05\n",
      "Loss: 1.22637175e-05\n",
      "Loss: 1.2258711e-05\n",
      "Loss: 1.22529445e-05\n",
      "Loss: 1.2244374e-05\n",
      "Loss: 1.22332385e-05\n",
      "Loss: 1.2218042e-05\n",
      "Loss: 1.234252e-05\n",
      "Loss: 1.2212087e-05\n",
      "Loss: 1.219549e-05\n",
      "Loss: 1.2177092e-05\n",
      "Loss: 1.21676385e-05\n",
      "Loss: 1.2179504e-05\n",
      "Loss: 1.2162215e-05\n",
      "Loss: 1.2147447e-05\n",
      "Loss: 1.2133066e-05\n",
      "Loss: 1.2118234e-05\n",
      "Loss: 1.2101976e-05\n",
      "Loss: 1.2079927e-05\n",
      "Loss: 1.2046278e-05\n",
      "Loss: 1.2146527e-05\n",
      "Loss: 1.2038858e-05\n",
      "Loss: 1.2024777e-05\n",
      "Loss: 1.2008723e-05\n",
      "Loss: 1.1997644e-05\n",
      "Loss: 1.19843535e-05\n",
      "Loss: 1.1972353e-05\n",
      "Loss: 1.1955416e-05\n",
      "Loss: 1.1955437e-05\n",
      "Loss: 1.1951e-05\n",
      "Loss: 1.1947226e-05\n",
      "Loss: 1.19354045e-05\n",
      "Loss: 1.1924662e-05\n",
      "Loss: 1.1902256e-05\n",
      "Loss: 1.1901342e-05\n",
      "Loss: 1.1893558e-05\n",
      "Loss: 1.1878301e-05\n",
      "Loss: 1.1868403e-05\n",
      "Loss: 1.185704e-05\n",
      "Loss: 1.1845249e-05\n",
      "Loss: 1.1824586e-05\n",
      "Loss: 1.1796417e-05\n",
      "Loss: 1.1767606e-05\n",
      "Loss: 1.175121e-05\n",
      "Loss: 1.1745035e-05\n",
      "Loss: 1.1714071e-05\n",
      "Loss: 1.1707694e-05\n",
      "Loss: 1.1694905e-05\n",
      "Loss: 1.1745419e-05\n",
      "Loss: 1.1689106e-05\n",
      "Loss: 1.1675827e-05\n",
      "Loss: 1.1657363e-05\n",
      "Loss: 1.1641947e-05\n",
      "Loss: 1.1629665e-05\n",
      "Loss: 1.1612461e-05\n",
      "Loss: 1.1599509e-05\n",
      "Loss: 1.1583839e-05\n",
      "Loss: 1.1571552e-05\n",
      "Loss: 1.1854893e-05\n",
      "Loss: 1.1565891e-05\n",
      "Loss: 1.15512985e-05\n",
      "Loss: 1.1543665e-05\n",
      "Loss: 1.1529224e-05\n",
      "Loss: 1.1523469e-05\n",
      "Loss: 1.1510311e-05\n",
      "Loss: 1.1669059e-05\n",
      "Loss: 1.15037265e-05\n",
      "Loss: 1.1477293e-05\n",
      "Loss: 1.1460052e-05\n",
      "Loss: 1.14490185e-05\n",
      "Loss: 1.1522736e-05\n",
      "Loss: 1.1446996e-05\n",
      "Loss: 1.1440842e-05\n",
      "Loss: 1.1434139e-05\n",
      "Loss: 1.1428028e-05\n",
      "Loss: 1.1415885e-05\n",
      "Loss: 1.1405021e-05\n",
      "Loss: 1.1394231e-05\n",
      "Loss: 1.13884225e-05\n",
      "Loss: 1.138432e-05\n",
      "Loss: 1.1380433e-05\n",
      "Loss: 1.137383e-05\n",
      "Loss: 1.13695e-05\n",
      "Loss: 1.1364268e-05\n",
      "Loss: 1.1359639e-05\n",
      "Loss: 1.1355195e-05\n",
      "Loss: 1.1351816e-05\n",
      "Loss: 1.13425485e-05\n",
      "Loss: 1.1495471e-05\n",
      "Loss: 1.1341127e-05\n",
      "Loss: 1.1333202e-05\n",
      "Loss: 1.1318908e-05\n",
      "Loss: 1.1309572e-05\n",
      "Loss: 1.1302071e-05\n",
      "Loss: 1.1293119e-05\n",
      "Loss: 1.1283889e-05\n",
      "Loss: 1.126073e-05\n",
      "Loss: 1.12397665e-05\n",
      "Loss: 1.1226376e-05\n",
      "Loss: 1.1217349e-05\n",
      "Loss: 1.1208555e-05\n",
      "Loss: 1.1253095e-05\n",
      "Loss: 1.1206357e-05\n",
      "Loss: 1.1197269e-05\n",
      "Loss: 1.1189825e-05\n",
      "Loss: 1.1182382e-05\n",
      "Loss: 1.1176199e-05\n",
      "Loss: 1.1171353e-05\n",
      "Loss: 1.1154612e-05\n",
      "Loss: 1.11471745e-05\n",
      "Loss: 1.1143304e-05\n",
      "Loss: 1.1140203e-05\n",
      "Loss: 1.1129657e-05\n",
      "Loss: 1.11866675e-05\n",
      "Loss: 1.1123082e-05\n",
      "Loss: 1.11147865e-05\n",
      "Loss: 1.1102427e-05\n",
      "Loss: 1.1092229e-05\n",
      "Loss: 1.1093982e-05\n",
      "Loss: 1.1082451e-05\n",
      "Loss: 1.106884e-05\n",
      "Loss: 1.10608335e-05\n",
      "Loss: 1.1056101e-05\n",
      "Loss: 1.1063536e-05\n",
      "Loss: 1.1054024e-05\n",
      "Loss: 1.1051194e-05\n",
      "Loss: 1.1045257e-05\n",
      "Loss: 1.1039406e-05\n",
      "Loss: 1.3603634e-05\n",
      "Loss: 1.1038257e-05\n",
      "Loss: 1.10326455e-05\n",
      "Loss: 1.1028092e-05\n",
      "Loss: 1.10214e-05\n",
      "Loss: 1.1015718e-05\n",
      "Loss: 1.1008857e-05\n",
      "Loss: 1.1129522e-05\n",
      "Loss: 1.1007455e-05\n",
      "Loss: 1.0999124e-05\n",
      "Loss: 1.0990145e-05\n",
      "Loss: 1.097971e-05\n",
      "Loss: 1.0969226e-05\n",
      "Loss: 1.0974809e-05\n",
      "Loss: 1.0965079e-05\n",
      "Loss: 1.0955606e-05\n",
      "Loss: 1.095157e-05\n",
      "Loss: 1.0946849e-05\n",
      "Loss: 1.0944741e-05\n",
      "Loss: 1.0935827e-05\n",
      "Loss: 1.0946755e-05\n",
      "Loss: 1.0930099e-05\n",
      "Loss: 1.09268485e-05\n",
      "Loss: 1.0924376e-05\n",
      "Loss: 1.0921748e-05\n",
      "Loss: 1.0916e-05\n",
      "Loss: 1.0907402e-05\n",
      "Loss: 1.0903927e-05\n",
      "Loss: 1.089579e-05\n",
      "Loss: 1.0893889e-05\n",
      "Loss: 1.089074e-05\n",
      "Loss: 1.0886896e-05\n",
      "Loss: 1.0880745e-05\n",
      "Loss: 1.0874339e-05\n",
      "Loss: 1.0865675e-05\n",
      "Loss: 1.1178212e-05\n",
      "Loss: 1.0863994e-05\n",
      "Loss: 1.0856362e-05\n",
      "Loss: 1.0844374e-05\n",
      "Loss: 1.0831239e-05\n",
      "Loss: 1.0822198e-05\n",
      "Loss: 1.0813819e-05\n",
      "Loss: 1.0808764e-05\n",
      "Loss: 1.0804656e-05\n",
      "Loss: 1.0796091e-05\n",
      "Loss: 1.0809075e-05\n",
      "Loss: 1.0792991e-05\n",
      "Loss: 1.0784562e-05\n",
      "Loss: 1.0774498e-05\n",
      "Loss: 1.0758127e-05\n",
      "Loss: 1.0753961e-05\n",
      "Loss: 1.07376845e-05\n",
      "Loss: 1.0731353e-05\n",
      "Loss: 1.0723442e-05\n",
      "Loss: 1.08546965e-05\n",
      "Loss: 1.0720828e-05\n",
      "Loss: 1.0714002e-05\n",
      "Loss: 1.0708781e-05\n",
      "Loss: 1.0703553e-05\n",
      "Loss: 1.0702503e-05\n",
      "Loss: 1.0695774e-05\n",
      "Loss: 1.0692768e-05\n",
      "Loss: 1.0690797e-05\n",
      "Loss: 1.0688807e-05\n",
      "Loss: 1.0683027e-05\n",
      "Loss: 1.07107135e-05\n",
      "Loss: 1.0680402e-05\n",
      "Loss: 1.0672321e-05\n",
      "Loss: 1.066575e-05\n",
      "Loss: 1.0659441e-05\n",
      "Loss: 1.0654004e-05\n",
      "Loss: 1.064576e-05\n",
      "Loss: 1.0709733e-05\n",
      "Loss: 1.0642902e-05\n",
      "Loss: 1.06340585e-05\n",
      "Loss: 1.0626554e-05\n",
      "Loss: 1.0621906e-05\n",
      "Loss: 1.0618516e-05\n",
      "Loss: 1.0937574e-05\n",
      "Loss: 1.0616952e-05\n",
      "Loss: 1.0614038e-05\n",
      "Loss: 1.0608847e-05\n",
      "Loss: 1.0604654e-05\n",
      "Loss: 1.0597734e-05\n",
      "Loss: 1.0585484e-05\n",
      "Loss: 1.0593485e-05\n",
      "Loss: 1.0581002e-05\n",
      "Loss: 1.057352e-05\n",
      "Loss: 1.05731015e-05\n",
      "Loss: 1.056832e-05\n",
      "Loss: 1.0566862e-05\n",
      "Loss: 1.0564381e-05\n",
      "Loss: 1.0561281e-05\n",
      "Loss: 1.0557547e-05\n",
      "Loss: 1.055291e-05\n",
      "Loss: 1.0548382e-05\n",
      "Loss: 1.0546301e-05\n",
      "Loss: 1.0543024e-05\n",
      "Loss: 1.0538314e-05\n",
      "Loss: 1.05354675e-05\n",
      "Loss: 1.0530624e-05\n",
      "Loss: 1.0525252e-05\n",
      "Loss: 1.0518397e-05\n",
      "Loss: 1.0515824e-05\n",
      "Loss: 1.0510537e-05\n",
      "Loss: 1.0508922e-05\n",
      "Loss: 1.0503365e-05\n",
      "Loss: 1.0500363e-05\n",
      "Loss: 1.049595e-05\n",
      "Loss: 1.0489972e-05\n",
      "Loss: 1.0562836e-05\n",
      "Loss: 1.0488097e-05\n",
      "Loss: 1.04846495e-05\n",
      "Loss: 1.0480978e-05\n",
      "Loss: 1.0478002e-05\n",
      "Loss: 1.0472882e-05\n",
      "Loss: 1.0467981e-05\n",
      "Loss: 1.0483757e-05\n",
      "Loss: 1.0464867e-05\n",
      "Loss: 1.0458325e-05\n",
      "Loss: 1.044993e-05\n",
      "Loss: 1.0443996e-05\n",
      "Loss: 1.0436004e-05\n",
      "Loss: 1.0430371e-05\n",
      "Loss: 1.0425137e-05\n",
      "Loss: 1.0418314e-05\n",
      "Loss: 1.0412253e-05\n",
      "Loss: 1.0404643e-05\n",
      "Loss: 1.0397225e-05\n",
      "Loss: 1.0388705e-05\n",
      "Loss: 1.0381193e-05\n",
      "Loss: 1.0374874e-05\n",
      "Loss: 1.0370781e-05\n",
      "Loss: 1.0368718e-05\n",
      "Loss: 1.0366105e-05\n",
      "Loss: 1.03619395e-05\n",
      "Loss: 1.0356079e-05\n",
      "Loss: 1.0349152e-05\n",
      "Loss: 1.0344121e-05\n",
      "Loss: 1.0334694e-05\n",
      "Loss: 1.0327421e-05\n",
      "Loss: 1.032182e-05\n",
      "Loss: 1.0319594e-05\n",
      "Loss: 1.03118155e-05\n",
      "Loss: 1.0305891e-05\n",
      "Loss: 1.02995255e-05\n",
      "Loss: 1.0292517e-05\n",
      "Loss: 1.0280694e-05\n",
      "Loss: 1.0272507e-05\n",
      "Loss: 1.0269165e-05\n",
      "Loss: 1.0266418e-05\n",
      "Loss: 1.0274207e-05\n",
      "Loss: 1.0265512e-05\n",
      "Loss: 1.02623e-05\n",
      "Loss: 1.0257202e-05\n",
      "Loss: 1.0253063e-05\n",
      "Loss: 1.0248873e-05\n",
      "Loss: 1.0243026e-05\n",
      "Loss: 1.0232825e-05\n",
      "Loss: 1.0225771e-05\n",
      "Loss: 1.0221849e-05\n",
      "Loss: 1.0215766e-05\n",
      "Loss: 1.0205785e-05\n",
      "Loss: 1.0198253e-05\n",
      "Loss: 1.0190686e-05\n",
      "Loss: 1.02055365e-05\n",
      "Loss: 1.0188779e-05\n",
      "Loss: 1.0183675e-05\n",
      "Loss: 1.0177555e-05\n",
      "Loss: 1.0170417e-05\n",
      "Loss: 1.0164977e-05\n",
      "Loss: 1.0160039e-05\n",
      "Loss: 1.0153661e-05\n",
      "Loss: 1.0148747e-05\n",
      "Loss: 1.0145763e-05\n",
      "Loss: 1.0141422e-05\n",
      "Loss: 1.0138693e-05\n",
      "Loss: 1.0128618e-05\n",
      "Loss: 1.0121218e-05\n",
      "Loss: 1.0111566e-05\n",
      "Loss: 1.0102262e-05\n",
      "Loss: 1.0088946e-05\n",
      "Loss: 1.0087997e-05\n",
      "Loss: 1.0087477e-05\n",
      "Loss: 1.0080721e-05\n",
      "Loss: 1.00757825e-05\n",
      "Loss: 1.0071628e-05\n",
      "Loss: 1.0060191e-05\n",
      "Loss: 1.0044007e-05\n",
      "Loss: 1.0036393e-05\n",
      "Loss: 1.0068326e-05\n",
      "Loss: 1.0023744e-05\n",
      "Loss: 1.0007159e-05\n",
      "Loss: 9.998773e-06\n",
      "Loss: 9.991549e-06\n",
      "Loss: 9.980368e-06\n",
      "Loss: 9.968728e-06\n",
      "Loss: 1.0007087e-05\n",
      "Loss: 9.961777e-06\n",
      "Loss: 9.944469e-06\n",
      "Loss: 9.934215e-06\n",
      "Loss: 9.933992e-06\n",
      "Loss: 9.929064e-06\n",
      "Loss: 9.932736e-06\n",
      "Loss: 9.91778e-06\n",
      "Loss: 9.907137e-06\n",
      "Loss: 9.883159e-06\n",
      "Loss: 9.87858e-06\n",
      "Loss: 9.867642e-06\n",
      "Loss: 9.85331e-06\n",
      "Loss: 9.995859e-06\n",
      "Loss: 9.851465e-06\n",
      "Loss: 9.844847e-06\n",
      "Loss: 9.836633e-06\n",
      "Loss: 9.870648e-06\n",
      "Loss: 9.8326145e-06\n",
      "Loss: 9.825284e-06\n",
      "Loss: 9.813876e-06\n",
      "Loss: 9.80461e-06\n",
      "Loss: 9.799434e-06\n",
      "Loss: 9.794914e-06\n",
      "Loss: 9.791199e-06\n",
      "Loss: 9.786022e-06\n",
      "Loss: 9.779896e-06\n",
      "Loss: 9.7673865e-06\n",
      "Loss: 9.765849e-06\n",
      "Loss: 9.745767e-06\n",
      "Loss: 9.737691e-06\n",
      "Loss: 9.722918e-06\n",
      "Loss: 9.702905e-06\n",
      "Loss: 9.688464e-06\n",
      "Loss: 9.677933e-06\n",
      "Loss: 9.673776e-06\n",
      "Loss: 9.668187e-06\n",
      "Loss: 9.657499e-06\n",
      "Loss: 9.643527e-06\n",
      "Loss: 9.632699e-06\n",
      "Loss: 9.613909e-06\n",
      "Loss: 9.597386e-06\n",
      "Loss: 9.572258e-06\n",
      "Loss: 9.5496625e-06\n",
      "Loss: 9.5372e-06\n",
      "Loss: 9.517298e-06\n",
      "Loss: 9.504747e-06\n",
      "Loss: 9.491856e-06\n",
      "Loss: 9.470614e-06\n",
      "Loss: 9.451133e-06\n",
      "Loss: 9.436494e-06\n",
      "Loss: 9.421306e-06\n",
      "Loss: 9.415373e-06\n",
      "Loss: 9.406827e-06\n",
      "Loss: 9.403637e-06\n",
      "Loss: 9.399365e-06\n",
      "Loss: 9.391724e-06\n",
      "Loss: 9.377014e-06\n",
      "Loss: 9.364247e-06\n",
      "Loss: 9.356916e-06\n",
      "Loss: 9.341731e-06\n",
      "Loss: 9.333236e-06\n",
      "Loss: 9.325948e-06\n",
      "Loss: 9.319803e-06\n",
      "Loss: 9.315115e-06\n",
      "Loss: 9.310795e-06\n",
      "Loss: 9.304631e-06\n",
      "Loss: 9.310628e-06\n",
      "Loss: 9.301839e-06\n",
      "Loss: 9.296119e-06\n",
      "Loss: 9.276549e-06\n",
      "Loss: 9.292089e-06\n",
      "Loss: 9.267407e-06\n",
      "Loss: 9.2547725e-06\n",
      "Loss: 9.240133e-06\n",
      "Loss: 9.2340115e-06\n",
      "Loss: 9.232737e-06\n",
      "Loss: 9.227195e-06\n",
      "Loss: 9.224123e-06\n",
      "Loss: 9.2214e-06\n",
      "Loss: 9.219122e-06\n",
      "Loss: 9.2172995e-06\n",
      "Loss: 9.215829e-06\n",
      "Loss: 9.204901e-06\n",
      "Loss: 9.200996e-06\n",
      "Loss: 9.19043e-06\n",
      "Loss: 9.181579e-06\n",
      "Loss: 9.173534e-06\n",
      "Loss: 9.166208e-06\n",
      "Loss: 9.161116e-06\n",
      "Loss: 9.156114e-06\n",
      "Loss: 9.1536685e-06\n",
      "Loss: 9.148259e-06\n",
      "Loss: 9.1428565e-06\n",
      "Loss: 9.13374e-06\n",
      "Loss: 9.12218e-06\n",
      "Loss: 9.114232e-06\n",
      "Loss: 9.105115e-06\n",
      "Loss: 9.115763e-06\n",
      "Loss: 9.101024e-06\n",
      "Loss: 9.094094e-06\n",
      "Loss: 9.085819e-06\n",
      "Loss: 9.080712e-06\n",
      "Loss: 9.074401e-06\n",
      "Loss: 9.064055e-06\n",
      "Loss: 9.056632e-06\n",
      "Loss: 9.053215e-06\n",
      "Loss: 9.049498e-06\n",
      "Loss: 9.041695e-06\n",
      "Loss: 9.052219e-06\n",
      "Loss: 9.038561e-06\n",
      "Loss: 9.033964e-06\n",
      "Loss: 9.030728e-06\n",
      "Loss: 9.027527e-06\n",
      "Loss: 9.021981e-06\n",
      "Loss: 9.012903e-06\n",
      "Loss: 9.002605e-06\n",
      "Loss: 8.990757e-06\n",
      "Loss: 8.98175e-06\n",
      "Loss: 8.982532e-06\n",
      "Loss: 8.979464e-06\n",
      "Loss: 8.974024e-06\n",
      "Loss: 8.968359e-06\n",
      "Loss: 8.96285e-06\n",
      "Loss: 8.958575e-06\n",
      "Loss: 8.952294e-06\n",
      "Loss: 8.940672e-06\n",
      "Loss: 8.936685e-06\n",
      "Loss: 9.001461e-06\n",
      "Loss: 8.929294e-06\n",
      "Loss: 8.920908e-06\n",
      "Loss: 8.917338e-06\n",
      "Loss: 8.908286e-06\n",
      "Loss: 8.90115e-06\n",
      "Loss: 8.890946e-06\n",
      "Loss: 8.878662e-06\n",
      "Loss: 8.86498e-06\n",
      "Loss: 8.855647e-06\n",
      "Loss: 8.84373e-06\n",
      "Loss: 8.834781e-06\n",
      "Loss: 8.823728e-06\n",
      "Loss: 8.813925e-06\n",
      "Loss: 8.807366e-06\n",
      "Loss: 8.7933895e-06\n",
      "Loss: 8.8020315e-06\n",
      "Loss: 8.787193e-06\n",
      "Loss: 8.775198e-06\n",
      "Loss: 8.766483e-06\n",
      "Loss: 8.75751e-06\n",
      "Loss: 8.744921e-06\n",
      "Loss: 8.731871e-06\n",
      "Loss: 8.725744e-06\n",
      "Loss: 8.721724e-06\n",
      "Loss: 8.717025e-06\n",
      "Loss: 8.707356e-06\n",
      "Loss: 8.702142e-06\n",
      "Loss: 8.695186e-06\n",
      "Loss: 9.441707e-06\n",
      "Loss: 8.69416e-06\n",
      "Loss: 8.689045e-06\n",
      "Loss: 8.680494e-06\n",
      "Loss: 8.678205e-06\n",
      "Loss: 8.675792e-06\n",
      "Loss: 8.67384e-06\n",
      "Loss: 8.671164e-06\n",
      "Loss: 8.666401e-06\n",
      "Loss: 8.661637e-06\n",
      "Loss: 8.983385e-06\n",
      "Loss: 8.660971e-06\n",
      "Loss: 8.6552645e-06\n",
      "Loss: 8.646061e-06\n",
      "Loss: 8.632222e-06\n",
      "Loss: 8.611442e-06\n",
      "Loss: 8.578253e-06\n",
      "Loss: 8.581607e-06\n",
      "Loss: 8.5577685e-06\n",
      "Loss: 8.528727e-06\n",
      "Loss: 8.517727e-06\n",
      "Loss: 8.50978e-06\n",
      "Loss: 8.498531e-06\n",
      "Loss: 8.490936e-06\n",
      "Loss: 8.482443e-06\n",
      "Loss: 8.478273e-06\n",
      "Loss: 8.463902e-06\n",
      "Loss: 8.451535e-06\n",
      "Loss: 8.444083e-06\n",
      "Loss: 8.439567e-06\n",
      "Loss: 8.43495e-06\n",
      "Loss: 8.428543e-06\n",
      "Loss: 8.422018e-06\n",
      "Loss: 8.418925e-06\n",
      "Loss: 8.41172e-06\n",
      "Loss: 8.405363e-06\n",
      "Loss: 8.401051e-06\n",
      "Loss: 8.39625e-06\n",
      "Loss: 8.387501e-06\n",
      "Loss: 8.3705345e-06\n",
      "Loss: 8.355029e-06\n",
      "Loss: 8.340719e-06\n",
      "Loss: 8.330408e-06\n",
      "Loss: 8.324546e-06\n",
      "Loss: 8.314271e-06\n",
      "Loss: 8.452224e-06\n",
      "Loss: 8.312269e-06\n",
      "Loss: 8.302136e-06\n",
      "Loss: 8.293513e-06\n",
      "Loss: 8.287079e-06\n",
      "Loss: 8.280912e-06\n",
      "Loss: 8.275238e-06\n",
      "Loss: 8.26836e-06\n",
      "Loss: 8.258436e-06\n",
      "Loss: 8.248965e-06\n",
      "Loss: 8.24177e-06\n",
      "Loss: 8.235867e-06\n",
      "Loss: 8.228733e-06\n",
      "Loss: 8.233155e-06\n",
      "Loss: 8.225187e-06\n",
      "Loss: 8.269834e-06\n",
      "Loss: 8.222609e-06\n",
      "Loss: 8.218683e-06\n",
      "Loss: 8.2133165e-06\n",
      "Loss: 8.206229e-06\n",
      "Loss: 8.19536e-06\n",
      "Loss: 8.184278e-06\n",
      "Loss: 8.177538e-06\n",
      "Loss: 8.16098e-06\n",
      "Loss: 8.147332e-06\n",
      "Loss: 8.133138e-06\n",
      "Loss: 8.118365e-06\n",
      "Loss: 8.108266e-06\n",
      "Loss: 8.103204e-06\n",
      "Loss: 8.100937e-06\n",
      "Loss: 8.078227e-06\n",
      "Loss: 8.06758e-06\n",
      "Loss: 8.059424e-06\n",
      "Loss: 8.051411e-06\n",
      "Loss: 8.042532e-06\n",
      "Loss: 8.037929e-06\n",
      "Loss: 8.0353675e-06\n",
      "Loss: 8.02864e-06\n",
      "Loss: 8.026245e-06\n",
      "Loss: 8.021372e-06\n",
      "Loss: 8.012337e-06\n",
      "Loss: 7.994763e-06\n",
      "Loss: 7.981141e-06\n",
      "Loss: 8.154991e-06\n",
      "Loss: 7.979048e-06\n",
      "Loss: 7.970958e-06\n",
      "Loss: 7.964906e-06\n",
      "Loss: 7.957611e-06\n",
      "Loss: 7.94882e-06\n",
      "Loss: 7.940975e-06\n",
      "Loss: 7.935231e-06\n",
      "Loss: 7.931277e-06\n",
      "Loss: 7.929713e-06\n",
      "Loss: 7.927458e-06\n",
      "Loss: 7.922862e-06\n",
      "Loss: 7.9126785e-06\n",
      "Loss: 7.9095e-06\n",
      "Loss: 7.903007e-06\n",
      "Loss: 7.898828e-06\n",
      "Loss: 7.893546e-06\n",
      "Loss: 7.8836365e-06\n",
      "Loss: 8.026447e-06\n",
      "Loss: 7.882048e-06\n",
      "Loss: 7.872445e-06\n",
      "Loss: 7.86767e-06\n",
      "Loss: 7.86537e-06\n",
      "Loss: 7.862788e-06\n",
      "Loss: 7.87265e-06\n",
      "Loss: 7.8606145e-06\n",
      "Loss: 7.854423e-06\n",
      "Loss: 7.8444955e-06\n",
      "Loss: 7.842332e-06\n",
      "Loss: 7.825997e-06\n",
      "Loss: 7.821294e-06\n",
      "Loss: 7.812179e-06\n",
      "Loss: 7.808591e-06\n",
      "Loss: 7.796093e-06\n",
      "Loss: 7.7866125e-06\n",
      "Loss: 7.774546e-06\n",
      "Loss: 7.770191e-06\n",
      "Loss: 7.765864e-06\n",
      "Loss: 7.759844e-06\n",
      "Loss: 7.751616e-06\n",
      "Loss: 7.763155e-06\n",
      "Loss: 7.748997e-06\n",
      "Loss: 7.745037e-06\n",
      "Loss: 7.73846e-06\n",
      "Loss: 7.735318e-06\n",
      "Loss: 7.730589e-06\n",
      "Loss: 7.725649e-06\n",
      "Loss: 7.720771e-06\n",
      "Loss: 7.714323e-06\n",
      "Loss: 7.708413e-06\n",
      "Loss: 7.702579e-06\n",
      "Loss: 7.7141185e-06\n",
      "Loss: 7.696531e-06\n",
      "Loss: 7.690753e-06\n",
      "Loss: 7.678162e-06\n",
      "Loss: 7.673222e-06\n",
      "Loss: 7.663656e-06\n",
      "Loss: 7.662187e-06\n",
      "Loss: 7.651097e-06\n",
      "Loss: 7.6408005e-06\n",
      "Loss: 7.631794e-06\n",
      "Loss: 7.6251017e-06\n",
      "Loss: 7.645251e-06\n",
      "Loss: 7.6148044e-06\n",
      "Loss: 7.605011e-06\n",
      "Loss: 7.5989587e-06\n",
      "Loss: 7.708542e-06\n",
      "Loss: 7.5967378e-06\n",
      "Loss: 7.592749e-06\n",
      "Loss: 7.5760977e-06\n",
      "Loss: 7.562526e-06\n",
      "Loss: 7.5478465e-06\n",
      "Loss: 7.543442e-06\n",
      "Loss: 7.537872e-06\n",
      "Loss: 7.536283e-06\n",
      "Loss: 7.52717e-06\n",
      "Loss: 7.5235334e-06\n",
      "Loss: 7.521615e-06\n",
      "Loss: 7.519464e-06\n",
      "Loss: 7.517769e-06\n",
      "Loss: 7.513301e-06\n",
      "Loss: 7.506747e-06\n",
      "Loss: 7.4992813e-06\n",
      "Loss: 7.504104e-06\n",
      "Loss: 7.4932636e-06\n",
      "Loss: 7.4884238e-06\n",
      "Loss: 7.4869286e-06\n",
      "Loss: 7.48104e-06\n",
      "Loss: 7.478418e-06\n",
      "Loss: 7.472315e-06\n",
      "Loss: 7.4694863e-06\n",
      "Loss: 7.462497e-06\n",
      "Loss: 7.4590007e-06\n",
      "Loss: 7.453808e-06\n",
      "Loss: 7.4506934e-06\n",
      "Loss: 7.4456616e-06\n",
      "Loss: 7.4543423e-06\n",
      "Loss: 7.442791e-06\n",
      "Loss: 7.4436393e-06\n",
      "Loss: 7.4403833e-06\n",
      "Loss: 7.436876e-06\n",
      "Loss: 7.4338245e-06\n",
      "Loss: 7.4298473e-06\n",
      "Loss: 7.425318e-06\n",
      "Loss: 7.4331624e-06\n",
      "Loss: 7.4227746e-06\n",
      "Loss: 7.418706e-06\n",
      "Loss: 7.4158534e-06\n",
      "Loss: 7.414513e-06\n",
      "Loss: 7.4120435e-06\n",
      "Loss: 7.407211e-06\n",
      "Loss: 7.4030354e-06\n",
      "Loss: 7.3964475e-06\n",
      "Loss: 7.391644e-06\n",
      "Loss: 7.384735e-06\n",
      "Loss: 7.3765023e-06\n",
      "Loss: 7.3851725e-06\n",
      "Loss: 7.3727697e-06\n",
      "Loss: 7.36366e-06\n",
      "Loss: 7.360567e-06\n",
      "Loss: 7.3577994e-06\n",
      "Loss: 7.3623414e-06\n",
      "Loss: 7.3568713e-06\n",
      "Loss: 7.3542415e-06\n",
      "Loss: 7.350559e-06\n",
      "Loss: 7.4920285e-06\n",
      "Loss: 7.3501487e-06\n",
      "Loss: 7.3484007e-06\n",
      "Loss: 7.345925e-06\n",
      "Loss: 7.3419274e-06\n",
      "Loss: 7.3373308e-06\n",
      "Loss: 7.333906e-06\n",
      "Loss: 7.3304655e-06\n",
      "Loss: 7.3257825e-06\n",
      "Loss: 7.3235346e-06\n",
      "Loss: 7.320187e-06\n",
      "Loss: 7.321951e-06\n",
      "Loss: 7.3196343e-06\n",
      "Loss: 7.31848e-06\n",
      "Loss: 7.3179863e-06\n",
      "Loss: 7.3169913e-06\n",
      "Loss: 7.3148885e-06\n",
      "Loss: 7.3118626e-06\n",
      "Loss: 7.311981e-06\n",
      "Loss: 7.308927e-06\n",
      "Loss: 7.305441e-06\n",
      "Loss: 7.3024103e-06\n",
      "Loss: 7.2995826e-06\n",
      "Loss: 7.2961393e-06\n",
      "Loss: 7.303205e-06\n",
      "Loss: 7.2945927e-06\n",
      "Loss: 7.2922116e-06\n",
      "Loss: 7.290342e-06\n",
      "Loss: 7.2887924e-06\n",
      "Loss: 7.287233e-06\n",
      "Loss: 7.2856096e-06\n",
      "Loss: 7.2845487e-06\n",
      "Loss: 7.284729e-06\n",
      "Loss: 7.283059e-06\n",
      "Loss: 7.280958e-06\n",
      "Loss: 7.279115e-06\n",
      "Loss: 7.2736757e-06\n",
      "Loss: 7.268769e-06\n",
      "Loss: 7.263995e-06\n",
      "Loss: 7.4045006e-06\n",
      "Loss: 7.2636312e-06\n",
      "Loss: 7.2597486e-06\n",
      "Loss: 7.2546322e-06\n",
      "Loss: 7.247824e-06\n",
      "Loss: 7.2408675e-06\n",
      "Loss: 7.233559e-06\n",
      "Loss: 7.2265925e-06\n",
      "Loss: 7.2550024e-06\n",
      "Loss: 7.224585e-06\n",
      "Loss: 7.217826e-06\n",
      "Loss: 7.213367e-06\n",
      "Loss: 7.20932e-06\n",
      "Loss: 7.2095727e-06\n",
      "Loss: 7.2077123e-06\n",
      "Loss: 7.20525e-06\n",
      "Loss: 7.2014427e-06\n",
      "Loss: 7.194389e-06\n",
      "Loss: 7.1880345e-06\n",
      "Loss: 7.2088606e-06\n",
      "Loss: 7.183907e-06\n",
      "Loss: 7.1947234e-06\n",
      "Loss: 7.1827576e-06\n",
      "Loss: 7.1771105e-06\n",
      "Loss: 7.162909e-06\n",
      "Loss: 7.1527124e-06\n",
      "Loss: 7.1443296e-06\n",
      "Loss: 7.1371014e-06\n",
      "Loss: 7.1322766e-06\n",
      "Loss: 7.1283202e-06\n",
      "Loss: 7.1236664e-06\n",
      "Loss: 7.120697e-06\n",
      "Loss: 7.118996e-06\n",
      "Loss: 7.1319146e-06\n",
      "Loss: 7.118977e-06\n",
      "Loss: 7.1175077e-06\n",
      "Loss: 7.1132067e-06\n",
      "Loss: 7.110138e-06\n",
      "Loss: 7.1059985e-06\n",
      "Loss: 7.094096e-06\n",
      "Loss: 7.0829255e-06\n",
      "Loss: 7.0875376e-06\n",
      "Loss: 7.075232e-06\n",
      "Loss: 7.0653846e-06\n",
      "Loss: 7.0613455e-06\n",
      "Loss: 7.0571828e-06\n",
      "Loss: 7.0577457e-06\n",
      "Loss: 7.0538904e-06\n",
      "Loss: 7.0504666e-06\n",
      "Loss: 7.047506e-06\n",
      "Loss: 7.046544e-06\n",
      "Loss: 7.042337e-06\n",
      "Loss: 7.036991e-06\n",
      "Loss: 7.033558e-06\n",
      "Loss: 7.0244823e-06\n",
      "Loss: 7.0207107e-06\n",
      "Loss: 7.0154274e-06\n",
      "Loss: 7.012304e-06\n",
      "Loss: 7.0029414e-06\n",
      "Loss: 6.994019e-06\n",
      "Loss: 6.993703e-06\n",
      "Loss: 6.9903144e-06\n",
      "Loss: 6.985861e-06\n",
      "Loss: 6.9839843e-06\n",
      "Loss: 6.9797843e-06\n",
      "Loss: 6.974139e-06\n",
      "Loss: 6.9637226e-06\n",
      "Loss: 6.9552157e-06\n",
      "Loss: 6.9969383e-06\n",
      "Loss: 6.9517705e-06\n",
      "Loss: 6.9441767e-06\n",
      "Loss: 6.932661e-06\n",
      "Loss: 6.9217954e-06\n",
      "Loss: 6.9510797e-06\n",
      "Loss: 6.9192e-06\n",
      "Loss: 6.9118023e-06\n",
      "Loss: 6.9056296e-06\n",
      "Loss: 6.9005036e-06\n",
      "Loss: 6.8940067e-06\n",
      "Loss: 6.887685e-06\n",
      "Loss: 6.8953377e-06\n",
      "Loss: 6.886931e-06\n",
      "Loss: 6.8839777e-06\n",
      "Loss: 6.8803333e-06\n",
      "Loss: 6.8860563e-06\n",
      "Loss: 6.87789e-06\n",
      "Loss: 6.8730974e-06\n",
      "Loss: 6.871081e-06\n",
      "Loss: 6.868682e-06\n",
      "Loss: 6.8656814e-06\n",
      "Loss: 6.8629474e-06\n",
      "Loss: 6.8600907e-06\n",
      "Loss: 6.8612617e-06\n",
      "Loss: 6.858931e-06\n",
      "Loss: 6.8577638e-06\n",
      "Loss: 6.8552804e-06\n",
      "Loss: 6.853712e-06\n",
      "Loss: 6.850995e-06\n",
      "Loss: 6.846794e-06\n",
      "Loss: 6.8420463e-06\n",
      "Loss: 6.8375525e-06\n",
      "Loss: 6.832598e-06\n",
      "Loss: 6.8254435e-06\n",
      "Loss: 6.822508e-06\n",
      "Loss: 6.8191166e-06\n",
      "Loss: 6.8179065e-06\n",
      "Loss: 6.8167246e-06\n",
      "Loss: 6.8146783e-06\n",
      "Loss: 6.811855e-06\n",
      "Loss: 6.809706e-06\n",
      "Loss: 6.80818e-06\n",
      "Loss: 6.807193e-06\n",
      "Loss: 6.8056097e-06\n",
      "Loss: 6.8037893e-06\n",
      "Loss: 6.8028826e-06\n",
      "Loss: 6.8012623e-06\n",
      "Loss: 6.798548e-06\n",
      "Loss: 6.7957067e-06\n",
      "Loss: 6.8171557e-06\n",
      "Loss: 6.794008e-06\n",
      "Loss: 6.79276e-06\n",
      "Loss: 6.7893584e-06\n",
      "Loss: 6.7848773e-06\n",
      "Loss: 6.7817905e-06\n",
      "Loss: 6.779942e-06\n",
      "Loss: 6.778586e-06\n",
      "Loss: 6.776271e-06\n",
      "Loss: 6.772671e-06\n",
      "Loss: 6.767702e-06\n",
      "Loss: 6.763105e-06\n",
      "Loss: 6.7760798e-06\n",
      "Loss: 6.7607893e-06\n",
      "Loss: 6.7581423e-06\n",
      "Loss: 6.7535157e-06\n",
      "Loss: 6.750335e-06\n",
      "Loss: 6.745902e-06\n",
      "Loss: 6.7408123e-06\n",
      "Loss: 6.7805017e-06\n",
      "Loss: 6.7368674e-06\n",
      "Loss: 6.7299625e-06\n",
      "Loss: 6.7223255e-06\n",
      "Loss: 6.7191204e-06\n",
      "Loss: 6.7161645e-06\n",
      "Loss: 6.7140154e-06\n",
      "Loss: 6.7105757e-06\n",
      "Loss: 6.7084716e-06\n",
      "Loss: 6.705479e-06\n",
      "Loss: 6.70399e-06\n",
      "Loss: 6.7024366e-06\n",
      "Loss: 6.700072e-06\n",
      "Loss: 6.6971643e-06\n",
      "Loss: 6.6952416e-06\n",
      "Loss: 6.6935027e-06\n",
      "Loss: 6.6926523e-06\n",
      "Loss: 6.692065e-06\n",
      "Loss: 6.691396e-06\n",
      "Loss: 6.689789e-06\n",
      "Loss: 6.6881757e-06\n",
      "Loss: 6.6864363e-06\n",
      "Loss: 6.683669e-06\n",
      "Loss: 6.6810135e-06\n",
      "Loss: 6.678635e-06\n",
      "Loss: 6.6775733e-06\n",
      "Loss: 6.6745097e-06\n",
      "Loss: 6.6723833e-06\n",
      "Loss: 6.669221e-06\n",
      "Loss: 6.66551e-06\n",
      "Loss: 6.6642724e-06\n",
      "Loss: 6.660544e-06\n",
      "Loss: 6.659325e-06\n",
      "Loss: 6.657395e-06\n",
      "Loss: 6.653245e-06\n",
      "Loss: 6.650087e-06\n",
      "Loss: 6.6471725e-06\n",
      "Loss: 6.643979e-06\n",
      "Loss: 6.63954e-06\n",
      "Loss: 6.6378234e-06\n",
      "Loss: 6.6331154e-06\n",
      "Loss: 6.6318516e-06\n",
      "Loss: 6.6305397e-06\n",
      "Loss: 6.6269313e-06\n",
      "Loss: 6.6224243e-06\n",
      "Loss: 6.619077e-06\n",
      "Loss: 6.6165594e-06\n",
      "Loss: 6.614375e-06\n",
      "Loss: 6.610965e-06\n",
      "Loss: 6.6082484e-06\n",
      "Loss: 6.6034054e-06\n",
      "Loss: 6.598986e-06\n",
      "Loss: 6.5923687e-06\n",
      "Loss: 6.5864488e-06\n",
      "Loss: 6.5798895e-06\n",
      "Loss: 6.5733084e-06\n",
      "Loss: 6.568518e-06\n",
      "Loss: 6.565403e-06\n",
      "Loss: 6.566474e-06\n",
      "Loss: 6.564387e-06\n",
      "Loss: 6.5627532e-06\n",
      "Loss: 6.560517e-06\n",
      "Loss: 6.557895e-06\n",
      "Loss: 6.5550394e-06\n",
      "Loss: 6.5535014e-06\n",
      "Loss: 6.5509626e-06\n",
      "Loss: 6.5488207e-06\n",
      "Loss: 6.546249e-06\n",
      "Loss: 6.5590693e-06\n",
      "Loss: 6.545661e-06\n",
      "Loss: 6.54265e-06\n",
      "Loss: 6.5408035e-06\n",
      "Loss: 6.5390186e-06\n",
      "Loss: 6.536499e-06\n",
      "Loss: 6.5321055e-06\n",
      "Loss: 6.534707e-06\n",
      "Loss: 6.5291433e-06\n",
      "Loss: 6.5265e-06\n",
      "Loss: 6.5241406e-06\n",
      "Loss: 6.520662e-06\n",
      "Loss: 6.5218446e-06\n",
      "Loss: 6.5176323e-06\n",
      "Loss: 6.5122513e-06\n",
      "Loss: 6.5094328e-06\n",
      "Loss: 6.5063405e-06\n",
      "Loss: 6.5029913e-06\n",
      "Loss: 6.503878e-06\n",
      "Loss: 6.500374e-06\n",
      "Loss: 6.4966544e-06\n",
      "Loss: 6.493351e-06\n",
      "Loss: 6.4911283e-06\n",
      "Loss: 6.4852907e-06\n",
      "Loss: 6.483467e-06\n",
      "Loss: 6.477175e-06\n",
      "Loss: 6.475162e-06\n",
      "Loss: 6.4716937e-06\n",
      "Loss: 6.4725327e-06\n",
      "Loss: 6.469557e-06\n",
      "Loss: 6.466623e-06\n",
      "Loss: 6.461715e-06\n",
      "Loss: 6.457394e-06\n",
      "Loss: 6.452589e-06\n",
      "Loss: 6.447208e-06\n",
      "Loss: 6.4446995e-06\n",
      "Loss: 6.4406377e-06\n",
      "Loss: 6.4373544e-06\n",
      "Loss: 6.434231e-06\n",
      "Loss: 6.4290125e-06\n",
      "Loss: 6.4286214e-06\n",
      "Loss: 6.426234e-06\n",
      "Loss: 6.4227406e-06\n",
      "Loss: 6.4186115e-06\n",
      "Loss: 6.4141404e-06\n",
      "Loss: 6.400559e-06\n",
      "Loss: 6.3936895e-06\n",
      "Loss: 6.3780694e-06\n",
      "Loss: 6.375237e-06\n",
      "Loss: 6.370362e-06\n",
      "Loss: 6.3668494e-06\n",
      "Loss: 6.363928e-06\n",
      "Loss: 6.358457e-06\n",
      "Loss: 6.353451e-06\n",
      "Loss: 6.38986e-06\n",
      "Loss: 6.352043e-06\n",
      "Loss: 6.3468515e-06\n",
      "Loss: 6.343799e-06\n",
      "Loss: 6.3404354e-06\n",
      "Loss: 6.3375114e-06\n",
      "Loss: 6.354754e-06\n",
      "Loss: 6.336639e-06\n",
      "Loss: 6.333078e-06\n",
      "Loss: 6.3308657e-06\n",
      "Loss: 6.3280695e-06\n",
      "Loss: 6.3257257e-06\n",
      "Loss: 6.3216494e-06\n",
      "Loss: 6.315479e-06\n",
      "Loss: 6.312272e-06\n",
      "Loss: 6.309663e-06\n",
      "Loss: 6.306287e-06\n",
      "Loss: 6.2949366e-06\n",
      "Loss: 6.2874838e-06\n",
      "Loss: 6.276811e-06\n",
      "Loss: 6.271154e-06\n",
      "Loss: 6.2677937e-06\n",
      "Loss: 6.262878e-06\n",
      "Loss: 6.2512595e-06\n",
      "Loss: 6.440711e-06\n",
      "Loss: 6.249827e-06\n",
      "Loss: 6.2456315e-06\n",
      "Loss: 6.2439144e-06\n",
      "Loss: 6.2425306e-06\n",
      "Loss: 6.24077e-06\n",
      "Loss: 6.2367712e-06\n",
      "Loss: 6.245714e-06\n",
      "Loss: 6.234286e-06\n",
      "Loss: 6.2284594e-06\n",
      "Loss: 6.2264057e-06\n",
      "Loss: 6.2243867e-06\n",
      "Loss: 6.2218687e-06\n",
      "Loss: 6.219225e-06\n",
      "Loss: 6.2177664e-06\n",
      "Loss: 6.2154218e-06\n",
      "Loss: 6.210614e-06\n",
      "Loss: 6.208098e-06\n",
      "Loss: 6.2156623e-06\n",
      "Loss: 6.2046875e-06\n",
      "Loss: 6.2004256e-06\n",
      "Loss: 6.194609e-06\n",
      "Loss: 6.1890237e-06\n",
      "Loss: 6.18147e-06\n",
      "Loss: 6.1736405e-06\n",
      "Loss: 6.1681512e-06\n",
      "Loss: 6.1683095e-06\n",
      "Loss: 6.1668893e-06\n",
      "Loss: 6.1711335e-06\n",
      "Loss: 6.1643873e-06\n",
      "Loss: 6.162346e-06\n",
      "Loss: 6.15698e-06\n",
      "Loss: 6.15572e-06\n",
      "Loss: 6.153474e-06\n",
      "Loss: 6.1517194e-06\n",
      "Loss: 6.1506926e-06\n",
      "Loss: 6.1494015e-06\n",
      "Loss: 6.2155523e-06\n",
      "Loss: 6.148876e-06\n",
      "Loss: 6.1463334e-06\n",
      "Loss: 6.1437822e-06\n",
      "Loss: 6.139648e-06\n",
      "Loss: 6.1347446e-06\n",
      "Loss: 6.13031e-06\n",
      "Loss: 6.1260944e-06\n",
      "Loss: 6.1215947e-06\n",
      "Loss: 6.118002e-06\n",
      "Loss: 6.1261735e-06\n",
      "Loss: 6.1163287e-06\n",
      "Loss: 6.1124683e-06\n",
      "Loss: 6.106228e-06\n",
      "Loss: 6.103147e-06\n",
      "Loss: 6.100533e-06\n",
      "Loss: 6.097623e-06\n",
      "Loss: 6.093685e-06\n",
      "Loss: 6.090053e-06\n",
      "Loss: 6.087488e-06\n",
      "Loss: 6.085294e-06\n",
      "Loss: 6.08396e-06\n",
      "Loss: 6.082516e-06\n",
      "Loss: 6.0806906e-06\n",
      "Loss: 6.123212e-06\n",
      "Loss: 6.0765014e-06\n",
      "Loss: 6.071913e-06\n",
      "Loss: 6.065745e-06\n",
      "Loss: 6.062838e-06\n",
      "Loss: 6.0599673e-06\n",
      "Loss: 6.061724e-06\n",
      "Loss: 6.0597617e-06\n",
      "Loss: 6.05712e-06\n",
      "Loss: 6.0562334e-06\n",
      "Loss: 6.0545467e-06\n",
      "Loss: 6.053195e-06\n",
      "Loss: 6.051294e-06\n",
      "Loss: 6.0480893e-06\n",
      "Loss: 6.0443176e-06\n",
      "Loss: 6.041593e-06\n",
      "Loss: 6.038218e-06\n",
      "Loss: 6.0335715e-06\n",
      "Loss: 6.0271086e-06\n",
      "Loss: 6.0233147e-06\n",
      "Loss: 6.02076e-06\n",
      "Loss: 6.0179505e-06\n",
      "Loss: 6.013339e-06\n",
      "Loss: 6.012572e-06\n",
      "Loss: 6.010144e-06\n",
      "Loss: 6.0086095e-06\n",
      "Loss: 6.006729e-06\n",
      "Loss: 6.009791e-06\n",
      "Loss: 6.004637e-06\n",
      "Loss: 6.0024945e-06\n",
      "Loss: 6.001189e-06\n",
      "Loss: 5.998347e-06\n",
      "Loss: 5.9955823e-06\n",
      "Loss: 5.9925505e-06\n",
      "Loss: 5.9896906e-06\n",
      "Loss: 5.9887634e-06\n",
      "Loss: 5.9872514e-06\n",
      "Loss: 5.985888e-06\n",
      "Loss: 5.9849717e-06\n",
      "Loss: 5.98315e-06\n",
      "Loss: 5.9785775e-06\n",
      "Loss: 5.97475e-06\n",
      "Loss: 5.97131e-06\n",
      "Loss: 5.9872846e-06\n",
      "Loss: 5.970613e-06\n",
      "Loss: 5.968795e-06\n",
      "Loss: 5.9663694e-06\n",
      "Loss: 5.964025e-06\n",
      "Loss: 5.9607946e-06\n",
      "Loss: 5.9588137e-06\n",
      "Loss: 5.954669e-06\n",
      "Loss: 5.9527383e-06\n",
      "Loss: 5.950562e-06\n",
      "Loss: 5.9496397e-06\n",
      "Loss: 5.9485737e-06\n",
      "Loss: 5.947223e-06\n",
      "Loss: 5.9450404e-06\n",
      "Loss: 5.941454e-06\n",
      "Loss: 5.940037e-06\n",
      "Loss: 5.956556e-06\n",
      "Loss: 5.9361387e-06\n",
      "Loss: 5.9339955e-06\n",
      "Loss: 5.932586e-06\n",
      "Loss: 5.9292156e-06\n",
      "Loss: 5.9280183e-06\n",
      "Loss: 5.95581e-06\n",
      "Loss: 5.926947e-06\n",
      "Loss: 5.9247054e-06\n",
      "Loss: 5.9206827e-06\n",
      "Loss: 5.91967e-06\n",
      "Loss: 5.91531e-06\n",
      "Loss: 5.913861e-06\n",
      "Loss: 5.9125273e-06\n",
      "Loss: 5.910504e-06\n",
      "Loss: 5.9069616e-06\n",
      "Loss: 5.902142e-06\n",
      "Loss: 5.8951878e-06\n",
      "Loss: 5.8934634e-06\n",
      "Loss: 5.886315e-06\n",
      "Loss: 5.884354e-06\n",
      "Loss: 5.883143e-06\n",
      "Loss: 5.8817623e-06\n",
      "Loss: 5.8790415e-06\n",
      "Loss: 5.87823e-06\n",
      "Loss: 5.8748747e-06\n",
      "Loss: 5.871851e-06\n",
      "Loss: 5.869274e-06\n",
      "Loss: 5.8657306e-06\n",
      "Loss: 5.8637547e-06\n",
      "Loss: 5.8618716e-06\n",
      "Loss: 5.860702e-06\n",
      "Loss: 5.857906e-06\n",
      "Loss: 5.8556034e-06\n",
      "Loss: 5.85264e-06\n",
      "Loss: 5.8480846e-06\n",
      "Loss: 5.8441874e-06\n",
      "Loss: 5.8488104e-06\n",
      "Loss: 5.8358396e-06\n",
      "Loss: 5.8522664e-06\n",
      "Loss: 5.831563e-06\n",
      "Loss: 5.821984e-06\n",
      "Loss: 5.8143555e-06\n",
      "Loss: 5.8099063e-06\n",
      "Loss: 5.809846e-06\n",
      "Loss: 5.803372e-06\n",
      "Loss: 5.796123e-06\n",
      "Loss: 5.79174e-06\n",
      "Loss: 5.7887673e-06\n",
      "Loss: 5.786275e-06\n",
      "Loss: 5.7842044e-06\n",
      "Loss: 5.781273e-06\n",
      "Loss: 5.777381e-06\n",
      "Loss: 5.77392e-06\n",
      "Loss: 5.7705797e-06\n",
      "Loss: 5.7670723e-06\n",
      "Loss: 5.763639e-06\n",
      "Loss: 5.7606494e-06\n",
      "Loss: 5.7558873e-06\n",
      "Loss: 5.7513007e-06\n",
      "Loss: 5.7467637e-06\n",
      "Loss: 5.7438797e-06\n",
      "Loss: 5.7388647e-06\n",
      "Loss: 5.8300316e-06\n",
      "Loss: 5.736055e-06\n",
      "Loss: 5.7304383e-06\n",
      "Loss: 5.722197e-06\n",
      "Loss: 5.7196744e-06\n",
      "Loss: 5.717877e-06\n",
      "Loss: 5.7140187e-06\n",
      "Loss: 5.7118223e-06\n",
      "Loss: 5.700548e-06\n",
      "Loss: 5.6965137e-06\n",
      "Loss: 5.6933436e-06\n",
      "Loss: 5.6985773e-06\n",
      "Loss: 5.6914296e-06\n",
      "Loss: 5.6895115e-06\n",
      "Loss: 5.686873e-06\n",
      "Loss: 5.6903064e-06\n",
      "Loss: 5.684542e-06\n",
      "Loss: 5.681959e-06\n",
      "Loss: 5.678066e-06\n",
      "Loss: 5.676263e-06\n",
      "Loss: 5.6732447e-06\n",
      "Loss: 5.670089e-06\n",
      "Loss: 5.66507e-06\n",
      "Loss: 5.662183e-06\n",
      "Loss: 5.659542e-06\n",
      "Loss: 5.6609088e-06\n",
      "Loss: 5.657619e-06\n",
      "Loss: 5.655544e-06\n",
      "Loss: 5.6515883e-06\n",
      "Loss: 5.648434e-06\n",
      "Loss: 5.6453728e-06\n",
      "Loss: 5.643343e-06\n",
      "Loss: 5.6411304e-06\n",
      "Loss: 5.6375447e-06\n",
      "Loss: 5.6340814e-06\n",
      "Loss: 5.6306926e-06\n",
      "Loss: 5.641109e-06\n",
      "Loss: 5.6283934e-06\n",
      "Loss: 5.62525e-06\n",
      "Loss: 5.622282e-06\n",
      "Loss: 5.6200133e-06\n",
      "Loss: 5.619235e-06\n",
      "Loss: 5.616962e-06\n",
      "Loss: 5.6157005e-06\n",
      "Loss: 5.6145727e-06\n",
      "Loss: 5.613108e-06\n",
      "Loss: 5.6108424e-06\n",
      "Loss: 5.7580037e-06\n",
      "Loss: 5.610502e-06\n",
      "Loss: 5.6069593e-06\n",
      "Loss: 5.6041363e-06\n",
      "Loss: 5.602149e-06\n",
      "Loss: 5.600037e-06\n",
      "Loss: 5.596617e-06\n",
      "Loss: 5.6039707e-06\n",
      "Loss: 5.594421e-06\n",
      "Loss: 5.5895516e-06\n",
      "Loss: 5.5872056e-06\n",
      "Loss: 5.585563e-06\n",
      "Loss: 5.58366e-06\n",
      "Loss: 5.580944e-06\n",
      "Loss: 5.5762366e-06\n",
      "Loss: 5.573564e-06\n",
      "Loss: 5.5762075e-06\n",
      "Loss: 5.571579e-06\n",
      "Loss: 5.568709e-06\n",
      "Loss: 5.5663436e-06\n",
      "Loss: 5.5649875e-06\n",
      "Loss: 5.563881e-06\n",
      "Loss: 5.561955e-06\n",
      "Loss: 5.560345e-06\n",
      "Loss: 5.560136e-06\n",
      "Loss: 5.5574833e-06\n",
      "Loss: 5.5565447e-06\n",
      "Loss: 5.555109e-06\n",
      "Loss: 5.5540963e-06\n",
      "Loss: 5.5602895e-06\n",
      "Loss: 5.554026e-06\n",
      "Loss: 5.552822e-06\n",
      "Loss: 5.551432e-06\n",
      "Loss: 5.5508635e-06\n",
      "Loss: 5.549333e-06\n",
      "Loss: 5.547038e-06\n",
      "Loss: 5.5476385e-06\n",
      "Loss: 5.5461683e-06\n",
      "Loss: 5.5433356e-06\n",
      "Loss: 5.5410774e-06\n",
      "Loss: 5.5390556e-06\n",
      "Loss: 5.5358423e-06\n",
      "Loss: 5.5322894e-06\n",
      "Loss: 5.528395e-06\n",
      "Loss: 5.5253863e-06\n",
      "Loss: 5.523715e-06\n",
      "Loss: 5.5213895e-06\n",
      "Loss: 5.51898e-06\n",
      "Loss: 5.5169207e-06\n",
      "Loss: 5.515513e-06\n",
      "Loss: 5.5123855e-06\n",
      "Loss: 5.5094924e-06\n",
      "Loss: 5.5068167e-06\n",
      "Loss: 5.4995517e-06\n",
      "Loss: 5.4965103e-06\n",
      "Loss: 5.4942775e-06\n",
      "Loss: 5.491715e-06\n",
      "Loss: 5.4981238e-06\n",
      "Loss: 5.4896964e-06\n",
      "Loss: 5.4874613e-06\n",
      "Loss: 5.4859565e-06\n",
      "Loss: 5.48424e-06\n",
      "Loss: 5.480578e-06\n",
      "Loss: 5.493971e-06\n",
      "Loss: 5.4797183e-06\n",
      "Loss: 5.4773936e-06\n",
      "Loss: 5.4762804e-06\n",
      "Loss: 5.4751763e-06\n",
      "Loss: 5.472513e-06\n",
      "Loss: 5.4687407e-06\n",
      "Loss: 5.4643597e-06\n",
      "Loss: 5.460781e-06\n",
      "Loss: 5.4573657e-06\n",
      "Loss: 5.485012e-06\n",
      "Loss: 5.4555435e-06\n",
      "Loss: 5.4536185e-06\n",
      "Loss: 5.4509674e-06\n",
      "Loss: 5.4498037e-06\n",
      "Loss: 5.448468e-06\n",
      "Loss: 5.447185e-06\n",
      "Loss: 5.445583e-06\n",
      "Loss: 5.443009e-06\n",
      "Loss: 5.4547168e-06\n",
      "Loss: 5.4416214e-06\n",
      "Loss: 5.440151e-06\n",
      "Loss: 5.437083e-06\n",
      "Loss: 5.4377942e-06\n",
      "Loss: 5.4358893e-06\n",
      "Loss: 5.4321854e-06\n",
      "Loss: 5.4293478e-06\n",
      "Loss: 5.4267584e-06\n",
      "Loss: 5.422715e-06\n",
      "Loss: 5.420029e-06\n",
      "Loss: 5.415522e-06\n",
      "Loss: 5.41307e-06\n",
      "Loss: 5.427909e-06\n",
      "Loss: 5.4118364e-06\n",
      "Loss: 5.409472e-06\n",
      "Loss: 5.406653e-06\n",
      "Loss: 5.4019015e-06\n",
      "Loss: 5.4031652e-06\n",
      "Loss: 5.400328e-06\n",
      "Loss: 5.3983445e-06\n",
      "Loss: 5.3962312e-06\n",
      "Loss: 5.394855e-06\n",
      "Loss: 5.392231e-06\n",
      "Loss: 5.389656e-06\n",
      "Loss: 5.3860485e-06\n",
      "Loss: 5.3823424e-06\n",
      "Loss: 5.375422e-06\n",
      "Loss: 5.3699405e-06\n",
      "Loss: 5.3667354e-06\n",
      "Loss: 5.3638905e-06\n",
      "Loss: 5.363281e-06\n",
      "Loss: 5.3608173e-06\n",
      "Loss: 5.358488e-06\n",
      "Loss: 5.353795e-06\n",
      "Loss: 5.3583935e-06\n",
      "Loss: 5.352484e-06\n",
      "Loss: 5.3484287e-06\n",
      "Loss: 5.346954e-06\n",
      "Loss: 5.344669e-06\n",
      "Loss: 5.3429976e-06\n",
      "Loss: 5.339429e-06\n",
      "Loss: 5.339426e-06\n",
      "Loss: 5.337819e-06\n",
      "Loss: 5.3365497e-06\n",
      "Loss: 5.3355275e-06\n",
      "Loss: 5.3344515e-06\n",
      "Loss: 5.3309436e-06\n",
      "Loss: 5.329664e-06\n",
      "Loss: 5.328207e-06\n",
      "Loss: 5.327005e-06\n",
      "Loss: 5.35973e-06\n",
      "Loss: 5.326665e-06\n",
      "Loss: 5.325886e-06\n",
      "Loss: 5.3248677e-06\n",
      "Loss: 5.3782114e-06\n",
      "Loss: 5.32469e-06\n",
      "Loss: 5.324082e-06\n",
      "Loss: 5.3233716e-06\n",
      "Loss: 5.3214344e-06\n",
      "Loss: 5.3217645e-06\n",
      "Loss: 5.3211998e-06\n",
      "Loss: 5.3203175e-06\n",
      "Loss: 5.31962e-06\n",
      "Loss: 5.318652e-06\n",
      "Loss: 5.315882e-06\n",
      "Loss: 5.312705e-06\n",
      "Loss: 5.325562e-06\n",
      "Loss: 5.3120807e-06\n",
      "Loss: 5.3101e-06\n",
      "Loss: 5.3083495e-06\n",
      "Loss: 5.306382e-06\n",
      "Loss: 5.307423e-06\n",
      "Loss: 5.305952e-06\n",
      "Loss: 5.3043923e-06\n",
      "Loss: 5.3026497e-06\n",
      "Loss: 5.3020767e-06\n",
      "Loss: 5.301109e-06\n",
      "Loss: 5.3000276e-06\n",
      "Loss: 5.298183e-06\n",
      "Loss: 5.2957266e-06\n",
      "Loss: 5.297659e-06\n",
      "Loss: 5.2943838e-06\n",
      "Loss: 5.293203e-06\n",
      "Loss: 5.2921755e-06\n",
      "Loss: 5.29236e-06\n",
      "Loss: 5.2914233e-06\n",
      "Loss: 5.2905916e-06\n",
      "Loss: 5.28928e-06\n",
      "Loss: 5.288805e-06\n",
      "Loss: 5.2876867e-06\n",
      "Loss: 5.2870796e-06\n",
      "Loss: 5.2849746e-06\n",
      "Loss: 5.284852e-06\n",
      "Loss: 5.2842015e-06\n",
      "Loss: 5.2842584e-06\n",
      "Loss: 5.2841497e-06\n",
      "Loss: 5.2842674e-06\n",
      "Loss: 5.284088e-06\n",
      "Loss: 5.284515e-06\n",
      "Loss: 5.284088e-06\n",
      "Loss: 5.282961e-06\n",
      "Loss: 5.2822706e-06\n",
      "Loss: 5.280284e-06\n",
      "Loss: 5.279718e-06\n",
      "Loss: 5.2873415e-06\n",
      "Loss: 5.2793475e-06\n",
      "Loss: 5.2781756e-06\n",
      "Loss: 5.2771416e-06\n",
      "Loss: 5.2764503e-06\n",
      "Loss: 5.2747364e-06\n",
      "Loss: 5.273353e-06\n",
      "Loss: 5.27177e-06\n",
      "Loss: 5.2684086e-06\n",
      "Loss: 5.265657e-06\n",
      "Loss: 5.263487e-06\n",
      "Loss: 5.262265e-06\n",
      "Loss: 5.2597807e-06\n",
      "Loss: 5.2578307e-06\n",
      "Loss: 5.255784e-06\n",
      "Loss: 5.2544974e-06\n",
      "Loss: 5.252867e-06\n",
      "Loss: 5.2519385e-06\n",
      "Loss: 5.2512087e-06\n",
      "Loss: 5.2497103e-06\n",
      "Loss: 5.248174e-06\n",
      "Loss: 5.2471805e-06\n",
      "Loss: 5.2446944e-06\n",
      "Loss: 5.243088e-06\n",
      "Loss: 5.251215e-06\n",
      "Loss: 5.241227e-06\n",
      "Loss: 5.239659e-06\n",
      "Loss: 5.2371065e-06\n",
      "Loss: 5.235724e-06\n",
      "Loss: 5.2337427e-06\n",
      "Loss: 5.236589e-06\n",
      "Loss: 5.2333335e-06\n",
      "Loss: 5.231826e-06\n",
      "Loss: 5.229653e-06\n",
      "Loss: 5.2280952e-06\n",
      "Loss: 5.226455e-06\n",
      "Loss: 5.223679e-06\n",
      "Loss: 5.2220685e-06\n",
      "Loss: 5.2173405e-06\n",
      "Loss: 5.2144737e-06\n",
      "Loss: 5.212159e-06\n",
      "Loss: 5.2103987e-06\n",
      "Loss: 5.208725e-06\n",
      "Loss: 5.2067307e-06\n",
      "Loss: 5.2043188e-06\n",
      "Loss: 5.201381e-06\n",
      "Loss: 5.1989064e-06\n",
      "Loss: 5.1999955e-06\n",
      "Loss: 5.197974e-06\n",
      "Loss: 5.1957845e-06\n",
      "Loss: 5.195166e-06\n",
      "Loss: 5.193742e-06\n",
      "Loss: 5.192579e-06\n",
      "Loss: 5.1913103e-06\n",
      "Loss: 5.1911757e-06\n",
      "Loss: 5.190115e-06\n",
      "Loss: 5.18905e-06\n",
      "Loss: 5.1873853e-06\n",
      "Loss: 5.1854768e-06\n",
      "Loss: 5.1839024e-06\n",
      "Loss: 5.181914e-06\n",
      "Loss: 5.1796887e-06\n",
      "Loss: 5.1751813e-06\n",
      "Loss: 5.170507e-06\n",
      "Loss: 5.1692327e-06\n",
      "Loss: 5.1623038e-06\n",
      "Loss: 5.1606758e-06\n",
      "Loss: 5.158935e-06\n",
      "Loss: 5.1570205e-06\n",
      "Loss: 5.161654e-06\n",
      "Loss: 5.156289e-06\n",
      "Loss: 5.154398e-06\n",
      "Loss: 5.2087694e-06\n",
      "Loss: 5.1532043e-06\n",
      "Loss: 5.14996e-06\n",
      "Loss: 5.148703e-06\n",
      "Loss: 5.1463585e-06\n",
      "Loss: 5.147923e-06\n",
      "Loss: 5.1450793e-06\n",
      "Loss: 5.1423362e-06\n",
      "Loss: 5.1370225e-06\n",
      "Loss: 5.133728e-06\n",
      "Loss: 5.131246e-06\n",
      "Loss: 5.1292736e-06\n",
      "Loss: 5.1267534e-06\n",
      "Loss: 5.1246598e-06\n",
      "Loss: 5.121572e-06\n",
      "Loss: 5.1181805e-06\n",
      "Loss: 5.1287707e-06\n",
      "Loss: 5.116012e-06\n",
      "Loss: 5.11309e-06\n",
      "Loss: 5.11008e-06\n",
      "Loss: 5.107282e-06\n",
      "Loss: 5.1040356e-06\n",
      "Loss: 5.0999965e-06\n",
      "Loss: 5.0930826e-06\n",
      "Loss: 5.0956232e-06\n",
      "Loss: 5.0892468e-06\n",
      "Loss: 5.0861067e-06\n",
      "Loss: 5.084874e-06\n",
      "Loss: 5.0840176e-06\n",
      "Loss: 5.0829008e-06\n",
      "Loss: 5.0813087e-06\n",
      "Loss: 5.083635e-06\n",
      "Loss: 5.079833e-06\n",
      "Loss: 5.0771946e-06\n",
      "Loss: 5.0773024e-06\n",
      "Loss: 5.076461e-06\n",
      "Loss: 5.0737003e-06\n",
      "Loss: 5.0697467e-06\n",
      "Loss: 5.0677722e-06\n",
      "Loss: 5.0626318e-06\n",
      "Loss: 5.0586973e-06\n",
      "Loss: 5.055271e-06\n",
      "Loss: 5.0812123e-06\n",
      "Loss: 5.0537183e-06\n",
      "Loss: 5.0498525e-06\n",
      "Loss: 5.0456224e-06\n",
      "Loss: 5.041468e-06\n",
      "Loss: 5.037238e-06\n",
      "Loss: 5.0339777e-06\n",
      "Loss: 5.030259e-06\n",
      "Loss: 5.027985e-06\n",
      "Loss: 5.0251924e-06\n",
      "Loss: 5.0236613e-06\n",
      "Loss: 5.0217154e-06\n",
      "Loss: 5.017937e-06\n",
      "Loss: 5.016032e-06\n",
      "Loss: 5.0128574e-06\n",
      "Loss: 5.029126e-06\n",
      "Loss: 5.0113513e-06\n",
      "Loss: 5.304237e-06\n",
      "Loss: 5.009737e-06\n",
      "Loss: 5.0038375e-06\n",
      "Loss: 4.9975756e-06\n",
      "Loss: 4.9916457e-06\n",
      "Loss: 4.985439e-06\n",
      "Loss: 4.9810074e-06\n",
      "Loss: 4.975745e-06\n",
      "Loss: 4.9713917e-06\n",
      "Loss: 4.9759137e-06\n",
      "Loss: 4.9686882e-06\n",
      "Loss: 4.9653017e-06\n",
      "Loss: 4.963129e-06\n",
      "Loss: 4.9616774e-06\n",
      "Loss: 4.9570335e-06\n",
      "Loss: 4.9535593e-06\n",
      "Loss: 4.9513164e-06\n",
      "Loss: 4.9492446e-06\n",
      "Loss: 4.9475766e-06\n",
      "Loss: 4.946441e-06\n",
      "Loss: 4.9446826e-06\n",
      "Loss: 4.9437767e-06\n",
      "Loss: 4.9425107e-06\n",
      "Loss: 4.9407663e-06\n",
      "Loss: 4.939029e-06\n",
      "Loss: 4.938554e-06\n",
      "Loss: 4.937613e-06\n",
      "Loss: 4.9344676e-06\n",
      "Loss: 4.9308887e-06\n",
      "Loss: 4.9267614e-06\n",
      "Loss: 4.9237087e-06\n",
      "Loss: 4.920033e-06\n",
      "Loss: 4.9174205e-06\n",
      "Loss: 4.9156065e-06\n",
      "Loss: 4.9129058e-06\n",
      "Loss: 4.9108776e-06\n",
      "Loss: 4.909089e-06\n",
      "Loss: 4.907835e-06\n",
      "Loss: 4.90687e-06\n",
      "Loss: 4.905535e-06\n",
      "Loss: 4.9048444e-06\n",
      "Loss: 4.9038435e-06\n",
      "Loss: 4.9030295e-06\n",
      "Loss: 4.902806e-06\n",
      "Loss: 4.901676e-06\n",
      "Loss: 4.9028845e-06\n",
      "Loss: 4.899608e-06\n",
      "Loss: 4.898923e-06\n",
      "Loss: 4.89626e-06\n",
      "Loss: 4.895629e-06\n",
      "Loss: 4.8942143e-06\n",
      "Loss: 4.891562e-06\n",
      "Loss: 4.8890633e-06\n",
      "Loss: 4.8851884e-06\n",
      "Loss: 4.877446e-06\n",
      "Loss: 4.8879347e-06\n",
      "Loss: 4.875261e-06\n",
      "Loss: 4.871835e-06\n",
      "Loss: 4.8702086e-06\n",
      "Loss: 4.869423e-06\n",
      "Loss: 4.8686525e-06\n",
      "Loss: 4.8668544e-06\n",
      "Loss: 4.8638076e-06\n",
      "Loss: 4.863953e-06\n",
      "Loss: 4.8619977e-06\n",
      "Loss: 4.8589204e-06\n",
      "Loss: 4.856408e-06\n",
      "Loss: 4.853716e-06\n",
      "Loss: 4.8513366e-06\n",
      "Loss: 4.8492084e-06\n",
      "Loss: 4.846878e-06\n",
      "Loss: 4.8435186e-06\n",
      "Loss: 4.8416005e-06\n",
      "Loss: 4.8359393e-06\n",
      "Loss: 4.833176e-06\n",
      "Loss: 4.829025e-06\n",
      "Loss: 4.836066e-06\n",
      "Loss: 4.828374e-06\n",
      "Loss: 4.8264305e-06\n",
      "Loss: 4.8240067e-06\n",
      "Loss: 4.821121e-06\n",
      "Loss: 4.8179368e-06\n",
      "Loss: 4.8146508e-06\n",
      "Loss: 4.810742e-06\n",
      "Loss: 4.8069755e-06\n",
      "Loss: 4.802302e-06\n",
      "Loss: 4.8003644e-06\n",
      "Loss: 4.7948397e-06\n",
      "Loss: 4.7925105e-06\n",
      "Loss: 4.790154e-06\n",
      "Loss: 4.7889653e-06\n",
      "Loss: 4.7875014e-06\n",
      "Loss: 4.7866533e-06\n",
      "Loss: 4.7858794e-06\n",
      "Loss: 4.784979e-06\n",
      "Loss: 4.7831195e-06\n",
      "Loss: 4.7852654e-06\n",
      "Loss: 4.7803187e-06\n",
      "Loss: 4.7772874e-06\n",
      "Loss: 4.773801e-06\n",
      "Loss: 4.7708513e-06\n",
      "Loss: 4.7686244e-06\n",
      "Loss: 4.765805e-06\n",
      "Loss: 4.7637604e-06\n",
      "Loss: 4.761289e-06\n",
      "Loss: 4.7588046e-06\n",
      "Loss: 4.7679173e-06\n",
      "Loss: 4.757131e-06\n",
      "Loss: 4.754281e-06\n",
      "Loss: 4.750315e-06\n",
      "Loss: 4.7478984e-06\n",
      "Loss: 4.7475196e-06\n",
      "Loss: 4.7459766e-06\n",
      "Loss: 4.744066e-06\n",
      "Loss: 4.7428557e-06\n",
      "Loss: 4.7419335e-06\n",
      "Loss: 4.741179e-06\n",
      "Loss: 4.7393023e-06\n",
      "Loss: 4.7363774e-06\n",
      "Loss: 4.7343647e-06\n",
      "Loss: 4.7322787e-06\n",
      "Loss: 4.7304447e-06\n",
      "Loss: 4.726982e-06\n",
      "Loss: 4.7280528e-06\n",
      "Loss: 4.725516e-06\n",
      "Loss: 4.722547e-06\n",
      "Loss: 4.721029e-06\n",
      "Loss: 4.7191543e-06\n",
      "Loss: 4.717944e-06\n",
      "Loss: 4.712684e-06\n",
      "Loss: 4.7102035e-06\n",
      "Loss: 4.708119e-06\n",
      "Loss: 4.706521e-06\n",
      "Loss: 4.702832e-06\n",
      "Loss: 4.7009526e-06\n",
      "Loss: 4.6982595e-06\n",
      "Loss: 4.6964265e-06\n",
      "Loss: 4.6936693e-06\n",
      "Loss: 4.6919067e-06\n",
      "Loss: 4.688681e-06\n",
      "Loss: 4.6865603e-06\n",
      "Loss: 4.6854448e-06\n",
      "Loss: 4.684981e-06\n",
      "Loss: 4.6840782e-06\n",
      "Loss: 4.6830705e-06\n",
      "Loss: 4.6821856e-06\n",
      "Loss: 4.6812156e-06\n",
      "Loss: 4.6800733e-06\n",
      "Loss: 4.680348e-06\n",
      "Loss: 4.6791997e-06\n",
      "Loss: 4.6781606e-06\n",
      "Loss: 4.676186e-06\n",
      "Loss: 4.6743435e-06\n",
      "Loss: 4.672142e-06\n",
      "Loss: 4.6689274e-06\n",
      "Loss: 4.666507e-06\n",
      "Loss: 4.6662108e-06\n",
      "Loss: 4.663133e-06\n",
      "Loss: 4.6619343e-06\n",
      "Loss: 4.659995e-06\n",
      "Loss: 4.656895e-06\n",
      "Loss: 4.65432e-06\n",
      "Loss: 4.6503255e-06\n",
      "Loss: 4.6476925e-06\n",
      "Loss: 4.644751e-06\n",
      "Loss: 4.638577e-06\n",
      "Loss: 4.635673e-06\n",
      "Loss: 4.6322602e-06\n",
      "Loss: 4.637089e-06\n",
      "Loss: 4.6292857e-06\n",
      "Loss: 4.625969e-06\n",
      "Loss: 4.6221517e-06\n",
      "Loss: 4.619471e-06\n",
      "Loss: 4.617455e-06\n",
      "Loss: 4.614221e-06\n",
      "Loss: 4.6129157e-06\n",
      "Loss: 4.610867e-06\n",
      "Loss: 4.609241e-06\n",
      "Loss: 4.607662e-06\n",
      "Loss: 4.6062646e-06\n",
      "Loss: 4.6051855e-06\n",
      "Loss: 4.6038035e-06\n",
      "Loss: 4.6016326e-06\n",
      "Loss: 4.598026e-06\n",
      "Loss: 4.8105676e-06\n",
      "Loss: 4.59676e-06\n",
      "Loss: 4.5928737e-06\n",
      "Loss: 4.5875827e-06\n",
      "Loss: 4.584156e-06\n",
      "Loss: 4.580904e-06\n",
      "Loss: 4.5812662e-06\n",
      "Loss: 4.578952e-06\n",
      "Loss: 4.5756146e-06\n",
      "Loss: 4.5730235e-06\n",
      "Loss: 4.5696875e-06\n",
      "Loss: 4.5667757e-06\n",
      "Loss: 4.5645893e-06\n",
      "Loss: 4.563579e-06\n",
      "Loss: 4.5613883e-06\n",
      "Loss: 4.5602783e-06\n",
      "Loss: 4.562727e-06\n",
      "Loss: 4.5594306e-06\n",
      "Loss: 4.5600136e-06\n",
      "Loss: 4.55559e-06\n",
      "Loss: 4.554363e-06\n",
      "Loss: 4.551551e-06\n",
      "Loss: 4.5486318e-06\n",
      "Loss: 4.567972e-06\n",
      "Loss: 4.5462957e-06\n",
      "Loss: 4.5386164e-06\n",
      "Loss: 4.5326005e-06\n",
      "Loss: 4.5298807e-06\n",
      "Loss: 4.528986e-06\n",
      "Loss: 4.5253987e-06\n",
      "Loss: 4.5240095e-06\n",
      "Loss: 4.5212273e-06\n",
      "Loss: 4.519212e-06\n",
      "Loss: 4.5182874e-06\n",
      "Loss: 4.5173656e-06\n",
      "Loss: 4.515629e-06\n",
      "Loss: 4.5131765e-06\n",
      "Loss: 4.526541e-06\n",
      "Loss: 4.5126035e-06\n",
      "Loss: 4.511108e-06\n",
      "Loss: 4.510028e-06\n",
      "Loss: 4.509257e-06\n",
      "Loss: 4.507417e-06\n",
      "Loss: 4.506553e-06\n",
      "Loss: 4.5057795e-06\n",
      "Loss: 4.5032084e-06\n",
      "Loss: 4.501071e-06\n",
      "Loss: 4.497808e-06\n",
      "Loss: 4.4953103e-06\n",
      "Loss: 4.4926505e-06\n",
      "Loss: 4.4954973e-06\n",
      "Loss: 4.491839e-06\n",
      "Loss: 4.4895996e-06\n",
      "Loss: 4.4863177e-06\n",
      "Loss: 4.4828785e-06\n",
      "Loss: 4.480127e-06\n",
      "Loss: 4.4775165e-06\n",
      "Loss: 4.4756985e-06\n",
      "Loss: 4.4726016e-06\n",
      "Loss: 4.4706626e-06\n",
      "Loss: 4.469024e-06\n",
      "Loss: 4.4672024e-06\n",
      "Loss: 4.465814e-06\n",
      "Loss: 4.4627977e-06\n",
      "Loss: 4.4603767e-06\n",
      "Loss: 4.458284e-06\n",
      "Loss: 4.4561884e-06\n",
      "Loss: 4.4533012e-06\n",
      "Loss: 4.451186e-06\n",
      "Loss: 4.4496505e-06\n",
      "Loss: 4.4485887e-06\n",
      "Loss: 4.4470303e-06\n",
      "Loss: 4.443624e-06\n",
      "Loss: 4.4421326e-06\n",
      "Loss: 4.4387316e-06\n",
      "Loss: 4.4364515e-06\n",
      "Loss: 4.433764e-06\n",
      "Loss: 4.4296216e-06\n",
      "Loss: 4.427074e-06\n",
      "Loss: 4.424399e-06\n",
      "Loss: 4.421823e-06\n",
      "Loss: 4.4197354e-06\n",
      "Loss: 4.4178814e-06\n",
      "Loss: 4.415384e-06\n",
      "Loss: 4.411673e-06\n",
      "Loss: 4.4097997e-06\n",
      "Loss: 4.4039725e-06\n",
      "Loss: 4.401199e-06\n",
      "Loss: 4.3985674e-06\n",
      "Loss: 4.396221e-06\n",
      "Loss: 4.3919376e-06\n",
      "Loss: 4.389721e-06\n",
      "Loss: 4.3864934e-06\n",
      "Loss: 4.3833347e-06\n",
      "Loss: 4.3796035e-06\n",
      "Loss: 4.375811e-06\n",
      "Loss: 4.371532e-06\n",
      "Loss: 4.368153e-06\n",
      "Loss: 4.3662762e-06\n",
      "Loss: 4.3635096e-06\n",
      "Loss: 4.3611535e-06\n",
      "Loss: 4.3572554e-06\n",
      "Loss: 4.354791e-06\n",
      "Loss: 4.3519826e-06\n",
      "Loss: 4.3494665e-06\n",
      "Loss: 4.3443442e-06\n",
      "Loss: 4.3432647e-06\n",
      "Loss: 4.341089e-06\n",
      "Loss: 4.336423e-06\n",
      "Loss: 4.3342966e-06\n",
      "Loss: 4.3317423e-06\n",
      "Loss: 4.330137e-06\n",
      "Loss: 4.3284285e-06\n",
      "Loss: 4.327277e-06\n",
      "Loss: 4.325592e-06\n",
      "Loss: 4.32426e-06\n",
      "Loss: 4.3234336e-06\n",
      "Loss: 4.3210057e-06\n",
      "Loss: 4.318902e-06\n",
      "Loss: 4.3160503e-06\n",
      "Loss: 4.314486e-06\n",
      "Loss: 4.312794e-06\n",
      "Loss: 4.3121468e-06\n",
      "Loss: 4.3102887e-06\n",
      "Loss: 4.308403e-06\n",
      "Loss: 4.3207974e-06\n",
      "Loss: 4.3071204e-06\n",
      "Loss: 4.3048103e-06\n",
      "Loss: 4.3009263e-06\n",
      "Loss: 4.2989486e-06\n",
      "Loss: 4.296356e-06\n",
      "Loss: 4.294197e-06\n",
      "Loss: 4.290217e-06\n",
      "Loss: 4.287571e-06\n",
      "Loss: 4.2853094e-06\n",
      "Loss: 4.2836396e-06\n",
      "Loss: 4.2821516e-06\n",
      "Loss: 4.28106e-06\n",
      "Loss: 4.2794636e-06\n",
      "Loss: 4.277837e-06\n",
      "Loss: 4.277865e-06\n",
      "Loss: 4.2770903e-06\n",
      "Loss: 4.274865e-06\n",
      "Loss: 4.27319e-06\n",
      "Loss: 4.2718857e-06\n",
      "Loss: 4.2715965e-06\n",
      "Loss: 4.2694064e-06\n",
      "Loss: 4.26694e-06\n",
      "Loss: 4.2636157e-06\n",
      "Loss: 4.2606757e-06\n",
      "Loss: 4.259421e-06\n",
      "Loss: 4.2559413e-06\n",
      "Loss: 4.254229e-06\n",
      "Loss: 4.249784e-06\n",
      "Loss: 4.2471506e-06\n",
      "Loss: 4.2851652e-06\n",
      "Loss: 4.246426e-06\n",
      "Loss: 4.2443344e-06\n",
      "Loss: 4.242598e-06\n",
      "Loss: 4.2405054e-06\n",
      "Loss: 4.238168e-06\n",
      "Loss: 4.2360775e-06\n",
      "Loss: 4.2353477e-06\n",
      "Loss: 4.2346574e-06\n",
      "Loss: 4.234321e-06\n",
      "Loss: 4.2329584e-06\n",
      "Loss: 4.230689e-06\n",
      "Loss: 4.2293077e-06\n",
      "Loss: 4.2354286e-06\n",
      "Loss: 4.2282427e-06\n",
      "Loss: 4.226571e-06\n",
      "Loss: 4.223015e-06\n",
      "Loss: 4.2216084e-06\n",
      "Loss: 4.219887e-06\n",
      "Loss: 4.2226425e-06\n",
      "Loss: 4.2192664e-06\n",
      "Loss: 4.218528e-06\n",
      "Loss: 4.2168413e-06\n",
      "Loss: 4.2149713e-06\n",
      "Loss: 4.2138327e-06\n",
      "Loss: 4.211424e-06\n",
      "Loss: 4.208595e-06\n",
      "Loss: 4.206674e-06\n",
      "Loss: 4.2048864e-06\n",
      "Loss: 4.202807e-06\n",
      "Loss: 4.2006427e-06\n",
      "Loss: 4.1992907e-06\n",
      "Loss: 4.1961202e-06\n",
      "Loss: 4.193879e-06\n",
      "Loss: 4.19157e-06\n",
      "Loss: 4.1894573e-06\n",
      "Loss: 4.1924e-06\n",
      "Loss: 4.188448e-06\n",
      "Loss: 4.184016e-06\n",
      "Loss: 4.180699e-06\n",
      "Loss: 4.173572e-06\n",
      "Loss: 4.1683566e-06\n",
      "Loss: 4.1656212e-06\n",
      "Loss: 4.163122e-06\n",
      "Loss: 4.1611283e-06\n",
      "Loss: 4.1589547e-06\n",
      "Loss: 4.15626e-06\n",
      "Loss: 4.1515787e-06\n",
      "Loss: 4.1469248e-06\n",
      "Loss: 4.1435424e-06\n",
      "Loss: 4.138313e-06\n",
      "Loss: 4.1343965e-06\n",
      "Loss: 4.129825e-06\n",
      "Loss: 4.1259736e-06\n",
      "Loss: 4.1214253e-06\n",
      "Loss: 4.1176663e-06\n",
      "Loss: 4.114469e-06\n",
      "Loss: 4.1125163e-06\n",
      "Loss: 4.1102257e-06\n",
      "Loss: 4.1087014e-06\n",
      "Loss: 4.10764e-06\n",
      "Loss: 4.1061435e-06\n",
      "Loss: 4.1063045e-06\n",
      "Loss: 4.105144e-06\n",
      "Loss: 4.1016892e-06\n",
      "Loss: 4.0980476e-06\n",
      "Loss: 4.088669e-06\n",
      "Loss: 4.0853793e-06\n",
      "Loss: 4.080958e-06\n",
      "Loss: 4.075692e-06\n",
      "Loss: 4.0721716e-06\n",
      "Loss: 4.0688096e-06\n",
      "Loss: 4.065791e-06\n",
      "Loss: 4.0639366e-06\n",
      "Loss: 4.0607492e-06\n",
      "Loss: 4.0592654e-06\n",
      "Loss: 4.0567907e-06\n",
      "Loss: 4.0541095e-06\n",
      "Loss: 4.0502982e-06\n",
      "Loss: 4.046789e-06\n",
      "Loss: 4.04154e-06\n",
      "Loss: 4.0407663e-06\n",
      "Loss: 4.0380364e-06\n",
      "Loss: 4.0349237e-06\n",
      "Loss: 4.0321083e-06\n",
      "Loss: 4.028837e-06\n",
      "Loss: 4.0263294e-06\n",
      "Loss: 4.0909745e-06\n",
      "Loss: 4.0225045e-06\n",
      "Loss: 4.016349e-06\n",
      "Loss: 4.011416e-06\n",
      "Loss: 4.0065925e-06\n",
      "Loss: 4.0046034e-06\n",
      "Loss: 4.003877e-06\n",
      "Loss: 4.0034415e-06\n",
      "Loss: 4.002826e-06\n",
      "Loss: 4.002106e-06\n",
      "Loss: 4.000128e-06\n",
      "Loss: 4.0327286e-06\n",
      "Loss: 3.9989645e-06\n",
      "Loss: 3.9952292e-06\n",
      "Loss: 3.9934616e-06\n",
      "Loss: 3.9912234e-06\n",
      "Loss: 3.989094e-06\n",
      "Loss: 3.9873466e-06\n",
      "Loss: 3.9858064e-06\n",
      "Loss: 3.982562e-06\n",
      "Loss: 3.980323e-06\n",
      "Loss: 3.9778183e-06\n",
      "Loss: 3.976827e-06\n",
      "Loss: 3.974082e-06\n",
      "Loss: 3.978173e-06\n",
      "Loss: 3.9727756e-06\n",
      "Loss: 3.9697434e-06\n",
      "Loss: 3.9658075e-06\n",
      "Loss: 3.9618717e-06\n",
      "Loss: 3.959617e-06\n",
      "Loss: 3.9579295e-06\n",
      "Loss: 3.9547526e-06\n",
      "Loss: 3.9521856e-06\n",
      "Loss: 3.949087e-06\n",
      "Loss: 3.9448128e-06\n",
      "Loss: 3.9404285e-06\n",
      "Loss: 3.9350502e-06\n",
      "Loss: 3.9332594e-06\n",
      "Loss: 3.931187e-06\n",
      "Loss: 3.930078e-06\n",
      "Loss: 3.9286983e-06\n",
      "Loss: 3.925815e-06\n",
      "Loss: 3.9240817e-06\n",
      "Loss: 4.060449e-06\n",
      "Loss: 3.9206325e-06\n",
      "Loss: 3.917027e-06\n",
      "Loss: 3.9133383e-06\n",
      "Loss: 3.9498113e-06\n",
      "Loss: 3.9126085e-06\n",
      "Loss: 3.910081e-06\n",
      "Loss: 3.9067095e-06\n",
      "Loss: 3.9229903e-06\n",
      "Loss: 3.9050537e-06\n",
      "Loss: 3.9025053e-06\n",
      "Loss: 3.8998037e-06\n",
      "Loss: 3.898448e-06\n",
      "Loss: 3.897128e-06\n",
      "Loss: 3.895965e-06\n",
      "Loss: 3.894605e-06\n",
      "Loss: 3.8931103e-06\n",
      "Loss: 3.903585e-06\n",
      "Loss: 3.8924236e-06\n",
      "Loss: 3.9063866e-06\n",
      "Loss: 3.891701e-06\n",
      "Loss: 3.891494e-06\n",
      "Loss: 3.8907215e-06\n",
      "Loss: 3.9166775e-06\n",
      "Loss: 3.889153e-06\n",
      "Loss: 3.886593e-06\n",
      "Loss: 3.8835533e-06\n",
      "Loss: 3.879846e-06\n",
      "Loss: 3.877651e-06\n",
      "Loss: 3.877051e-06\n",
      "Loss: 3.874031e-06\n",
      "Loss: 3.8723906e-06\n",
      "Loss: 3.8705157e-06\n",
      "Loss: 3.8691287e-06\n",
      "Loss: 3.86762e-06\n",
      "Loss: 3.8657386e-06\n",
      "Loss: 3.8630837e-06\n",
      "Loss: 3.863965e-06\n",
      "Loss: 3.860927e-06\n",
      "Loss: 3.857893e-06\n",
      "Loss: 3.8526277e-06\n",
      "Loss: 3.850371e-06\n",
      "Loss: 3.8457893e-06\n",
      "Loss: 3.8429444e-06\n",
      "Loss: 3.8406906e-06\n",
      "Loss: 3.839977e-06\n",
      "Loss: 3.836867e-06\n",
      "Loss: 3.8356734e-06\n",
      "Loss: 3.834353e-06\n",
      "Loss: 3.833672e-06\n",
      "Loss: 3.832798e-06\n",
      "Loss: 3.831022e-06\n",
      "Loss: 3.8296334e-06\n",
      "Loss: 3.8272374e-06\n",
      "Loss: 3.824615e-06\n",
      "Loss: 3.821147e-06\n",
      "Loss: 3.8237663e-06\n",
      "Loss: 3.819493e-06\n",
      "Loss: 3.8164444e-06\n",
      "Loss: 3.8142955e-06\n",
      "Loss: 3.8120456e-06\n",
      "Loss: 3.8101884e-06\n",
      "Loss: 3.8091873e-06\n",
      "Loss: 3.8086273e-06\n",
      "Loss: 3.8084163e-06\n",
      "Loss: 3.8076837e-06\n",
      "Loss: 3.8068229e-06\n",
      "Loss: 3.804948e-06\n",
      "Loss: 3.8033982e-06\n",
      "Loss: 3.8022536e-06\n",
      "Loss: 3.8003575e-06\n",
      "Loss: 3.8020664e-06\n",
      "Loss: 3.79972e-06\n",
      "Loss: 3.7974087e-06\n",
      "Loss: 3.7962575e-06\n",
      "Loss: 3.794242e-06\n",
      "Loss: 3.7915413e-06\n",
      "Loss: 3.7886953e-06\n",
      "Loss: 3.786227e-06\n",
      "Loss: 3.782828e-06\n",
      "Loss: 3.7786137e-06\n",
      "Loss: 3.7761429e-06\n",
      "Loss: 3.7735517e-06\n",
      "Loss: 3.7726086e-06\n",
      "Loss: 3.7713878e-06\n",
      "Loss: 3.770202e-06\n",
      "Loss: 3.76791e-06\n",
      "Loss: 3.7671862e-06\n",
      "Loss: 3.7651625e-06\n",
      "Loss: 3.7644063e-06\n",
      "Loss: 3.7623176e-06\n",
      "Loss: 3.7606956e-06\n",
      "Loss: 3.8058613e-06\n",
      "Loss: 3.7598993e-06\n",
      "Loss: 3.7579184e-06\n",
      "Loss: 3.7556365e-06\n",
      "Loss: 3.7541624e-06\n",
      "Loss: 3.7526206e-06\n",
      "Loss: 3.750995e-06\n",
      "Loss: 3.7481461e-06\n",
      "Loss: 3.7467216e-06\n",
      "Loss: 3.7454495e-06\n",
      "Loss: 3.7449174e-06\n",
      "Loss: 3.7441152e-06\n",
      "Loss: 3.7431003e-06\n",
      "Loss: 3.7414964e-06\n",
      "Loss: 3.7400112e-06\n",
      "Loss: 3.739116e-06\n",
      "Loss: 3.738124e-06\n",
      "Loss: 3.736901e-06\n",
      "Loss: 3.7362406e-06\n",
      "Loss: 3.7355808e-06\n",
      "Loss: 3.7338273e-06\n",
      "Loss: 3.7349803e-06\n",
      "Loss: 3.7330865e-06\n",
      "Loss: 3.732153e-06\n",
      "Loss: 3.7323139e-06\n",
      "Loss: 3.7313125e-06\n",
      "Loss: 3.7306243e-06\n",
      "Loss: 3.729957e-06\n",
      "Loss: 3.728168e-06\n",
      "Loss: 3.7258444e-06\n",
      "Loss: 3.724464e-06\n",
      "Loss: 3.7217042e-06\n",
      "Loss: 3.719516e-06\n",
      "Loss: 3.717409e-06\n",
      "Loss: 3.7166367e-06\n",
      "Loss: 3.714504e-06\n",
      "Loss: 3.7135724e-06\n",
      "Loss: 3.7121733e-06\n",
      "Loss: 3.7105901e-06\n",
      "Loss: 3.7089505e-06\n",
      "Loss: 3.7075522e-06\n",
      "Loss: 3.705663e-06\n",
      "Loss: 3.7030068e-06\n",
      "Loss: 3.7016132e-06\n",
      "Loss: 3.7004347e-06\n",
      "Loss: 3.6993843e-06\n",
      "Loss: 3.6983477e-06\n",
      "Loss: 3.6972342e-06\n",
      "Loss: 3.6963131e-06\n",
      "Loss: 3.6949295e-06\n",
      "Loss: 3.6939723e-06\n",
      "Loss: 3.69246e-06\n",
      "Loss: 3.6908104e-06\n",
      "Loss: 3.6898562e-06\n",
      "Loss: 3.6888055e-06\n",
      "Loss: 3.6879624e-06\n",
      "Loss: 3.6871008e-06\n",
      "Loss: 3.6862673e-06\n",
      "Loss: 3.6853548e-06\n",
      "Loss: 3.6847214e-06\n",
      "Loss: 3.6843203e-06\n",
      "Loss: 3.6833103e-06\n",
      "Loss: 3.6828117e-06\n",
      "Loss: 3.6821643e-06\n",
      "Loss: 3.6816032e-06\n",
      "Loss: 3.6805811e-06\n",
      "Loss: 3.6787794e-06\n",
      "Loss: 3.676702e-06\n",
      "Loss: 3.6744866e-06\n",
      "Loss: 3.6723666e-06\n",
      "Loss: 3.6698034e-06\n",
      "Loss: 3.6692538e-06\n",
      "Loss: 3.6692632e-06\n",
      "Loss: 3.6688207e-06\n",
      "Loss: 3.668148e-06\n",
      "Loss: 3.6671029e-06\n",
      "Loss: 3.6665083e-06\n",
      "Loss: 3.6653992e-06\n",
      "Loss: 3.6644064e-06\n",
      "Loss: 3.6640156e-06\n",
      "Loss: 3.6632557e-06\n",
      "Loss: 3.6615857e-06\n",
      "Loss: 3.6598076e-06\n",
      "Loss: 3.658617e-06\n",
      "Loss: 3.6571391e-06\n",
      "Loss: 3.6554277e-06\n",
      "Loss: 3.6538327e-06\n",
      "Loss: 3.6518982e-06\n",
      "Loss: 3.6526526e-06\n",
      "Loss: 3.651031e-06\n",
      "Loss: 3.6503216e-06\n",
      "Loss: 3.6487008e-06\n",
      "Loss: 3.6471297e-06\n",
      "Loss: 3.6451331e-06\n",
      "Loss: 3.644256e-06\n",
      "Loss: 3.6432962e-06\n",
      "Loss: 3.641848e-06\n",
      "Loss: 3.6405686e-06\n",
      "Loss: 3.6392214e-06\n",
      "Loss: 3.637632e-06\n",
      "Loss: 3.6356262e-06\n",
      "Loss: 3.6345916e-06\n",
      "Loss: 3.6340177e-06\n",
      "Loss: 3.6331785e-06\n",
      "Loss: 3.6325946e-06\n",
      "Loss: 3.6311035e-06\n",
      "Loss: 3.630029e-06\n",
      "Loss: 3.628526e-06\n",
      "Loss: 3.627385e-06\n",
      "Loss: 3.6257768e-06\n",
      "Loss: 3.6245126e-06\n",
      "Loss: 3.6232966e-06\n",
      "Loss: 3.6220238e-06\n",
      "Loss: 3.6213114e-06\n",
      "Loss: 3.6227543e-06\n",
      "Loss: 3.6207757e-06\n",
      "Loss: 3.6195488e-06\n",
      "Loss: 3.6183912e-06\n",
      "Loss: 3.6172382e-06\n",
      "Loss: 3.6165966e-06\n",
      "Loss: 3.637013e-06\n",
      "Loss: 3.6162508e-06\n",
      "Loss: 3.6157135e-06\n",
      "Loss: 3.6149822e-06\n",
      "Loss: 3.6138285e-06\n",
      "Loss: 3.6127399e-06\n",
      "Loss: 3.6114766e-06\n",
      "Loss: 3.608768e-06\n",
      "Loss: 3.6102492e-06\n",
      "Loss: 3.607216e-06\n",
      "Loss: 3.605803e-06\n",
      "Loss: 3.60444e-06\n",
      "Loss: 3.6036415e-06\n",
      "Loss: 3.6023378e-06\n",
      "Loss: 3.600161e-06\n",
      "Loss: 3.598946e-06\n",
      "Loss: 3.597575e-06\n",
      "Loss: 3.5967628e-06\n",
      "Loss: 3.595614e-06\n",
      "Loss: 3.5946284e-06\n",
      "Loss: 3.5941725e-06\n",
      "Loss: 3.5924359e-06\n",
      "Loss: 3.5914666e-06\n",
      "Loss: 3.5905293e-06\n",
      "Loss: 3.5893215e-06\n",
      "Loss: 3.5886387e-06\n",
      "Loss: 3.5866467e-06\n",
      "Loss: 3.5852227e-06\n",
      "Loss: 3.5824787e-06\n",
      "Loss: 3.583075e-06\n",
      "Loss: 3.5819633e-06\n",
      "Loss: 3.5804505e-06\n",
      "Loss: 3.5790854e-06\n",
      "Loss: 3.5779324e-06\n",
      "Loss: 3.5766975e-06\n",
      "Loss: 3.576043e-06\n",
      "Loss: 3.5750813e-06\n",
      "Loss: 3.5747373e-06\n",
      "Loss: 3.573959e-06\n",
      "Loss: 3.5734672e-06\n",
      "Loss: 3.5717915e-06\n",
      "Loss: 3.5708988e-06\n",
      "Loss: 3.5694818e-06\n",
      "Loss: 3.5686328e-06\n",
      "Loss: 3.5681182e-06\n",
      "Loss: 3.5670162e-06\n",
      "Loss: 3.5668636e-06\n",
      "Loss: 3.5660714e-06\n",
      "Loss: 3.565286e-06\n",
      "Loss: 3.567367e-06\n",
      "Loss: 3.5653572e-06\n",
      "Loss: 3.5652738e-06\n",
      "Loss: 3.5644307e-06\n",
      "Loss: 3.5638898e-06\n",
      "Loss: 3.563311e-06\n",
      "Loss: 3.5622363e-06\n",
      "Loss: 3.5606972e-06\n",
      "Loss: 3.6042093e-06\n",
      "Loss: 3.56057e-06\n",
      "Loss: 3.55981e-06\n",
      "Loss: 3.5590315e-06\n",
      "Loss: 3.5584035e-06\n",
      "Loss: 3.5579762e-06\n",
      "Loss: 3.5569863e-06\n",
      "Loss: 3.5561598e-06\n",
      "Loss: 3.5553696e-06\n",
      "Loss: 3.5542553e-06\n",
      "Loss: 3.5536432e-06\n",
      "Loss: 3.5523467e-06\n",
      "Loss: 3.5512244e-06\n",
      "Loss: 3.550336e-06\n",
      "Loss: 3.5495011e-06\n",
      "Loss: 3.5483067e-06\n",
      "Loss: 3.547529e-06\n",
      "Loss: 3.5469566e-06\n",
      "Loss: 3.5468188e-06\n",
      "Loss: 3.546655e-06\n",
      "Loss: 3.5461214e-06\n",
      "Loss: 3.5456394e-06\n",
      "Loss: 3.5444557e-06\n",
      "Loss: 3.5438118e-06\n",
      "Loss: 3.542853e-06\n",
      "Loss: 3.5421492e-06\n",
      "Loss: 3.5404041e-06\n",
      "Loss: 3.539307e-06\n",
      "Loss: 3.538129e-06\n",
      "Loss: 3.5375626e-06\n",
      "Loss: 3.5366268e-06\n",
      "Loss: 3.5353582e-06\n",
      "Loss: 3.5350831e-06\n",
      "Loss: 3.5341043e-06\n",
      "Loss: 3.5336886e-06\n",
      "Loss: 3.5316818e-06\n",
      "Loss: 3.5305716e-06\n",
      "Loss: 3.526976e-06\n",
      "Loss: 3.5259038e-06\n",
      "Loss: 3.5253352e-06\n",
      "Loss: 3.5242267e-06\n",
      "Loss: 3.522191e-06\n",
      "Loss: 3.5190342e-06\n",
      "Loss: 3.5159565e-06\n",
      "Loss: 3.5143287e-06\n",
      "Loss: 3.51352e-06\n",
      "Loss: 3.5128828e-06\n",
      "Loss: 3.5118844e-06\n",
      "Loss: 3.5105445e-06\n",
      "Loss: 3.5117507e-06\n",
      "Loss: 3.509856e-06\n",
      "Loss: 3.5083572e-06\n",
      "Loss: 3.5066444e-06\n",
      "Loss: 3.5055516e-06\n",
      "Loss: 3.5046796e-06\n",
      "Loss: 3.5039914e-06\n",
      "Loss: 3.5038352e-06\n",
      "Loss: 3.5025228e-06\n",
      "Loss: 3.5014752e-06\n",
      "Loss: 3.499863e-06\n",
      "Loss: 3.4987702e-06\n",
      "Loss: 3.4974128e-06\n",
      "Loss: 3.4968666e-06\n",
      "Loss: 3.495848e-06\n",
      "Loss: 3.5144717e-06\n",
      "Loss: 3.4955249e-06\n",
      "Loss: 3.4948243e-06\n",
      "Loss: 3.4944132e-06\n",
      "Loss: 3.4941986e-06\n",
      "Loss: 3.493515e-06\n",
      "Loss: 3.4918953e-06\n",
      "Loss: 3.4911156e-06\n",
      "Loss: 3.492264e-06\n",
      "Loss: 3.490239e-06\n",
      "Loss: 3.4892664e-06\n",
      "Loss: 3.4879872e-06\n",
      "Loss: 3.487407e-06\n",
      "Loss: 3.4867433e-06\n",
      "Loss: 3.4857046e-06\n",
      "Loss: 3.4841391e-06\n",
      "Loss: 3.4821478e-06\n",
      "Loss: 3.480552e-06\n",
      "Loss: 3.4797858e-06\n",
      "Loss: 3.4787192e-06\n",
      "Loss: 3.4784152e-06\n",
      "Loss: 3.4773013e-06\n",
      "Loss: 3.4761813e-06\n",
      "Loss: 3.4744367e-06\n",
      "Loss: 3.4934335e-06\n",
      "Loss: 3.4744767e-06\n",
      "Loss: 3.47447e-06\n",
      "Loss: 3.4745512e-06\n",
      "Loss: 3.4747725e-06\n",
      "Loss: 3.4744367e-06\n",
      "Loss: 3.4744367e-06\n",
      "Loss: 3.4744367e-06\n",
      "Loss: 3.4744367e-06\n",
      "Loss: 3.4744367e-06\n",
      "Loss: 3.4744367e-06\n",
      "Loss: 3.4744367e-06\n",
      "Loss: 3.4744367e-06\n",
      "Loss: 3.4744367e-06\n",
      "Loss: 3.4744367e-06\n",
      "Loss: 3.4744367e-06\n",
      "Loss: 3.4744367e-06\n",
      "Loss: 3.4744367e-06\n",
      "INFO:tensorflow:Optimization terminated with:\n",
      "  Message: CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH\n",
      "  Objective function value: 0.000003\n",
      "  Number of iterations: 5329\n",
      "  Number of functions evaluations: 5841\n",
      "Training time: 104.6003\n",
      "Error u: 1.719242e-03\n"
     ]
    }
   ],
   "source": [
    "#Python中的一个常见模式。在Python文件被直接运行时，特殊变量__name__的值为\"__main__\"。所以，这行代码的意思是：如果这个文件被直接运行（而不是被导入作为模块），那么就执行后面的代码\n",
    "if __name__ == \"__main__\":   #这种模式常常用于在一个Python文件中区分出哪些代码是用于定义函数、类等，哪些代码是用于直接执行的。这样，当这个文件被导入作为模块时，只有函数和类的定义会被执行，而直接执行的代码则不会被执行\n",
    "     \n",
    "    nu = 0.01/np.pi\n",
    "    #设置噪声水平为0\n",
    "    noise = 0.0        \n",
    "\n",
    "    N_u = 100\n",
    "    N_f = 10000\n",
    "    #定义一个列表layers，其中包含了神经网络的层数和每一层的神经元数量\n",
    "    layers = [2, 20, 20, 20, 20, 20, 20, 20, 20, 1]\n",
    "    #读取名为burgers_shock的Matlab文件，文件中的数据存储在data变量中。这里的路径也要随着设备的情况修改 \n",
    "    data = scipy.io.loadmat('C:/Users/cheny/Documents/GitHub/PINNs/appendix/Data/burgers_shock.mat')\n",
    "    #从data字典中取出变量tt和x的值，并转换为一维数组（flatten方法），最后tongg[:,None]将一维数组转换为二维数组\n",
    "    t = data['t'].flatten()[:,None]\n",
    "    x = data['x'].flatten()[:,None]\n",
    "    Exact = np.real(data['usol']).T #从data数据中取出usol的值，并取实部，最后转置，赋值给Exact\n",
    "    #生成一个二位网络，X和T是输出的二维数组\n",
    "    #这个点结果是X和T均为形状为[len(t),len(x)]的二维数组，X的每一行都是x，一共len(t)行，T的每一列都是t，一共len(x)列\n",
    "    X, T = np.meshgrid(x,t)\n",
    "    \n",
    "    X_star = np.hstack((X.flatten()[:,None], T.flatten()[:,None]))  #按列堆叠数组，X_star是一个二维数组，其中第一列是X的展平，第二列是T的展平\n",
    "    u_star = Exact.flatten()[:,None]    #对Exact_u使用flatten方法将其转换为一维数组，最后使用[:,None]将其转换为二维数组         \n",
    "\n",
    "    # Doman bounds，分别获得X_star的相应列上的最小值和最大值，赋值给lb和ub,也就是说lb是x和t的最小值，ub是x和t的最大值，即lb和ub分别为[-1,0]和[1,1]\n",
    "    lb = X_star.min(0)\n",
    "    ub = X_star.max(0)    \n",
    "        \n",
    "    xx1 = np.hstack((X[0:1,:].T, T[0:1,:].T)) #分别取X，T的第一行的转置(分别是x和全0列)，分别构成xx1的第一列和第二列\n",
    "    uu1 = Exact[0:1,:].T #取Exact的第一行的转置，赋值给uu1\n",
    "    xx2 = np.hstack((X[:,0:1], T[:,0:1])) #分别取X，T的第一列，分别构成xx2的第一列和第二列\n",
    "    uu2 = Exact[:,0:1] #取Exact的第一列，赋值给uu2\n",
    "    xx3 = np.hstack((X[:,-1:], T[:,-1:])) #分别取X，T的最后一列，分别构成xx3的第一列和第二列\n",
    "    uu3 = Exact[:,-1:] #取Exact的最后一列，赋值给uu3\n",
    "    \n",
    "    X_u_train = np.vstack([xx1, xx2, xx3]) #X_u_train=(xx1;xx2;xx3)\n",
    "    X_f_train = lb + (ub-lb)*lhs(2, N_f)  #lhs函数采用拉丁超采样方法，生成一个近似均匀分布的多维样本点集，返回的是一个形状为（N_f，2）的数组，每一行都是一个2维的样本点，所有样本点都在[0,1]范围内，并对该样本集进行缩放，把每个样本从[0,1]区间缩放到[lb,ub]区域内，即得到了指定范围内均匀分布的样本X_f_train。\n",
    "    X_f_train = np.vstack((X_f_train, X_u_train)) #按行堆叠数组，即将X_f_train和X_u_train按行合并，得到一个新的数组X_f_train\n",
    "    u_train = np.vstack([uu1, uu2, uu3]) #u_train=(uu1;uu2;uu3)\n",
    "    \n",
    "    idx = np.random.choice(X_u_train.shape[0], N_u, replace=False) #从0~数组X_u_train的行数 中随机选择N_u个数，replace=False表示不允许重复选择，最后将这N_u个数赋值给idx\n",
    "    X_u_train = X_u_train[idx, :] #从X_u_train中选取idx对应的的N_u行，赋值给X_u_train\n",
    "    u_train = u_train[idx,:] #从u_train中选取idx对应的的N_u行，赋值给u_train\n",
    "\n",
    "    #创建PINN模型并输入各种参数     \n",
    "    model = PhysicsInformedNN(X_u_train, u_train, X_f_train, layers, lb, ub, nu)\n",
    "    \n",
    "    #获取当前时间并赋值给start_time  \n",
    "    start_time = time.time()   \n",
    "    #开始训练模型            \n",
    "    model.train()\n",
    "    #训练结束后获取当前时间并减去start_time，得到训练时间并赋值给elapsed\n",
    "    elapsed = time.time() - start_time\n",
    "    #打印训练所花时间                \n",
    "    print('Training time: %.4f' % (elapsed))\n",
    "    \n",
    "    #用训练好的模型调用predict方法进行预测，返回两个值（均为数组）\n",
    "    u_pred, f_pred = model.predict(X_star)\n",
    "    #计算误差（基于2范数）        \n",
    "    error_u = np.linalg.norm(u_star-u_pred,2)/np.linalg.norm(u_star,2)\n",
    "    #打印误差值\n",
    "    print('Error u: %e' % (error_u))                     \n",
    "\n",
    "    #使用griddata函数将X_star、u_pred插值到网格上，得到U_pred\n",
    "    U_pred = griddata(X_star, u_pred.flatten(), (X, T), method='cubic')\n",
    "    #计算插值方法的误差\n",
    "    Error = np.abs(Exact - U_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 539.643x366.869 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "######################################################################\n",
    "############################# Plotting ###############################\n",
    "######################################################################    \n",
    "\n",
    "fig, ax = newfig(1.0, 1.1) #这里ax是一个axes对象，代表子图，figure是一个figure对象，是一个图形窗口，代表整个图形\n",
    "ax.axis('off') #关闭子图的轴显示\n",
    "\n",
    "####### Row 0: u(t,x) ##################    \n",
    "gs0 = gridspec.GridSpec(1, 2) #创建一个1×2的网络，用于存放子图\n",
    "gs0.update(top=1-0.06, bottom=1-1/3, left=0.15, right=0.85, wspace=0) #更新该网络的参数，第一个表示子图的顶部位置为0.94，第二个参数表示子图的底部位置为0.667，第三个表示子图左侧的位置为0.15，第四个参数表示子图的右侧位置为0.85，第五个参数表示子图之间的宽度，0表示子图之间没有空隙\n",
    "ax = plt.subplot(gs0[:, :]) #在gs0[:,:] 指定的位置创建了一个子图，并将返回的axes对象赋值给ax。gs0[:,:]表示GridSpec对象gs0的所有行和所有列，所以这行代码创建的子图占据了整个图形。\n",
    "\n",
    "h = ax.imshow(U_pred.T, interpolation='nearest', cmap='rainbow', \n",
    "                extent=[t.min(), t.max(), x.min(), x.max()], \n",
    "                origin='lower', aspect='auto')   #imshow函数用于显示图像，接受一些参数，第一个参数是图像数据，这里是U_pred的转置；第二个参数是插值方法（用于在像素之间插入新的像素），这里是最邻近插值；\n",
    "                                                #第三个参数是颜色映射，这里是用彩虹色图；第四个参数是图像的范围；第五个参数是图像的原点位置，这里表示原点在右下角；第六个参数是图像的纵横比，这里表示调整横纵比以填充整个axes对象\n",
    "                                                #最后的结果返回一个axesimage对象，也就是h，可以通过这个对象进一步设置图像的属性\n",
    "divider = make_axes_locatable(ax)  #使用 make_axes_locatable 函数创建了一个 AxesDivider 对象。这个函数接受一个 Axes 对象作为参数，返回一个 AxesDivider 对象。AxesDivider 对象可以用来管理子图的布局，特别是当你需要在一个图的旁边添加另一个图时。\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05) #使用append_axes方法在原始轴的右侧添加了一个新的轴。append_axes 方法接受三个参数：位置（\"right\"）、大小（\"5%\"）和间距（0.05）。在原始轴的右侧添加了一个新的轴，新轴的大小是原始轴的 5%，新轴与原始轴之间的间距是 0.05 英寸\n",
    "fig.colorbar(h, cax=cax) #使用colorbar方法在新轴上添加了一个颜色条。colorbar 方法接受两个参数：axesimage 对象（h）和新轴（cax）。\n",
    "\n",
    "ax.plot(X_u_train[:,1], X_u_train[:,0], 'kx', label = 'Data (%d points)' % (u_train.shape[0]), markersize = 4, clip_on = False)  #在ax上绘制散点图，前两个参数是散点的x坐标和y坐标；kx表示黑色的x（散点形状是x），label是散点的标签，clip_on表示散点可以绘制在轴的边界外\n",
    "\n",
    "line = np.linspace(x.min(), x.max(), 2)[:,None]#生成了一个包含2个等间距的数值的数组，这些数值在 x.min() 到 x.max() 之间。[:,None] 是一个索引操作，用于将一维数组转换为二维数组。这里其实就是[-5;5]\n",
    "#第一个参数是虚线的x坐标，line是虚线y的坐标，第三个参数是虚线的样式，w表示白色，-表示实线，最后一个参数表示虚线的参数是1\n",
    "ax.plot(t[25]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
    "ax.plot(t[50]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
    "ax.plot(t[75]*np.ones((2,1)), line, 'w-', linewidth = 1)    \n",
    "#设置ax子图的x轴的标签为t，y轴的标签为x。这里$t$和$x$是latex格式的文本，用于生成数学公式\n",
    "ax.set_xlabel('$t$')\n",
    "ax.set_ylabel('$x$')\n",
    "#设置子图ax的图例，frameon=False表示不显示图例的边框，loc='best'表示图例的位置是最佳位置，最后返回的leg是一个legend对象，表示图形的图例\n",
    "ax.legend(frameon=False, loc = 'best')\n",
    "#设置子图ax的标题为$u(t,x)$，表示latex格式的文本，用于生成数学公式，fontsize=10表示字体大小为10\n",
    "ax.set_title('$u(t,x)$', fontsize = 10)\n",
    "\n",
    "####### Row 1: u(t,x) slices ##################    \n",
    "gs1 = gridspec.GridSpec(1, 3)#创建一个1×3的网络，用于存放子图\n",
    "gs1.update(top=1-1/3, bottom=0, left=0.1, right=0.9, wspace=0.5)  #更新该网络的参数，第一个表示子图的顶部位置为0.667，第二个参数表示子图的底部位置为0，第三个表示子图左侧的位置为0，第四个参数表示子图的右侧位置为0.9，第五个参数表示子图之间的宽度为0.5\n",
    "\n",
    "ax = plt.subplot(gs1[0, 0])  #在gs1[0,0]指定的位置，也就是网格的第一行第一列，创建了第一个子图，并将返回的axes对象赋值给ax。\n",
    "#绘制了两条线，一条表示精确值，一条表示预测值\n",
    "ax.plot(x,Exact[25,:], 'b-', linewidth = 2, label = 'Exact')   #第一个参数表示x轴上的坐标；第二个参数表示y轴上的坐标；第三个参数b-表示蓝色的实线；linewidth表示线的宽度为2；label表示线的标签，这条线表示实际值     \n",
    "ax.plot(x,U_pred[25,:], 'r--', linewidth = 2, label = 'Prediction') #同上，红色虚线表示预测值\n",
    "ax.set_xlabel('$x$') #设置子图的x轴的标签为x\n",
    "ax.set_ylabel('$u(t,x)$')    #设置子图的y轴的标签为u(t,x)\n",
    "ax.set_title('$t = 0.25$', fontsize = 10) #设置子图的标题为t=0.25，fontsize=10表示字体大小为10\n",
    "ax.axis('square') #设置子图的纵横比为1，即正方形的区域\n",
    "ax.set_xlim([-1.1,1.1]) #设置第一个子图的x轴的范围为[-1.1,1.1]\n",
    "ax.set_ylim([-1.1,1.1]) #设置第一个子图的y轴的范围为[-1.1,1.1]\n",
    "\n",
    "ax = plt.subplot(gs1[0, 1]) #在gs1[0,1]指定的位置，也就是网格的第一行第二列，创建了第二个子图，并将返回的axes对象赋值给ax。\n",
    "ax.plot(x,Exact[50,:], 'b-', linewidth = 2, label = 'Exact')   #第一个参数表示x轴上的坐标；第二个参数表示y轴上的坐标；第三个参数b-表示蓝色的实线；linewidth表示线的宽度为2；label表示线的标签，这条线表示实际值    \n",
    "ax.plot(x,U_pred[50,:], 'r--', linewidth = 2, label = 'Prediction') #同上，红色虚线表示预测值\n",
    "ax.set_xlabel('$x$') #设置子图的x轴的标签为x\n",
    "ax.set_ylabel('$u(t,x)$') #设置子图的y轴的标签为u(t,x)\n",
    "ax.axis('square') #设置子图的纵横比为1，即正方形的区域\n",
    "ax.set_xlim([-1.1,1.1]) #设置第二个子图的x轴的范围为[-1.1,1.1]\n",
    "ax.set_ylim([-1.1,1.1]) #设置第二个子图的y轴的范围为[-1.1,1.1]\n",
    "ax.set_title('$t = 0.50$', fontsize = 10) #设置子图的标题为t=0.50，fontsize=10表示字体大小为10\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.35), ncol=5, frameon=False) #设置子图的图例，loc='upper center'表示图例的位置是上方中间，bbox_to_anchor=(0.5, -0.35)表示图例的位置是在子图的中间下方0.35的位置，ncol=5表示图例的列数为5，frameon=False表示不显示图例的边框\n",
    "\n",
    "ax = plt.subplot(gs1[0, 2]) #在gs1[0,2]指定的位置，也就是网格的第一行第三列，创建了第三个子图，并将返回的axes对象赋值给ax。\n",
    "ax.plot(x,Exact[75,:], 'b-', linewidth = 2, label = 'Exact')  #第一个参数表示x轴上的坐标；第二个参数表示y轴上的坐标；第三个参数b-表示蓝色的实线；linewidth表示线的宽度为2；label表示线的标签，这条线表示实际值\n",
    "ax.plot(x,U_pred[75,:], 'r--', linewidth = 2, label = 'Prediction') #同上，红色虚线表示预测值\n",
    "ax.set_xlabel('$x$') #设置子图的x轴的标签为x\n",
    "ax.set_ylabel('$u(t,x)$') #设置子图的y轴的标签为u(t,x)\n",
    "ax.axis('square') #设置子图的纵横比为1，即正方形的区域\n",
    "ax.set_xlim([-1.1,1.1]) #设置第三个子图的x轴的范围为[-1.1,1.1]\n",
    "ax.set_ylim([-1.1,1.1]) #设置第三个子图的y轴的范围为[-1.1,1.1]\n",
    "ax.set_title('$t = 0.75$', fontsize = 10) #设置子图的标题为t=0.75，fontsize=10表示字体大小为10\n",
    "\n",
    "# savefig('./figures/Burgers')     #用来保存图形，将当前图形保存为名为‘NLS’的文件，保存到位置是当前目录下的‘figures’文件夹；这里的路径也要随着设备的情况修改。注意这边来必须提前建立好figures文件夹，否则会报错\n",
    "#在文件路径中，\".\"和\"..\"有特殊的含义。\".\"表示当前目录，\"..\"表示上一级目录。例如，如果你在\"/home/user/documents\"目录下，\".\"就表示\"/home/user/documents\"，而\"..\"表示\"/home/user\"。\"当前文件夹\"通常指的是正在执行的脚本所在的文件夹。在Python中，你可以使用os.getcwd()来获取当前工作目录。\n",
    "savefig('C:/Users/cheny/Documents/GitHub/PINNs/appendix/continuous_time_inference (Burgers)/figures/Burgers')\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf1.15",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
